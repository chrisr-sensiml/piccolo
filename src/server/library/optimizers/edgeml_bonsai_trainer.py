"""
Copyright 2017-2024 SensiML Corporation

This file is part of SensiML™ Piccolo AI™.

SensiML Piccolo AI is free software: you can redistribute it and/or
modify it under the terms of the GNU Affero General Public License
as published by the Free Software Foundation, either version 3 of
the License, or (at your option) any later version.

SensiML Piccolo AI is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public
License along with SensiML Piccolo AI. If not, see <https://www.gnu.org/licenses/>.
"""

# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT license.
from __future__ import print_function

import logging
import os

import engine.base.edgeml_utils as utils
import numpy as np

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import tensorflow as tf
from logger.log_handler import LogHandler

logger = LogHandler(logging.getLogger(__name__))

PRINT_PROGRESS_LOGS = False


class EdgemlBonsaiTrainer:
    def __init__(
        self,
        bonsaiObj,
        lW,
        lT,
        lV,
        lZ,
        sW,
        sT,
        sV,
        sZ,
        learningRate,
        useMCHLoss=False,
        regLoss="huber",
        pipeline_id=None,
    ):
        """
        bonsaiObj - Initialised Bonsai Object and Graph
        lW, lT, lV and lZ are regularisers to Bonsai Params
        sW, sT, sV and sZ are sparsity factors to Bonsai Params
        learningRate - learningRate fro optimizer
        X is the Data Placeholder - Dims [_, dataDimension]
        Y - Label placeholder for loss computation
        useMCHLoss - For choice between HingeLoss vs CrossEntropy
        useMCHLoss - True - MultiClass - multiClassHingeLoss
        useMCHLoss - False - MultiClass - crossEntropyLoss
        """

        self.pipeline_id = pipeline_id
        self.bonsaiObj = bonsaiObj
        self.regressionLoss = regLoss

        self.lW = lW
        self.lV = lV
        self.lT = lT
        self.lZ = lZ

        self.sW = sW
        self.sV = sV
        self.sT = sT
        self.sZ = sZ

        self.useMCHLoss = useMCHLoss

        self.learningRate = learningRate

        self.assertInit()

        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:
            self.isDenseTraining = True
        else:
            self.isDenseTraining = False

        self.optimizer = self.trainGraph()

        # self.sigmaI = tf.Variable(dtype=tf.float32, name="sigmaI")

        # self.score, self.X_ = self.bonsaiObj(self.X, self.sigmaI)

        # self.loss, self.marginLoss, self.regLoss = self.lossGraph()

        """
        self.accuracy -> 'MAE' for Regression.
        self.accuracy -> 'Accuracy' for Classification.
        """
        # self.accuracy = self.accuracyGraph()
        # self.prediction = self.bonsaiObj.getPrediction()

        # self.hardThrsd()
        # self.sparseTraining()

    def loss(self, logits, Y):
        """
        Loss Graph for given Bonsai Obj
        """
        self.regLoss = 0.5 * (
            self.lZ * tf.square(tf.norm(tensor=self.bonsaiObj.Z))
            + self.lW * tf.square(tf.norm(tensor=self.bonsaiObj.W))
            + self.lV * tf.square(tf.norm(tensor=self.bonsaiObj.V))
            + self.lT * tf.square(tf.norm(tensor=self.bonsaiObj.T))
        )

        # Loss functions for classification.
        if self.bonsaiObj.numClasses > 2:
            if self.useMCHLoss is True:
                self.marginLoss = utils.multiClassHingeLoss(tf.transpose(a=logits), Y)
            else:
                self.marginLoss = utils.crossEntropyLoss(tf.transpose(a=logits), Y)
            loss = self.marginLoss + self.regLoss
        else:
            self.marginLoss = tf.reduce_mean(
                input_tensor=tf.nn.relu(1.0 - (2 * Y - 1) * tf.transpose(a=logits))
            )
            loss = self.marginLoss + self.regLoss

        return loss, self.marginLoss, self.regLoss

    def trainGraph(self):
        """
        Train Graph for the loss generated by Bonsai
        """
        return tf.keras.optimizers.Adam(
            learning_rate=self.learningRate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07,
            amsgrad=False,
            name="Adam",
        )

    def accuracy(self, logits, Y):
        """
        Accuracy Graph to evaluate accuracy when needed
        """

        if self.bonsaiObj.numClasses > 2:
            correctPrediction = tf.equal(
                tf.argmax(input=tf.transpose(a=logits), axis=1),
                tf.argmax(input=Y, axis=1),
            )
            accuracy = tf.reduce_mean(
                input_tensor=tf.cast(correctPrediction, tf.float32)
            )
        else:
            y_ = Y * 2 - 1
            correctPrediction = tf.multiply(tf.transpose(a=logits), y_)
            correctPrediction = tf.nn.relu(correctPrediction)
            correctPrediction = tf.math.ceil(tf.tanh(correctPrediction))
            accuracy = tf.reduce_mean(
                input_tensor=tf.cast(correctPrediction, tf.float32)
            )

        return accuracy

    def sparseTraining(self):
        """
        Set up for Sparse Retraining Functionality
        """
        self.__Wops = self.bonsaiObj.W.assign(self.__Wth)
        self.__Vops = self.bonsaiObj.V.assign(self.__Vth)
        self.__Zops = self.bonsaiObj.Z.assign(self.__Zth)
        self.__Tops = self.bonsaiObj.T.assign(self.__Tth)

        self.sparseRetrainGroup = tf.group(
            self.__Wops, self.__Vops, self.__Tops, self.__Zops
        )

    def runHardThrsd(self):
        """
        Function to run the IHT routine on Bonsai Obj
        """

        currW = self.bonsaiObj.W
        currV = self.bonsaiObj.V
        currZ = self.bonsaiObj.Z
        currT = self.bonsaiObj.T

        self._thrsdW = utils.hardThreshold(currW, self.sW)
        self._thrsdV = utils.hardThreshold(currV, self.sV)
        self._thrsdZ = utils.hardThreshold(currZ, self.sZ)
        self._thrsdT = utils.hardThreshold(currT, self.sT)

        self.bonsaiObj.W.assign(self._thrsdW)
        self.bonsaiObj.V.assign(self._thrsdV)
        self.bonsaiObj.Z.assign(self._thrsdZ)
        self.bonsaiObj.T.assign(self._thrsdT)

    def runSparseTraining(self):
        """
        Function to run the Sparse Retraining routine on Bonsai Obj
        """

        utils.updateSource(self._thrsdW, self.bonsaiObj.W)
        utils.updateSource(self._thrsdV, self.bonsaiObj.V)
        utils.updateSource(self._thrsdZ, self.bonsaiObj.Z)
        utils.updateSource(self._thrsdT, self.bonsaiObj.T)

    def assertInit(self):
        err = "sparsity must be between 0 and 1"
        assert self.sW >= 0 and self.sW <= 1, "W " + err
        assert self.sV >= 0 and self.sV <= 1, "V " + err
        assert self.sZ >= 0 and self.sZ <= 1, "Z " + err
        assert self.sT >= 0 and self.sT <= 1, "T " + err

    def saveParamsForSeeDot(self):
        """
        Function to save Parameter matrices into a given folder for SeeDot compiler
        """
        seeDotDir = "/home/cknorow/SeeDot/"

        if os.path.isdir(seeDotDir) is False:
            try:
                os.mkdir(seeDotDir)
            except OSError:
                print("Creation of the directory %s failed" % seeDotDir)

        np.savetxt(
            seeDotDir + "W",
            utils.restructreMatrixBonsaiSeeDot(
                self.bonsaiObj.W,
                self.bonsaiObj.numClasses,
                self.bonsaiObj.totalNodes,
            ),
            delimiter="\t",
        )
        np.savetxt(
            seeDotDir + "V",
            utils.restructreMatrixBonsaiSeeDot(
                self.bonsaiObj.V,
                self.bonsaiObj.numClasses,
                self.bonsaiObj.totalNodes,
            ),
            delimiter="\t",
        )
        np.savetxt(seeDotDir + "T", self.bonsaiObj.T, delimiter="\t")
        np.savetxt(seeDotDir + "Z", self.bonsaiObj.Z, delimiter="\t")
        np.savetxt(
            seeDotDir + "Sigma", np.array([self.bonsaiObj.sigma]), delimiter="\t"
        )

    def loadModel(self, currDir):
        """
        Load the Saved model and load it to the model using constructor
        Returns two dict one for params and other for hyperParams
        """
        paramDir = currDir + "/"
        paramDict = {}
        paramDict["W"] = np.load(paramDir + "W.npy")
        paramDict["V"] = np.load(paramDir + "V.npy")
        paramDict["T"] = np.load(paramDir + "T.npy")
        paramDict["Z"] = np.load(paramDir + "Z.npy")
        hyperParamDict = np.load(paramDir + "hyperParam.npy").item()

        return paramDict, hyperParamDict

    # Function to get aimed model size
    def getModelSize(self):
        """
        Function to get aimed model size
        """
        nnzZ, sizeZ, sparseZ = utils.countnnZ(self.bonsaiObj.Z, self.sZ)
        nnzW, sizeW, sparseW = utils.countnnZ(self.bonsaiObj.W, self.sW)
        nnzV, sizeV, sparseV = utils.countnnZ(self.bonsaiObj.V, self.sV)
        nnzT, sizeT, sparseT = utils.countnnZ(self.bonsaiObj.T, self.sT)

        totalnnZ = nnzZ + nnzT + nnzV + nnzW
        totalSize = sizeZ + sizeW + sizeV + sizeT
        hasSparse = sparseW or sparseV or sparseT or sparseZ
        return totalnnZ, totalSize, hasSparse

    def fit(self, batchSize, totalEpochs, Xtrain, Xtest, Ytrain, Ytest):
        """
        The Dense - IHT - Sparse Retrain Routine for Bonsai Training
        """
        errMsg = (
            "Dimension Mismatch, Y has to be [_, "
            + str(self.bonsaiObj.numClasses)
            + "]"
        )
        errCont = " numClasses are 1 in case of Binary case by design"
        assert (
            len(Ytrain.shape) == 2 and Ytrain.shape[1] == self.bonsaiObj.numClasses
        ), (errMsg + errCont)

        training_metrics = {
            "accuracy": [],
            "loss": [],
            "val_accuracy": [],
            "val_loss": [],
        }

        numIters = Xtrain.shape[0] / batchSize

        totalBatches = numIters * totalEpochs

        SigmaI = 1

        counter = 0
        if self.bonsaiObj.numClasses > 2:
            trimlevel = 15
        else:
            trimlevel = 5
        ihtDone = 0

        maxTestAcc = -10000
        maxTestAccEpoch = 0
        testAcc = 0

        if self.isDenseTraining is True:
            ihtDone = 1
            SigmaI = 1
            itersInPhase = 0

        "*" * 20
        for i in range(totalEpochs):

            if PRINT_PROGRESS_LOGS and i % 25 == 0:
                logger.debug(
                    {
                        "message": "Bonsai Epoch Number: " + str(i),
                        "log_type": "PID",
                        "UUID": self.pipeline_id,
                    }
                )

            """
            trainAcc -> For Classification, it is 'Accuracy'.
            """
            trainAcc = 0.0
            trainLoss = 0.0

            numIters = int(numIters)
            for j in range(numIters):

                if PRINT_PROGRESS_LOGS and counter == 0:

                    logger.debug(
                        {
                            "message": "Bonsai Dense Training Phase Started",
                            "log_type": "PID",
                            "UUID": self.pipeline_id,
                        }
                    )

                # Updating the indicator sigma
                if (
                    (counter == 0)
                    or (counter == int(totalBatches / 3.0))
                    or (counter == int(2 * totalBatches / 3.0))
                ) and (self.isDenseTraining is False):
                    SigmaI = 1
                    itersInPhase = 0

                elif itersInPhase % 100 == 0:
                    indices = np.random.choice(Xtrain.shape[0], 100)
                    batchX = Xtrain[indices, :]
                    batchY = Ytrain[indices, :]
                    batchY = np.reshape(batchY, [-1, self.bonsaiObj.numClasses])

                    Teval = self.bonsaiObj.T
                    Xcapeval = (
                        tf.matmul(self.bonsaiObj.Z, tf.transpose(batchX))
                        / self.bonsaiObj.projectionDimension
                    )

                    sum_tr = 0.0
                    for k in range(0, self.bonsaiObj.internalNodes):
                        sum_tr += np.sum(np.abs(np.dot(Teval[k], Xcapeval)))

                    if self.bonsaiObj.internalNodes > 0:
                        sum_tr /= 100 * self.bonsaiObj.internalNodes
                        sum_tr = 0.1 / sum_tr
                    else:
                        sum_tr = 0.1
                    sum_tr = min(
                        1000,
                        sum_tr
                        * (2 ** (float(itersInPhase) / (float(totalBatches) / 30.0))),
                    )

                    SigmaI = sum_tr

                itersInPhase += 1
                batchX = Xtrain[j * batchSize : (j + 1) * batchSize]
                batchY = Ytrain[j * batchSize : (j + 1) * batchSize]
                batchY = np.reshape(batchY, [-1, self.bonsaiObj.numClasses])
                ###########################################################
                # ## Starting Training Stuff Here`
                ###########################################################

                # Mini-batch training

                with tf.GradientTape() as T:
                    T.watch(self.bonsaiObj.W)
                    T.watch(self.bonsaiObj.T)
                    T.watch(self.bonsaiObj.V)
                    T.watch(self.bonsaiObj.Z)
                    logits, _ = self.bonsaiObj.feedfoward(batchX, SigmaI)
                    batchLoss, _, _ = self.loss(logits, batchY)

                # TODO FIGURE THIS OUT
                Grads = T.gradient(
                    batchLoss,
                    [
                        self.bonsaiObj.W,
                        self.bonsaiObj.V,
                        self.bonsaiObj.T,
                        self.bonsaiObj.Z,
                    ],
                )
                self.optimizer.apply_gradients(
                    zip(
                        Grads,
                        [
                            self.bonsaiObj.W,
                            self.bonsaiObj.V,
                            self.bonsaiObj.T,
                            self.bonsaiObj.Z,
                        ],
                    )
                )

                batchAcc = self.accuracy(logits, batchY)

                # Classification.
                trainAcc += batchAcc
                trainLoss += batchLoss

                # Training routine involving IHT and sparse retraining
                if (
                    counter >= int(totalBatches / 3.0)
                    and (counter < int(2 * totalBatches / 3.0))
                    and counter % trimlevel == 0
                    and self.isDenseTraining is False
                ):
                    ###################
                    self.runHardThrsd()
                    ########################
                    if ihtDone == 0:
                        logger.debug(
                            {
                                "message": "Bonsai IHT Phase Started ",
                                "log_type": "PID",
                                "UUID": self.pipeline_id,
                            }
                        )
                    ihtDone = 1
                elif (
                    ihtDone == 1
                    and counter >= int(totalBatches / 3.0)
                    and (counter < int(2 * totalBatches / 3.0))
                    and counter % trimlevel != 0
                    and self.isDenseTraining is False
                ) or (
                    counter >= int(2 * totalBatches / 3.0)
                    and self.isDenseTraining is False
                ):
                    ########################
                    self.runSparseTraining()
                    ##########################
                    if counter == int(2 * totalBatches / 3.0):
                        logger.debug(
                            {
                                "message": "Bonsai Sparse Retraining Phase Started",
                                "log_type": "PID",
                                "UUID": self.pipeline_id,
                            }
                        )
                counter += 1

            logger.debug(
                {
                    "message": "Bonsai"
                    + " Train accuracy: "
                    + str((trainAcc.numpy() / numIters).round(2))
                    + ", Train loss: "
                    + str((trainLoss.numpy() / numIters).round(2)),
                    "log_type": "PID",
                    "UUID": self.pipeline_id,
                }
            )
            training_metrics["loss"].append(float(trainLoss.numpy() / numIters))
            training_metrics["accuracy"].append(float(trainAcc.numpy() / numIters))

            oldSigmaI = SigmaI
            SigmaI = 1e9

            # This helps in direct testing instead of extracting the model out
            logits, _ = self.bonsaiObj.feedfoward(Xtest, SigmaI)
            testLoss, _, _ = self.loss(logits, Ytest)
            testAcc = self.accuracy(logits, Ytest)

            # self.saveParamsForSeeDot()

            if ihtDone == 0:
                maxTestAcc = -10000
                maxTestAccEpoch = i
            else:
                if maxTestAcc <= testAcc:
                    maxTestAccEpoch = i
                    maxTestAcc = testAcc

            logger.debug(
                {
                    "message": "Bonsai   Test accuracy {test_acc:.2},   Test Loss {test_loss:.2}".format(
                        test_acc=np.mean(testAcc).round(2),
                        test_loss=np.mean(testLoss).round(2),
                    ),
                    "log_type": "PID",
                    "UUID": self.pipeline_id,
                }
            )
            training_metrics["val_loss"].append(float(np.mean(testAcc)))
            training_metrics["val_accuracy"].append(float(np.mean(testLoss)))

            testAcc = testAcc
            maxTestAcc = maxTestAcc

            # logger.debug("MarginLoss + RegLoss: " + str(testLoss - regTestLoss) +
            #      " + " + str(regTestLoss) + " = " + str(testLoss) + "\n",
            #      )

            SigmaI = oldSigmaI

        # sigmaI has to be set to infinity to ensure
        # only a single path is used in inference
        SigmaI = 1e9
        # logger.debug("\nNon-Zero : " + str(self.getModelSize()[0]) + " Model Size: " +
        #      str(float(self.getModelSize()[1]) / 1024.0) + " KB hasSparse: " +
        #      str(self.getModelSize()[2]) + "\n")

        logger.debug(
            {
                "message": "Bonsai For Classification, Maximum Test accuracy at compressed"
                + " model size(including early stopping): "
                + str(maxTestAcc)
                + " at Epoch: "
                + str(maxTestAccEpoch + 1)
                + "\nFinal Test"
                + " Accuracy: "
                + str(testAcc),
                "log_type": "PID",
                "UUID": self.pipeline_id,
            }
        )

        training_metrics.update(
            {
                "max_test_accuracy": float(maxTestAcc),
                "max_test_accuracy_epoch": float(maxTestAccEpoch),
                "model_size": float(self.getModelSize()[1]) / 1024.0,
                "KB_has_sparse": float(self.getModelSize()[2]),
            }
        )

        return training_metrics
