---
- model: library.transform
  pk: 1
  fields:
    uuid: c45861f7-3dc8-4c74-9828-381b08de11fa
    name: Trim
    function_in_file: trimseries
    input_contract:
      - type: DataFrame
        name: input_data
      - type: int
        name: number_of_samples
      - element_type: str
        type: list
        name: group_columns
    output_contract:
      - type: DataFrame
        name: output_data
    description: Truncate all event data streams to the same number of samples.
    path: "../library/core_functions/tr_trimseries.py"
    type: Transform
    subtype: Segment
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 2
  fields:
    uuid: f8dab7eb-6c85-4be6-9a2b-f62bc2ef725f
    name: Detect Cycles
    function_in_file: detect_cycles_by_group
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: sensor_name
      - type: int
        name: number_of_tops_to_check
      - type: str
        name: group_column
      - type: int
        name: pad_cycles
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Apply cycle detection to a multi-cycle data set using cumulative
      sum\n    integration; create a new  column named 'Cycle'. Pad all cycles of
      the\n    same group to be consistent length. Applies to multiple groups of data.\n
      \   "
    path: "../library/core_functions/sg_cycledetection.py"
    type: Segmenter
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 3
  fields:
    uuid: a9245a84-eff9-4d0c-a441-1f9502655d8c
    name: Standardize
    function_in_file: standardize
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: passthrough_columns
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This function standardize each feature column in a dataframe
      by removing\n    the mean and scaling to unit variance. Data in the `passthrough_columns`
      are not modified.\n\n    Args:\n        input_data: input dataframe\n        passthrough_columns:
      These are all those columns on which\n                            standardization
      and scaling are not applicable\n\n\n    Returns:\n        DataFrame containing
      standardized values.\n\n    Examples:\n        >>> import pandas as pd\n        >>>
      import numpy as np\n        >>> Fs = 10. # sample rate\n        >>> f = 1./2
      # the frequency of the signal\n        >>> df = pd.DataFrame({ 'Subject': ['s01']
      * 10,\n                                'Class': ['Crawling'] * 10 ,\n                                'Rep':
      [1] * 10})\n        >>> sinwave = [np.sin(2*np.pi*f * (i/Fs)) for i in np.arange(Fs)]\n
      \       >>> df['accelx'] = sinwave\n        >>> df\n            out:\n                      Class
      \ Rep Subject    accelx\n                0  Crawling    1     s01  0.000000\n
      \               1  Crawling    1     s01  0.309017\n                2  Crawling
      \   1     s01  0.587785\n                3  Crawling    1     s01  0.809017\n
      \               4  Crawling    1     s01  0.951057\n                5  Crawling
      \   1     s01  1.000000\n                6  Crawling    1     s01  0.951057\n
      \               7  Crawling    1     s01  0.809017\n                8  Crawling
      \   1     s01  0.587785\n                9  Crawling    1     s01  0.309017\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force = True)\n        >>> dsk.pipeline.add_transform('Standardize',\n                            params={'passthrough_columns':['Subject',
      'Class', 'Rep']})\n        >>> results, stats = dsk.pipeline.execute()\n        >>>
      print results\n            out:\n                    Class  Rep Subject    accelx\n
      \               0  Crawling    1     s01 -1.983091\n                1  Crawling
      \   1     s01 -1.012497\n                2  Crawling    1     s01 -0.136912\n
      \               3  Crawling    1     s01  0.557957\n                4  Crawling
      \   1     s01  1.004089\n                5  Crawling    1     s01  1.157816\n
      \               6  Crawling    1     s01  1.004089\n                7  Crawling
      \   1     s01  0.557957\n                8  Crawling    1     s01 -0.136912\n
      \               9  Crawling    1     s01 -1.012497\n    "
    path: "../library/core_functions/tr_scale.py"
    type: Transform
    subtype: Feature
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 4
  fields:
    uuid: a403d3c6-6fe2-419a-9bb9-a7e2ac661cd6
    name: Scale Factor
    function_in_file: tr_segment_scale_factor
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: input_columns
      - type: float
        name: scale_factor
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Convert data to integer values and scale columns by scale_factor.\n
      \   Note: On the device streaming data is currently int16. This function converst
      data\n    to integer values and scales by scale factor. Be careful about losing
      resolution here.\n\n    Args:\n        input_data: DataFrame\n        input_columns:
      list of column names\n        scale_factor: float; number by which `input_columns`
      are divided.\n\n    Returns:\n        DataFrame\n\n    Examples:\n        >>>
      dsk.pipeline.reset()\n        >>> df = dsk.datasets.load_activity_raw_toy()\n
      \       >>> print df\n            out:\n                   Subject     Class
      \ Rep  accelx  accely  accelz\n                0      s01  Crawling    1     377
      \    569    4019\n                1      s01  Crawling    1     357     594
      \   4051\n                2      s01  Crawling    1     333     638    4049\n
      \               3      s01  Crawling    1     340     678    4053\n                4
      \     s01  Crawling    1     372     708    4051\n                5      s01
      \ Crawling    1     410     733    4028\n                6      s01  Crawling
      \   1     450     733    3988\n                7      s01  Crawling    1     492
      \    696    3947\n                8      s01  Crawling    1     518     677
      \   3943\n                9      s01  Crawling    1     528     695    3988\n
      \               10     s01  Crawling    1      -1    2558    4609\n                11
      \    s01   Running    1     -44   -3971     843\n                12     s01
      \  Running    1     -47   -3982     836\n                13     s01   Running
      \   1     -43   -3973     832\n                14     s01   Running    1     -40
      \  -3973     834\n                15     s01   Running    1     -48   -3978
      \    844\n                16     s01   Running    1     -52   -3993     842\n
      \               17     s01   Running    1     -64   -3984     821\n                18
      \    s01   Running    1     -64   -3966     813\n                19     s01
      \  Running    1     -66   -3971     826\n                20     s01   Running
      \   1     -62   -3988     827\n                21     s01   Running    1     -57
      \  -3984     843\n        >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n
      \       >>> dsk.pipeline.add_transform('Scale Factor',\n                            params={'scale_factor':4096.,\n
      \                           'input_columns':['accely']})\n        >>> results,
      stats = dsk.pipeline.execute()\n        >>> print results\n            out:\n
      \                      Class  Rep Subject  accelx    accely  accelz\n                0
      \  Crawling    1     s01     377  0.138916    4019\n                1   Crawling
      \   1     s01     357  0.145020    4051\n                2   Crawling    1     s01
      \    333  0.155762    4049\n                3   Crawling    1     s01     340
      \ 0.165527    4053\n                4   Crawling    1     s01     372  0.172852
      \   4051\n                5   Crawling    1     s01     410  0.178955    4028\n
      \               6   Crawling    1     s01     450  0.178955    3988\n                7
      \  Crawling    1     s01     492  0.169922    3947\n                8   Crawling
      \   1     s01     518  0.165283    3943\n                9   Crawling    1     s01
      \    528  0.169678    3988\n                10  Crawling    1     s01      -1
      \ 0.624512    4609\n                11   Running    1     s01     -44 -0.969482
      \    843\n                12   Running    1     s01     -47 -0.972168     836\n
      \               13   Running    1     s01     -43 -0.969971     832\n                14
      \  Running    1     s01     -40 -0.969971     834\n                15   Running
      \   1     s01     -48 -0.971191     844\n                16   Running    1     s01
      \    -52 -0.974854     842\n                17   Running    1     s01     -64
      -0.972656     821\n                18   Running    1     s01     -64 -0.968262
      \    813\n                19   Running    1     s01     -66 -0.969482     826\n
      \               20   Running    1     s01     -62 -0.973633     827\n                21
      \  Running    1     s01     -57 -0.972656     843\n    "
    path: "../library/core_functions/tr_segment_scale_factor.py"
    type: Transform
    subtype: Segment
    has_c_version: true
    c_file_name: tr_segment_scale_factor.c
    core: true
    dcl_executable: false
    c_function_name: tr_segment_scale_factor
- model: library.transform
  pk: 8
  fields:
    uuid: 6ec7b47b-5d8c-4c46-a3c0-f1d65c36a781
    name: Clipper
    function_in_file: remove_first_and_last_cycle
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: group_column
      - type: str
        name: cycle_column
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    For each group (e.g. subject-run), remove the first and last
      cycle.\n    "
    path: "../library/core_functions/sg_cycledetection.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 11
  fields:
    uuid: bae6cd33-0590-42e3-b910-840c8c452851
    name: Standardize and Scale
    function_in_file: standardize_scale
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: passthrough_columns
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Two steps are performed here: Step 1. Standardize each feature
      column by\n    removing the mean and scaling to unit variance.  In step 2, the
      entire\n    column is scaled to fit within the range 0 to 255.\n\n    Args:\n
      \       input_data: input dataframe\n        passthrough_columns: These are
      all those columns on which\n                            standardization and
      scaling are not applicable\n\n    Returns:\n        DataFrame containing standardized
      and scaled values\n\n    Examples:\n        >>> import pandas as pd\n        >>>
      import numpy as np\n        >>> Fs = 10. # sample rate\n        >>> f = 1./2
      # the frequency of the signal\n        >>> df = pd.DataFrame({ 'Subject': ['s01']
      * 10, 'Class': ['Crawling'] * 10 , 'Rep': [1] * 10})\n        >>> sinwave =
      [np.sin(2*np.pi*f * (i/Fs)) for i in np.arange(Fs)]\n        >>> df['accelx']
      = sinwave\n        >>> df\n            out:\n                      Class  Rep
      Subject    accelx\n                0  Crawling    1     s01  0.000000\n                1
      \ Crawling    1     s01  0.309017\n                2  Crawling    1     s01
      \ 0.587785\n                3  Crawling    1     s01  0.809017\n                4
      \ Crawling    1     s01  0.951057\n                5  Crawling    1     s01
      \ 1.000000\n                6  Crawling    1     s01  0.951057\n                7
      \ Crawling    1     s01  0.809017\n                8  Crawling    1     s01
      \ 0.587785\n                9  Crawling    1     s01  0.309017\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force = True)\n        >>> dsk.pipeline.add_transform('Standardize and Scale',\n
      \                   params={'passthrough_columns':['Subject', 'Class', 'Rep']})\n
      \       >>> results, stats = dsk.pipeline.execute()\n        >>> print results\n
      \           out:\n                      Class  Rep Subject  accelx\n                0
      \ Crawling    1     s01       0\n                1  Crawling    1     s01      78\n
      \               2  Crawling    1     s01     149\n                3  Crawling
      \   1     s01     206\n                4  Crawling    1     s01     242\n                5
      \ Crawling    1     s01     255\n                6  Crawling    1     s01     242\n
      \               7  Crawling    1     s01     206\n                8  Crawling
      \   1     s01     149\n                9  Crawling    1     s01      78\n    "
    path: "../library/core_functions/tr_scale.py"
    type: Transform
    subtype: Feature
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 12
  fields:
    uuid: 665661e8-b7d4-4f6c-afea-80c2893fd0d9
    name: Min Max Scale
    function_in_file: min_max_scale
    input_contract:
      - type: DataFrame
        name: input_data
      - type: list
        name: passthrough_columns
      - default: 0
        type: numeric
        name: min_bound
      - default: 255
        type: numeric
        name: max_bound
      - default: {}
        type: dict
        name: feature_min_max_parameters
    output_contract:
      - type: DataFrame
        name: df_out
      - type: list
        name: feature_min_max_parameters
        persist: true
    description:
      "\n    Normalize and scale data to integer values between min_bound
      and max_bound,\n    while leaving specified passthrough columns unscaled. This
      operates on each\n    feature column separately and  saves min/max data for
      the knowledgepack to\n    use for recognition of new feature vectors.\n\n    Args:\n
      \       input_data: Dataframe that needs to be normalized\n        passthrough_columns:
      List of column names. If all columns need to be normalized,\n            then
      this value will be an empty list\n        min_bound: min value in the output\n
      \       max_bound: max value in the output\n        feature_min_max_parameters:
      Dictionary of 'maximums' and 'minimums'.\n            If a non-empty dictionary
      is passed as parameter, the minimum and maximum value will\n            be calculated
      based on the 'maximums' and 'minimums' in the dictionary. If the value\n            of
      this parameter is {}, then a new min-max value for each feature is\n            calculated.\n\n
      \   Returns:\n        The scaled dataframe and minimums and maximums for each
      feature.\n        If 'feature_min_max_parameters' values is {} then the minimums\n
      \       and maximums for each feature are calculated based on the data passed.\n\n
      \   Examples:\n        >>> from pandas import DataFrame\n        >>> df = DataFrame([[-3,
      6, 5], [3, 7, 8], [0, 6, 3],\n                            [-2, 8, 7], [2, 9,
      6]],\n                            columns=['feature1', 'feature2', 'feature3'])\n
      \       >>> df['Subject'] = 's01'\n        >>> df\n            Out:\n               feature1
      \ feature2  feature3 Subject\n            0        -3         6         5     s01\n
      \           1         3         7         8     s01\n            2         0
      \        6         3     s01\n            3        -2         8         7     s01\n
      \           4         2         9         6     s01\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force = True)\n        >>>
      dsk.pipeline.add_transform('Min Max Scale',\n                params={'passthrough_columns':['Subject'],\n
      \                       'min_bound' : 0, 'max_bound' : 255})\n            Out:\n
      \                 Subject  feature1  feature2  feature3\n                0     s01
      \        0         0       101\n                1     s01       254        84
      \      254\n                2     s01       127         0         0\n                3
      \    s01        42       169       203\n                4     s01       212
      \      254       152\n\n        Passing min-max parameter as arguments\n\n        >>>
      my_min_max_param = {'maximums': {'feature1': 30,\n                                            'feature2':
      100,\n                                            'feature3': 500},\n                                'minimums':
      {'feature1': 0,\n                                            'feature2': 0,\n
      \                                           'feature3': -100}}\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force = True)\n        >>> dsk.pipeline.add_transform('Min Max Scale',\n                            params={'passthrough_columns':['Subject'],\n
      \                                   'min_bound' : 0,\n                                    'max_bound'
      : 255,\n                                    'feature_min_max_parameters': my_min_max_param})\n
      \       >>> results, stats = dsk.pipeline.execute()\n        >>> print results,
      stats\n            Out:\n                    feature1  feature2  feature3 Subject\n
      \                0         0        15        44     s01\n                 1
      \       25        17        45     s01\n                 2         0        15
      \       43     s01\n                 3         0        20        45     s01\n
      \                4        16        22        45     s01\n    "
    path: "../library/core_functions/tr_scale.py"
    type: Transform
    subtype: Feature Vector
    has_c_version: true
    c_file_name: tr_min_max_scale.c
    core: true
    dcl_executable: false
    c_function_name: min_max_scale
- model: library.transform
  pk: 13
  fields:
    uuid: 42906860-4ce7-41c9-a802-2b766c67210c
    name: Strip
    function_in_file: tr_segment_strip
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: group_columns
      - element_type: str
        type: list
        name: input_columns
      - type: str
        name: type
        options:
          - name: mean
          - name: min
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Remove each signal's mean or min from its values, while leaving
      specified\n    passthrough columns  unmodified. This function transforms a dataframe\n
      \   in such a way that the entire signal is shifted towards 'mean' or 'min'.\n\n
      \   Args:\n        input_data: The pandas dataframe containing the data\n        group_columns:
      The list of columns on which the grouping is to be\n            done. Each group
      constitutes one event.\n        input_columns: The list of columns names to
      use.\n        type: Possible values are 'mean' or 'min'.\n\n    Returns:\n        If
      type = 'mean', mean of each qualified columns is calculated. Each value in a
      column\n        will be subtracted by column-mean.\n\n    Examples:\n        >>>
      from pandas import DataFrame\n        >>> df = DataFrame([[3, 3], [4, 5], [5,
      7], [4, 6], [3, 1],\n                             [3, 1], [4, 3], [5, 5], [4,
      7], [3, 6]],\n                             columns=['accelx', 'accely'])\n        >>>
      df['Subject'] = 's01'\n        >>> df['Rep'] = [0] * 5 + [1] * 5\n        >>>
      df\n            Out:\n               accelx  accely Subject  Rep\n            0
      \      3       3     s01    0\n            1       4       5     s01    0\n
      \           2       5       7     s01    0\n            3       4       6     s01
      \   0\n            4       3       1     s01    0\n            5       3       1
      \    s01    1\n            6       4       3     s01    1\n            7       5
      \      5     s01    1\n            8       4       7     s01    1\n            9
      \      3       6     s01    1\n\n        >>> dsk.pipeline.reset()\n        >>>
      dsk.pipeline.set_input_data('test_data', df, force = True)\n        >>> dsk.pipeline.add_transform('Strip',\n
      \                               params={'group_columns':['Subject', 'Rep'],\n
      \                                       'input_columns' : ['accelx','accley'],\n
      \                                       'type' : 'mean'})\n        >>> results,
      stats = dsk.pipeline.execute()\n        >>> print results\n           Out:\n
      \                  Rep Subject  accelx  accely\n                0    0     s01
      \   -0.8    -1.4\n                1    0     s01     0.2     0.6\n                2
      \   0     s01     1.2     2.6\n                3    0     s01     0.2     1.6\n
      \               4    0     s01    -0.8    -3.4\n                5    1     s01
      \   -0.8    -3.4\n                6    1     s01     0.2    -1.4\n                7
      \   1     s01     1.2     0.6\n                8    1     s01     0.2     2.6\n
      \               9    1     s01    -0.8     1.6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force = True)\n        >>>
      dsk.pipeline.add_transform('Strip',\n                            params={'group_columns':['Subject',
      'Rep'],\n                                    'input_columns' : ['accelx','accely'],\n
      \                                   'type' : 'min'})\n        >>> results, stats
      = dsk.pipeline.execute()\n        >>> print results\n            Out:\n                     Rep
      Subject  accelx  accely\n                0    0     s01     0.0     2.0\n                1
      \   0     s01     1.0     4.0\n                2    0     s01     2.0     6.0\n
      \               3    0     s01     1.0     5.0\n                4    0     s01
      \    0.0     0.0\n                5    1     s01     0.0     0.0\n                6
      \   1     s01     1.0     2.0\n                7    1     s01     2.0     4.0\n
      \               8    1     s01     1.0     6.0\n                9    1     s01
      \    0.0     5.0\n    "
    path: "../library/core_functions/tr_segment_strip.py"
    type: Transform
    subtype: Segment
    has_c_version: true
    c_file_name: tr_segment_strip.c
    core: true
    dcl_executable: false
    c_function_name: tr_segment_strip
- model: library.transform
  pk: 14
  fields:
    uuid: 11897939-8286-4f46-9368-07866418e7a5
    name: Normalize
    function_in_file: normalize
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: passthrough_columns
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Scale each feature vector to between -1 and 1 by dividing
      each feature\n    in a feature vector by the absolute maximum value in that
      feature vector.\n\n    Args:\n        input_data: The dataframe to be normalized\n\n
      \       passthrough_columns: List of column names will remain untouched\n                            during
      this process\n\n    Returns:\n        dataframe: Normalized dataframe\n\n    Examples:\n
      \       >>> from pandas import DataFrame\n        >>> df = DataFrame([[3, 3],
      [4, 5], [5, 7], [4, 6], [3, 1],\n                    [3, 1], [4, 3], [5, 5],
      [4, 7], [3, 6]],\n                    columns=['accelx', 'accely'])\n        >>>
      df['Subject'] = 's01'\n        >>> df['Rep'] = [0] * 5 + [1] * 5\n        >>>
      df\n            Out:\n               accelx  accely Subject  Rep\n            0
      \      3       3     s01    0\n            1       4       5     s01    0\n
      \           2       5       7     s01    0\n            3       4       6     s01
      \   0\n            4       3       1     s01    0\n            5       3       1
      \    s01    1\n            6       4       3     s01    1\n            7       5
      \      5     s01    1\n            8       4       7     s01    1\n            9
      \      3       6     s01    1\n        >>> dsk.pipeline.reset()\n        >>>
      dsk.pipeline.set_input_data('testn', df, data_columns=['accelx', 'accely'],
      group_columns=['Subject','Rep'])\n        >>> dsk.pipeline.add_transform('Normalize')\n
      \       >>> r, s = dsk.pipeline.execute()\n        >>> r\n            Out:\n
      \                 Rep Subject  accelx    accely\n                    0   0   s01
      1.000000    1.000000\n                    1   0   s01 0.800000    1.000000\n
      \                   2   0   s01 0.714286    1.000000\n                    3
      \  0   s01 0.666667    1.000000\n                    4   0   s01 1.000000    0.333333\n
      \                   5   1   s01 1.000000    0.333333\n                    6
      \  1   s01 1.000000    0.750000\n                    7   1   s01 1.000000    1.000000\n
      \                   8   1   s01 0.571429    1.000000\n                    9
      \  1   s01 0.500000    1.000000\n\n\n    "
    path: "../library/core_functions/tr_scale.py"
    type: Transform
    subtype: Feature Vector
    has_c_version: true
    c_file_name: tr_normalize.c
    core: true
    dcl_executable: false
    c_function_name: normalize
- model: library.transform
  pk: 16
  fields:
    uuid: c53f44d8-e4b3-4bbe-b137-855042f6ecde
    name: Quantize254
    function_in_file: quantize_254
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: passthrough_columns
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Scalar quantization of a normalized dataframe to integers
      between 0 and 254.\n    This step should only be applied after features have
      been normalized to the\n    range [-1, 1]. This quantization method does not
      require any feature-specific\n    statistics to be saved to the knowledgepack.\n\n
      \   Args:\n        input_data: The dataframe to be quantized\n\n        passthrough_columns:
      List of column names will remain untouched\n            during this process\n\n
      \   Returns:\n        dataframe: quantized dataframe\n    "
    path: "../library/core_functions/tr_scale.py"
    type: Transform
    subtype: Feature Vector
    has_c_version: true
    c_file_name: tr_quantize254.c
    core: true
    dcl_executable: false
    c_function_name: quantize_254
- model: library.transform
  pk: 19
  fields:
    uuid: 0755b017-1ebd-4719-95bf-62fe6b16adc4
    name: Downsampler
    function_in_file: downsampler
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the transform
      - type: int
        name: new_length
    output_contract:
      - type: DataFrame
        name: data_out
    description:
      "\n    Downsample the entire dataframe into a dataframe of size `new_length`.\n\n
      \   Args:\n        input_data: dataframe\n        columns: List of columns to
      be downsampled\n        group_columns (a list): List of columns on which grouping
      is to be done.\n                             Each group will go through downsampling
      one at a time\n        new_length: integer; Downsampled length\n\n    Returns:\n
      \       DataFrame: The downsampled dataframe.\n\n    Examples:\n        >>>
      from pandas import DataFrame\n        >>> df = DataFrame([[3, 3], [4, 5], [5,
      7], [4, 6], [3, 1], \n                            [3, 1], [4, 3], [5, 5], [4,
      7], [3, 6]], \n                            columns=['accelx', 'accely'])\n        >>>
      df\n        Out:\n               accelx  accely\n            0       3       3\n
      \           1       4       5\n            2       5       7\n            3
      \      4       6\n            4       3       1\n            5       3       1\n
      \           6       4       3\n            7       5       5\n            8
      \      4       7\n            9       3       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force = True)\n        >>>
      dsk.pipeline.add_transform('Downsampler', params={'group_columns':[],                                                            'columns'
      : ['accelx', 'accely'],                                                            'new_length'
      : 5 })\n        >>> results, stats = dsk.pipeline.execute()\n        >>> print
      results\n            Out:\n                    accelx  accely\n                0
      \    3.5     4.0\n                1     4.5     6.5\n                2     3.0
      \    1.0\n                3     4.5     4.0\n                4     3.5     6.5\n
      \   "
    path: "../library/core_functions/tr_downsampler.py"
    type: Transform
    subtype: Segment
    has_c_version: false
    c_file_name: downsampler.c
    core: true
    dcl_executable: false
    c_function_name: downsampler
- model: library.transform
  pk: 20
  fields:
    uuid: 9b577546-939a-4317-9456-8f5f4ba86237
    name: Mean
    function_in_file: stats_mean
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the arithmetic mean of each column in `columns` in
      the dataframe.\n\n    Args:\n        input_data (DataFrame) : input data as
      pandas dataframe\n        columns:  list of columns on which to apply the feature
      generator\n        group_columns: List of column names for grouping\n        **kwargs:\n\n
      \   Returns:\n        DataFrame: Returns data frame containing mean values of
      each specified column.\n\n    Examples:\n        >>> import pandas as pd\n        >>>
      df = pd.DataFrame([[-3, 6, 5], [3, 7, 8],\n                               [0,
      6, 3], [-2, 8, 7],\n                               [2, 9, 6]], columns= ['accelx',
      'accely', 'accelz'])\n        >>> df\n            out:\n               accelx
      \ accely  accelz\n            0      -3       6       5\n            1       3
      \      7       8\n            2       0       6       3\n            3      -2
      \      8       7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Mean\"],\n                 params = {\"group_columns\":
      []},\n                 function_defaults={\"columns\":['accelx', 'accely', 'accelz']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                   accelxMean  accelyMean  accelzMean\n            0
      \        0.0         7.2         5.8\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_mean.c
    core: true
    dcl_executable: false
    c_function_name: stats_mean
- model: library.transform
  pk: 21
  fields:
    uuid: 75c82e54-41a5-4cef-ae2d-4d4c8c4adb71
    name: Median
    function_in_file: stats_median
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    The median of a vector V with N items, is the middle value
      of a sorted\n    copy of V (V_sorted). When N is even, it is the average of
      the two\n    middle values in V_sorted.\n\n    Args:\n        input_data (DataFrame)
      : input data as pandas dataframe\n        columns:  list of columns on which
      to apply the feature generator\n        group_columns: List of column names
      for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame: Returns
      data frame with median of each specified column.\n\n    Examples:\n        >>>
      import pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0,
      6, 3],\n                                [-2, 8, 7], [2, 9, 6]],\n                               columns=
      ['accelx', 'accely', 'accelz'])\n        >>> df\n            out:\n               accelx
      \ accely  accelz\n            0      -3       6       5\n            1       3
      \      7       8\n            2       0       6       3\n            3      -2
      \      8       7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Median\"],\n                 params =
      {\"group_columns\": []},\n                 function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n               accelxMedian  accelyMedian  accelzMedian\n
      \           0           0.0           7.0           6.0\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_median.c
    core: true
    dcl_executable: false
    c_function_name: stats_median
- model: library.transform
  pk: 22
  fields:
    uuid: 9395f2a1-cfa2-4eb4-848e-50cce437c0a8
    name: Standard Deviation
    function_in_file: stats_stdev
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    The standard deviation of a vector V with N items, is the
      measure of spread\n    of the distribution. The standard deviation is the square
      root of the average     of the squared deviations from the mean, i.e., std =
      sqrt(mean(abs(x - x.mean())**2)).\n\n    Args:\n        input_data (DataFrame)\n
      \       columns:  list of columns on which to apply the feature generator\n
      \       group_columns: List of column names for grouping\n        **kwargs:\n\n
      \   Returns:\n        DataFrame: Returns data frame with standard deviation
      of each specified column.\n\n    Examples:\n        >>> import pandas as pd\n
      \       >>> df = pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0, 6, 3],\n                                [-2,
      8, 7], [2, 9, 6]],\n                                columns= ['accelx', 'accely',
      'accelz'])\n        >>> df\n            out:\n               accelx  accely
      \ accelz\n            0      -3       6       5\n            1       3       7
      \      8\n            2       0       6       3\n            3      -2       8
      \      7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Standard Deviation\"],\n                params
      = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelxStd  accelyStd
      \ accelzStd\n            0   2.280351    1.16619   1.720465\n\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_stdev.c
    core: true
    dcl_executable: false
    c_function_name: stats_stdev
- model: library.transform
  pk: 23
  fields:
    uuid: 6e2cdccc-9b47-499c-8c19-41a338161579
    name: Skewness
    function_in_file: stats_skewness
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    The skewness is the measure of asymmetry of the distribution
      of a variable\n    about its mean. The skewness value can be positive, negative,
      or even undefined.\n    A positive skew indicates that the tail on the right
      side is fatter than the left.\n    A negative value indicates otherwise.\n\n
      \   Args:\n        input_data (DataFrame) : input data as pandas dataframe\n
      \       columns:  list of columns on which to apply the feature generator\n
      \       group_columns: List of column names for grouping\n        **kwargs:\n\n
      \   Returns:\n        DataFrame: Returns data frame with skewness of each specified
      column.\n\n    Examples:\n        >>> import pandas as pd\n        >>> df =
      pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0, 6, 3],\n                              [-2,
      8, 7], [2, 9, 6]],\n                              columns= ['accelx', 'accely',
      'accelz'])\n        >>> df\n            out:\n               accelx  accely
      \ accelz\n            0      -3       6       5\n            1       3       7
      \      8\n            2       0       6       3\n            3      -2       8
      \      7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Skewness\"],\n                params =
      {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelxSkew  accelySkew
      \ accelzSkew\n            0         0.0    0.363173    -0.39587\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_skewness.c
    core: true
    dcl_executable: false
    c_function_name: stats_skewness
- model: library.transform
  pk: 24
  fields:
    uuid: 0954c2a5-fae7-46d5-b50a-94225b756295
    name: Kurtosis
    function_in_file: stats_kurtosis
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Kurtosis is the degree of 'peakedness' or 'tailedness' in
      the distribution and\n    is related to the shape. A high Kurtosis portrays
      a chart with fat tail and\n    peaky distribution, whereas a low Kurtosis corresponds
      to the skinny tails and\n    the distribution is concentrated towards the mean.
      Kurtosis is calculated using Fisher's method.\n\n    Args:\n        input_data
      (DataFrame) : input data as pandas dataframe\n        columns:  list of columns
      on which to apply the feature generator\n        group_columns: List of column
      names for grouping\n        **kwargs:\n    Returns:\n        DataFrame: Returns
      data frame with Kurtosis of each specified column.\n                    If all
      values are equal, return -3 for Fishers mrthod.\n\n    Examples:\n        >>>
      import pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0,
      6, 3],\n                                    [-2, 8, 7], [2, 9, 6]],\n                                    columns=
      ['accelx', 'accely', 'accelz'])\n        >>> df\n            out:\n               accelx
      \ accely  accelz\n            0      -3       6       5\n            1       3
      \      7       8\n            2       0       6       3\n            3      -2
      \      8       7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Kurtosis\"],\n                 params
      = {\"group_columns\": []},\n                 function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelxKurtosis  accelyKurtosis
      \ accelzKurtosis\n            0       -1.565089       -1.371972       -1.005478\n
      \   "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_kurtosis.c
    core: true
    dcl_executable: false
    c_function_name: stats_kurtosis
- model: library.transform
  pk: 25
  fields:
    uuid: a8115430-de1d-40ac-b06a-efca365b48c3
    name: Interquartile Range
    function_in_file: stats_iqr
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    The IQR (inter quartile range) of a vector V with N items,
      is the\n    difference between  the 75th percentile and 25th percentile value.\n\n
      \   Args:\n        input_data (DataFrame) : input data as pandas dataframe\n
      \       columns:  list of columns on which to apply the feature generator\n
      \       group_columns: List of column names for grouping\n        **kwargs:\n\n
      \   Returns:\n        DataFrame: Returns dataframe with IQR of each specified
      column.\n\n    Examples:\n        >>> import pandas as pd\n        >>> df =
      pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0, 6, 3],\n                                 [-2,
      8, 7], [2, 9, 6]],\n                                 columns= ['accelx', 'accely',
      'accelz'])\n        >>> df\n            out:\n               accelx  accely
      \ accelz\n            0      -3       6       5\n            1       3       7
      \      8\n            2       0       6       3\n            3      -2       8
      \      7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Interquartile Range\"],\n                        params
      = {\"group_columns\": []},\n                        function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelxIQR  accelyIQR
      \ accelzIQR\n            0        4.0        2.0        2.0\n\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_iqr.c
    core: true
    dcl_executable: false
    c_function_name: stats_iqr
- model: library.transform
  pk: 26
  fields:
    uuid: f1cfa5b2-e4bb-4033-9482-c870a898f449
    name: 25th Percentile
    function_in_file: stats_pct025
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the 25th percentile of each column in 'columns' in
      the dataframe.\n    A q-th percentile of a vector V of length N is the q-th
      ranked value in\n    a sorted copy of V. If the normalized ranking doesn't match
      the q exactly,\n    interpolation is done on two nearest values.\n\n    Args:\n
      \       input_data (DataFrame) : input data as pandas dataframe\n        columns:
      \ list of columns on which to apply the feature generator\n        group_columns:
      List of column names for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame:
      Returns 25-th percentile of each specified column.\n\n    Examples:\n        >>>
      import pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0,
      6, 3],]\n                                [-2, 8, 7], [2, 9, 6]],\n                                columns=
      ['accelx', 'accely', 'accelz'])\n        >>> df\n            out:\n               accelx
      \ accely  accelz\n            0      -3       6       5\n            1       3
      \      7       8\n            2       0       6       3\n            3      -2
      \      8       7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"25th Percentile\"],\n                    params
      = {\"group_columns\": []},\n                    function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelx25Percentile
      \ accely25Percentile  accelz25Percentile\n            0                -2.0
      \                6.0                 5.0\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_pct025.c
    core: true
    dcl_executable: false
    c_function_name: stats_pct025
- model: library.transform
  pk: 27
  fields:
    uuid: 11f10f1c-97e1-49f7-8e96-e30a24ea2047
    name: 75th Percentile
    function_in_file: stats_pct075
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the 75th percentile of each column in 'columns' in
      the dataframe.\n    A q-th percentile of a vector V of length N is the q-th
      ranked value in a\n    sorted copy of V. If the normalized ranking doesn't match
      the q exactly,\n    interpolation is done on two nearest values.\n\n    Args:\n
      \       input_data (DataFrame) : input data as pandas dataframe\n        columns:
      \ list of columns on which to apply the feature generator\n        group_columns:
      List of column names for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame:
      Returns data frame with 75th percentile of each specified column.\n\n    Examples:\n
      \       >>> import pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5],
      [3, 7, 8], [0, 6, 3],\n                               [-2, 8, 7], [2, 9, 6]],\n
      \                              columns= ['accelx', 'accely', 'accelz'])\n        >>>
      df\n            out:\n               accelx  accely  accelz\n            0      -3
      \      6       5\n            1       3       7       8\n            2       0
      \      6       3\n            3      -2       8       7\n            4       2
      \      9       6\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"75th Percentile\"],\n
      \               params = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelx75Percentile
      \ accely75Percentile  accelz75Percentile\n            0                 2.0
      \                8.0                 7.0\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_pct075.c
    core: true
    dcl_executable: false
    c_function_name: stats_pct075
- model: library.transform
  pk: 28
  fields:
    uuid: a010d605-8ed4-4fc9-8c0b-feb72b0da23d
    name: 100th Percentile
    function_in_file: stats_pct100
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the 100th percentile of each column in 'columns'
      in the dataframe.\n    A 100th percentile of a vector V the maximum value in
      V.\n\n    Args:\n        input_data (DataFrame) : input data as pandas dataframe\n
      \       columns:  list of columns on which to apply the feature generator\n
      \       group_columns: List of column names for grouping\n        **kwargs:\n\n
      \   Returns:\n        DataFrame: Returns feature vector with 100th percentile
      (sample maximum) of each specified column.\n\n    Examples:\n        >>> import
      pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0, 6, 3],\n
      \                               [-2, 8, 7], [2, 9, 6]],\n                                columns=
      ['accelx', 'accely', 'accelz'])\n        >>> df\n            out:\n               accelx
      \ accely  accelz\n            0      -3       6       5\n            1       3
      \      7       8\n            2       0       6       3\n            3      -2
      \      8       7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"100th Percentile\"],\n                params
      = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n               accelx100Percentile
      \ accely100Percentile  accelz100Percentile\n            0                  3.0
      \                 9.0                  8.0\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_pct100.c
    core: true
    dcl_executable: false
    c_function_name: stats_pct100
- model: library.transform
  pk: 29
  fields:
    uuid: 0f11774c-7cd1-4243-b4cd-719da46e5f2b
    name: Mean Difference
    function_in_file: mean_difference
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculate the mean difference of each specified column. Works
      with grouped data.\n    For a given column, it finds difference of ith element
      and (i-1)th element and\n    finally takes the mean value of the entire column.\n\n
      \   mean(diff(arr)) = mean(arr[i] - arr[i-1]), for all 1 <= i <= n.\n\n    Args:\n
      \       input_data: is dataframe.\n        columns:  The `columns` represents
      a list of all column names\n        on which `mean_difference` is to be found.\n
      \       group_columns: List of column names for grouping\n        **kwargs:\n\n\n
      \   Returns:\n    Examples:\n        >>> import pandas as pd\n        >>> import
      numpy as np\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,'Class':
      ['Crawling'] * 20 ,'Rep': [1] * 20 })\n        >>> df['accelx'] = [0, 9, 5,
      -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df\n
      \           Out:\n                 Class     Rep   Subject  accelx\n            0
      \  Crawling    1     s01       0\n            1   Crawling    1     s01       9\n
      \           2   Crawling    1     s01       5\n            3   Crawling    1
      \    s01      -5\n            4   Crawling    1     s01      -9\n            5
      \  Crawling    1     s01       0\n            6   Crawling    1     s01       9\n
      \           7   Crawling    1     s01       5\n            8   Crawling    1
      \    s01      -5\n            9   Crawling    1     s01      -9\n            10
      \ Crawling    1     s01       0\n            11  Crawling    1     s01       9\n
      \           12  Crawling    1     s01       5\n            13  Crawling    1
      \    s01      -5\n            14  Crawling    1     s01      -9\n            15
      \ Crawling    1     s01       0\n            16  Crawling    1     s01       9\n
      \           17  Crawling    1     s01       5\n            18  Crawling    1
      \    s01      -5\n            19  Crawling    1     s01      -9\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df)\n
      \       >>> dsk.pipeline.add_feature_generator([\"Mean Difference\"],\n                params
      = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           Out:\n               gen_0001_accelxMeanDifference\n            0
      \                     -0.105263\n    "
    path: "../library/core_functions/fg_rate_of_change.py"
    type: Feature Generator
    subtype: Rate of Change
    has_c_version: true
    c_file_name: fg_roc_mean_difference.c
    core: true
    dcl_executable: false
    c_function_name: mean_difference
- model: library.transform
  pk: 30
  fields:
    uuid: de510022-6ef0-4d02-b9ca-7cba4c9f499d
    name: Mean Crossing Rate
    function_in_file: mean_crossing_rate
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculates the rate at which mean value is crossed for each
      specified column.\n    Works with grouped data. The total number of mean value
      crossings are found\n    and then the number is divided by total number of samples
      \ to get\n    the `mean_crossing_rate`.\n\n    Args:\n        input_data: is
      dataframe.\n        columns:  The `columns` represents a list of all column
      names on\n                  which `mean_crossing_rate` is to be found.\n        group_columns:
      List of column names for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame
      : Return the number of zero crossings divided by the length of the signal.\n\n
      \   Examples:\n        >>> import pandas as pd\n        >>> import numpy as
      np\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20 ,\n                               'Rep': [1] * 20 })\n        >>>
      df['accelx'] = [0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9]\n        >>> df\n            Out:\n                  Class    Rep   Subject
      \ accelx\n            0   Crawling    1     s01       0\n            1   Crawling
      \   1     s01       9\n            2   Crawling    1     s01       5\n            3
      \  Crawling    1     s01      -5\n            4   Crawling    1     s01      -9\n
      \           5   Crawling    1     s01       0\n            6   Crawling    1
      \    s01       9\n            7   Crawling    1     s01       5\n            8
      \  Crawling    1     s01      -5\n            9   Crawling    1     s01      -9\n
      \           10  Crawling    1     s01       0\n            11  Crawling    1
      \    s01       9\n            12  Crawling    1     s01       5\n            13
      \ Crawling    1     s01      -5\n            14  Crawling    1     s01      -9\n
      \           15  Crawling    1     s01       0\n            16  Crawling    1
      \    s01       9\n            17  Crawling    1     s01       5\n            18
      \ Crawling    1     s01      -5\n            19  Crawling    1     s01      -9\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df)\n        >>> dsk.pipeline.add_feature_generator([\"Mean Crossing Rate\"],\n
      \                params = {\"group_columns\": []},\n                           function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           Out:\n               gen_0001_accelxMeanCrossingRate\n            0
      \                            0.35\n    "
    path: "../library/core_functions/fg_rate_of_change.py"
    type: Feature Generator
    subtype: Rate of Change
    has_c_version: true
    c_file_name: fg_roc_mean_crossing_rate.c
    core: true
    dcl_executable: false
    c_function_name: mean_crossing_rate
- model: library.transform
  pk: 31
  fields:
    uuid: f498e41e-f9c0-4d58-8252-375fe19d6743
    name: Zero Crossing Rate
    function_in_file: zero_crossing_rate
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculates the rate at which zero value is crossed for each
      specified column.\n    The total number of zero crossings are found and then
      the number is divided\n    by total number of samples to get the `zero_crossing_rate`.\n\n
      \   Args:\n        input_data: is dataframe.\n        columns:  The `columns`
      represents a list of all column names on which\n                 `zero_crossing_rate`
      is to be found.\n        group_columns: List of column names for grouping\n
      \       **kwargs:\n\n    Returns:\n        A dataframe of containing ZCR\n\n
      \   Examples:\n        >>> import pandas as pd\n        >>> import numpy as
      np\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                             'Class':
      ['Crawling'] * 20 ,\n                             'Rep': [1] * 20 })\n        >>>
      df['accelx'] = [0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9]\n        >>> df\n            Out:\n               Class  Rep Subject
      \ accelx\n            0   Crawling    1     s01       0\n            1   Crawling
      \   1     s01       9\n            2   Crawling    1     s01       5\n            3
      \  Crawling    1     s01      -5\n            4   Crawling    1     s01      -9\n
      \           5   Crawling    1     s01       0\n            6   Crawling    1
      \    s01       9\n            7   Crawling    1     s01       5\n            8
      \  Crawling    1     s01      -5\n            9   Crawling    1     s01      -9\n
      \           10  Crawling    1     s01       0\n            11  Crawling    1
      \    s01       9\n            12  Crawling    1     s01       5\n            13
      \ Crawling    1     s01      -5\n            14  Crawling    1     s01      -9\n
      \           15  Crawling    1     s01       0\n            16  Crawling    1
      \    s01       9\n            17  Crawling    1     s01       5\n            18
      \ Crawling    1     s01      -5\n            19  Crawling    1     s01      -9\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df)\n        >>> dsk.pipeline.add_feature_generator([\"Zero Crossing Rate\"],\n
      \                   params = {\"group_columns\": []},\n                    function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           Out:\n               gen_0001_accelxZeroCrossingRate\n            0
      \                            0.35\n    "
    path: "../library/core_functions/fg_rate_of_change.py"
    type: Feature Generator
    subtype: Rate of Change
    has_c_version: true
    c_file_name: fg_roc_zero_crossing_rate.c
    core: true
    dcl_executable: false
    c_function_name: zero_crossing_rate
- model: library.transform
  pk: 32
  fields:
    uuid: 7b7a4314-b418-43b7-aaf8-1f8aba0c3fd1
    name: Sigma Crossing Rate
    function_in_file: sigma_crossing_rate
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculates the rate at which standard deviation value (sigma)
      is crossed for\n    each specified column. The total number of sigma crossings
      are found and then\n    the number is divided by total number of samples to
      get the `sigma_crossing_rate`.\n\n    Args:\n        input_data: is dataframe.\n
      \       columns:  The `columns` represents a list of all column names on which\n
      \                `sigma_crossing_rate` is to be found.\n        group_columns:
      List of column names for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame:\n\n
      \   Examples:\n        >>> import pandas as pd\n        >>> import numpy as
      np\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20 ,\n                               'Rep': [1] * 20 })\n        >>>
      df['accelx'] = [0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9]\n        >>> df\n            Out:\n               Class  Rep Subject
      \ accelx\n            0   Crawling    1     s01       0\n            1   Crawling
      \   1     s01       9\n            2   Crawling    1     s01       5\n            3
      \  Crawling    1     s01      -5\n            4   Crawling    1     s01      -9\n
      \           5   Crawling    1     s01       0\n            6   Crawling    1
      \    s01       9\n            7   Crawling    1     s01       5\n            8
      \  Crawling    1     s01      -5\n            9   Crawling    1     s01      -9\n
      \           10  Crawling    1     s01       0\n            11  Crawling    1
      \    s01       9\n            12  Crawling    1     s01       5\n            13
      \ Crawling    1     s01      -5\n            14  Crawling    1     s01      -9\n
      \           15  Crawling    1     s01       0\n            16  Crawling    1
      \    s01       9\n            17  Crawling    1     s01       5\n            18
      \ Crawling    1     s01      -5\n            19  Crawling    1     s01      -9\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df)\n        >>> dsk.pipeline.add_feature_generator([\"Sigma Crossing Rate\"],\n
      \               params = {\"group_columns\": []},\n                          function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           Out:\n               gen_0001_accelxSigmaCrossingRate\n            0
      \                              0.2\n    "
    path: "../library/core_functions/fg_rate_of_change.py"
    type: Feature Generator
    subtype: Rate of Change
    has_c_version: true
    c_file_name: fg_roc_sigma_crossing_rate.c
    core: true
    dcl_executable: false
    c_function_name: sigma_crossing_rate
- model: library.transform
  pk: 33
  fields:
    uuid: cae3d1ea-3e8e-412c-a1d4-e34bebf06ad2
    name: Second Sigma Crossing Rate
    function_in_file: second_sigma_crossing_rate
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculates the rate at which 2nd standard deviation value
      (second sigma) is\n    crossed for each specified column. The total number of
      second sigma crossings\n    are found and then the number is divided by total
      number of samples  to get\n    the `second_sigma_crossing_rate`.\n\n\n    Args:\n
      \       input_data: is dataframe.\n        columns:  The `columns` represents
      a list of all column names on which\n                  `second_sigma_crossing_rate`
      is to be found.\n        group_columns: List of column names for grouping\n
      \       **kwargs:\n\n    Returns:\n\n    Examples:\n        >>> import pandas
      as pd\n        >>> import numpy as np\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                               'Class': ['Crawling'] * 20 ,\n
      \                              'Rep': [1] * 20 })\n        >>> df['accelx']
      = [0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>>
      df\n            Out:\n               Class  Rep Subject  accelx\n            0
      \  Crawling    1     s01       0\n            1   Crawling    1     s01       9\n
      \           2   Crawling    1     s01       5\n            3   Crawling    1
      \    s01      -5\n            4   Crawling    1     s01      -9\n            5
      \  Crawling    1     s01       0\n            6   Crawling    1     s01       9\n
      \           7   Crawling    1     s01       5\n            8   Crawling    1
      \    s01      -5\n            9   Crawling    1     s01      -9\n            10
      \ Crawling    1     s01       0\n            11  Crawling    1     s01       9\n
      \           12  Crawling    1     s01       5\n            13  Crawling    1
      \    s01      -5\n            14  Crawling    1     s01      -9\n            15
      \ Crawling    1     s01       0\n            16  Crawling    1     s01       9\n
      \           17  Crawling    1     s01       5\n            18  Crawling    1
      \    s01      -5\n            19  Crawling    1     s01      -9\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df)\n
      \       >>> dsk.pipeline.add_feature_generator([\"Second Sigma Crossing Rate\"],\n
      \               params = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           Out:\n               accelx2ndSigmaCrossingRate\n            0                           0\n
      \   "
    path: "../library/core_functions/fg_rate_of_change.py"
    type: Feature Generator
    subtype: Rate of Change
    has_c_version: true
    c_file_name: fg_roc_second_sigma_crossing_rate.c
    core: true
    dcl_executable: false
    c_function_name: second_sigma_crossing_rate
- model: library.transform
  pk: 34
  fields:
    uuid: a4c327cb-7c6f-404a-ae6e-36c5a5317ff8
    name: Dominant Frequency
    function_in_file: dominant_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculate the dominant frequency for each specified signal.
      For each column,\n    find the frequency at which the signal has highest power.\n\n
      \   Args:\n        input_data: Input dataframe\n        sample_rate: float;
      Sampling rate\n        columns: List of columns on which `dominant_frequency`
      needs\n                 to be calculated\n        group_columns: List of column
      names for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame of
      `dominant_frequency` for each column and the specified group_columns\n\n    Examples:\n
      \       >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20 ,\n                               'Rep': [0] * 10 + [1] *10
      })\n        >>> df['accelx'] = [0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5,
      -9, 0, 9, 5, -5, -9]\n        >>> df\n            out:\n                   Class
      \ Rep Subject  accelx\n            0   Crawling    0     s01       0\n            1
      \  Crawling    0     s01       9\n            2   Crawling    0     s01       5\n
      \           3   Crawling    0     s01      -5\n            4   Crawling    0
      \    s01      -9\n            5   Crawling    0     s01       0\n            6
      \  Crawling    0     s01       9\n            7   Crawling    0     s01       5\n
      \           8   Crawling    0     s01      -5\n            9   Crawling    0
      \    s01      -9\n            10  Crawling    1     s01       0\n            11
      \ Crawling    1     s01       9\n            12  Crawling    1     s01       5\n
      \           13  Crawling    1     s01      -5\n            14  Crawling    1
      \    s01      -9\n            15  Crawling    1     s01       0\n            16
      \ Crawling    1     s01       9\n            17  Crawling    1     s01       5\n
      \           18  Crawling    1     s01      -5\n            19  Crawling    1
      \    s01      -9\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Dominant
      Frequency\"],\n                    params = {\"group_columns\": []},\n                    function_defaults={\"columns\":['accelx'],\n
      \                                      \"sample_rate\" : 10})\n        >>> result,
      stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \              accelxDomFreq\n            0            2.0\n\n    "
    path: "../library/core_functions/fg_frequency"
    type: Feature Generator
    subtype: Frequency
    has_c_version: false
    c_file_name: fg_frequency_dominant_frequency.c
    core: true
    dcl_executable: false
    c_function_name: dominant_frequency
- model: library.transform
  pk: 35
  fields:
    uuid: 259582c3-8e2f-4f3b-a6c6-415d7b969717
    name: Spectral Entropy
    function_in_file: spectral_entropy
    input_contract:
      - type: DataFrame
        name: input_data
        description: Input signal data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Calculate the spectral entropy for each specified signal.
      For each column,\n    first calculate the power spectrum, and then using the
      power spectrum, calculate\n    the entropy in the spectral domain. Spectral
      entropy measures the spectral\n    complexity of the signal.\n\n    Args:\n
      \       input_data: Input dataframe.\n        sample_rate: float; Sampling rate\n
      \       columns: List of all columns for which `spectral_entropy` is to be calculated\n
      \       group_columns: List of column names for grouping.\n        **kwargs:\n\n
      \   Returns:\n        DataFrame of `spectral_entropy` for each column and the
      specified group_columns\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20 ,\n
      \                             'Rep': [0] * 10 + [1] *10 })\n        >>> df['accelx']
      = [0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>>
      df\n            out:\n                   Class  Rep Subject  accelx\n            0
      \  Crawling    0     s01       0\n            1   Crawling    0     s01       9\n
      \           2   Crawling    0     s01       5\n            3   Crawling    0
      \    s01      -5\n            4   Crawling    0     s01      -9\n            5
      \  Crawling    0     s01       0\n            6   Crawling    0     s01       9\n
      \           7   Crawling    0     s01       5\n            8   Crawling    0
      \    s01      -5\n            9   Crawling    0     s01      -9\n            10
      \ Crawling    1     s01       0\n            11  Crawling    1     s01       9\n
      \           12  Crawling    1     s01       5\n            13  Crawling    1
      \    s01      -5\n            14  Crawling    1     s01      -9\n            15
      \ Crawling    1     s01       0\n            16  Crawling    1     s01       9\n
      \           17  Crawling    1     s01       5\n            18  Crawling    1
      \    s01      -5\n            19  Crawling    1     s01      -9\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Spectral Entropy\"],\n
      \               params = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx'],\n
      \                                  \"sample_rate\" : 10})\n        >>> result,
      stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \              accelxSpecEntr\n            0        1.022225\n    "
    path: "../library/core_functions/fg_frequency"
    type: Feature Generator
    subtype: Frequency
    has_c_version: true
    c_file_name: fg_frequency_spectral_entropy.c
    core: true
    dcl_executable: false
    c_function_name: spectral_entropy
- model: library.transform
  pk: 36
  fields:
    uuid: ae422fd1-be3f-4657-aa33-d2a8058be269
    name: Total Area
    function_in_file: total_area
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Total area under the signal. Total area = sum(signal(t)*dt),
      where\n    signal(t) is signal value at time t, and dt is sampling time (dt
      = 1/sample_rate).\n\n    Args:\n        input_data: input dataframe\n\n        sample_rate:
      Sampling rate of the signal\n\n        columns: List of str; Set of columns
      on which to apply the feature generator\n\n        group_columns: List of str;
      Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                             'Class': ['Crawling'] * 20,\n                              'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n
      \                     Class  Rep Subject  accelx\n                0   Crawling
      \   0     s01       1\n                1   Crawling    0     s01      -2\n                2
      \  Crawling    0     s01      -3\n                3   Crawling    0     s01
      \      1\n                4   Crawling    0     s01       2\n                5
      \  Crawling    0     s01       5\n                6   Crawling    0     s01
      \      2\n                7   Crawling    0     s01      -2\n                8
      \  Crawling    1     s01      -3\n                9   Crawling    1     s01
      \     -1\n                10  Crawling    1     s01       1\n                11
      \ Crawling    1     s01      -3\n                12  Crawling    1     s01      -4\n
      \               13  Crawling    1     s01       1\n                14  Crawling
      \   1     s01       2\n                15  Crawling    1     s01       6\n                16
      \ Crawling    1     s01       2\n                17  Crawling    1     s01      -3\n
      \               18  Crawling    1     s01      -2\n                19  Crawling
      \   1     s01      -1\n\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Total Area\"],\n
      \                       params = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n
      \                                 function_defaults={\"columns\":['accelx'],\n
      \                                                    'sample_rate' : 10.0})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                  Class  Rep Subject  gen_0001_accelxTotArea\n
      \           0  Crawling    0     s01                     0.4\n            1
      \ Crawling    1     s01                     0.5\n\n\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_total_area.c
    core: true
    dcl_executable: false
    c_function_name: total_area
- model: library.transform
  pk: 37
  fields:
    uuid: 50e57f02-add1-48e3-88b1-200ead104c19
    name: Absolute Area
    function_in_file: absolute_area
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Absolute area of the signal. Absolute area = sum(abs(signal(t))
      dt), where\n    `abs(signal(t))` is absolute signal value at time t, and dt
      is sampling time (dt = 1/sample_rate).\n\n    Args:\n        input_data: Input
      dataframe\n\n        sample_rate: Sampling rate of the signal\n\n        columns:
      List of str; Set of columns on which to apply the feature generator\n\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20,\n                              'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n
      \                     Class  Rep Subject  accelx\n                0   Crawling
      \   0     s01       1\n                1   Crawling    0     s01      -2\n                2
      \  Crawling    0     s01      -3\n                3   Crawling    0     s01
      \      1\n                4   Crawling    0     s01       2\n                5
      \  Crawling    0     s01       5\n                6   Crawling    0     s01
      \      2\n                7   Crawling    0     s01      -2\n                8
      \  Crawling    1     s01      -3\n                9   Crawling    1     s01
      \     -1\n                10  Crawling    1     s01       1\n                11
      \ Crawling    1     s01      -3\n                12  Crawling    1     s01      -4\n
      \               13  Crawling    1     s01       1\n                14  Crawling
      \   1     s01       2\n                15  Crawling    1     s01       6\n                16
      \ Crawling    1     s01       2\n                17  Crawling    1     s01      -3\n
      \               18  Crawling    1     s01      -2\n                19  Crawling
      \   1     s01      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Absolute
      Area\"],\n                        params = {\"group_columns\": ['Subject', 'Class',
      'Rep']},\n                        function_defaults={\"columns\":['accelx'],
      'sample_rate' : 10.})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0001_accelxAbsArea\n            0  Crawling    0     s01                     1.8\n
      \           1  Crawling    1     s01                     2.9\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_absolute_area.c
    core: true
    dcl_executable: false
    c_function_name: absolute_area
- model: library.transform
  pk: 38
  fields:
    uuid: 24ab894e-c135-4de0-89ef-1353ca0628dc
    name: Total Area of Low Frequency
    function_in_file: total_area_low_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Total area of low frequency components of the signal. It calculates
      total area\n    by first applying a moving average filter on the signal with
      a smoothing factor.\n\n    Args:\n        input_data: Input dataframe\n\n        sample_rate:
      float; Sampling rate of the signal\n\n        smoothing_factor: int; Determines
      the amount of attenuation for\n                          frequencies over the
      cutoff frequency.\n\n        columns: List of str; Set of columns on which to
      apply the feature generator\n\n        group_columns: List of str; Set of columns
      by which to aggregate\n\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                              'Class':
      ['Crawling'] * 20,\n                              'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n                      Class
      \ Rep Subject  accelx\n                0   Crawling    0     s01       1\n                1
      \  Crawling    0     s01      -2\n                2   Crawling    0     s01
      \     -3\n                3   Crawling    0     s01       1\n                4
      \  Crawling    0     s01       2\n                5   Crawling    0     s01
      \      5\n                6   Crawling    0     s01       2\n                7
      \  Crawling    0     s01      -2\n                8   Crawling    1     s01
      \     -3\n                9   Crawling    1     s01      -1\n                10
      \ Crawling    1     s01       1\n                11  Crawling    1     s01      -3\n
      \               12  Crawling    1     s01      -4\n                13  Crawling
      \   1     s01       1\n                14  Crawling    1     s01       2\n                15
      \ Crawling    1     s01       6\n                16  Crawling    1     s01       2\n
      \               17  Crawling    1     s01      -3\n                18  Crawling
      \   1     s01      -2\n                19  Crawling    1     s01      -1\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Total Area of
      Low Frequency\"],\n                         params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                                    function_defaults={\"columns\":['accelx'],\n
      \                                                      'sample_rate' : 10.,\n
      \                                                      'smoothing_factor' :2})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                  Class  Rep Subject  gen_0001_accelxTotAreaDc\n
      \           0  Crawling    0     s01                      0.25\n            1
      \ Crawling    1     s01                      0.40\n\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_total_area_low_frequency.c
    core: true
    dcl_executable: false
    c_function_name: total_area_low_frequency
- model: library.transform
  pk: 39
  fields:
    uuid: a667a5c6-86d8-4c09-b43c-769555a251a0
    name: Absolute Area of Low Frequency
    function_in_file: absolute_area_low_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Absolute area of low frequency components of the signal. It
      calculates absolute area\n    by first applying a moving average filter on the
      signal with a smoothing factor.\n\n    Args:\n        input_data: Input dataframe\n\n
      \       sample_rate: float; Sampling rate of the signal\n\n        smoothing_factor:
      int; Determines the amount of attenuation\n                         for frequencies
      over the cutoff frequency.\n\n        columns: List of str; Set of columns on
      which to apply the feature generator\n\n        group_columns: List of str;
      Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                               'Class': ['Crawling'] * 20,\n
      \                              'Rep': [0] * 8 + [1] * 12})\n        >>> df['accelx']
      = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                      Class  Rep Subject  accelx\n                0
      \  Crawling    0     s01       1\n                1   Crawling    0     s01
      \     -2\n                2   Crawling    0     s01      -3\n                3
      \  Crawling    0     s01       1\n                4   Crawling    0     s01
      \      2\n                5   Crawling    0     s01       5\n                6
      \  Crawling    0     s01       2\n                7   Crawling    0     s01
      \     -2\n                8   Crawling    1     s01      -3\n                9
      \  Crawling    1     s01      -1\n                10  Crawling    1     s01
      \      1\n                11  Crawling    1     s01      -3\n                12
      \ Crawling    1     s01      -4\n                13  Crawling    1     s01       1\n
      \               14  Crawling    1     s01       2\n                15  Crawling
      \   1     s01       6\n                16  Crawling    1     s01       2\n                17
      \ Crawling    1     s01      -3\n                18  Crawling    1     s01      -2\n
      \               19  Crawling    1     s01      -1\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Absolute Area of Low Frequency\"],\n                        params
      = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n                                 function_defaults={\"columns\":['accelx'],\n
      \                                'sample_rate' : 10.,\n                                 'smoothing_factor'
      :2})\n        >>> result, stats = dsk.pipeline.execute()\n        >>> print
      result\n            out:\n                  Class  Rep Subject  gen_0001_accelxTotAreaDc\n
      \           0  Crawling    0     s01                      0.25\n            1
      \ Crawling    1     s01                      0.40\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_absolute_area_low_frequency.c
    core: true
    dcl_executable: false
    c_function_name: absolute_area_low_frequency
- model: library.transform
  pk: 40
  fields:
    uuid: 9348b2a8-d93f-46ad-8cde-1b3314b508d6
    name: Total Area of High Frequency
    function_in_file: total_area_high_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Total area of high frequency components of the signal. It
      calculates total\n     area by applying a moving average filter on the signal
      with a smoothing factor\n      and subtracting the filtered signal from the
      original.\n\n    Args:\n        input_data: Input dataframe\n\n        sample_rate:
      float; Sampling rate of the signal\n\n        smoothing_factor: int; Determines
      the amount of attenuation for\n                             frequencies over
      the cutoff frequency.\n\n        columns: List of str; Set of columns on which
      to apply the feature generator\n\n        group_columns: List of str; Set of
      columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20,\n                               'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n                      Class
      \ Rep Subject  accelx\n                0   Crawling    0     s01       1\n                1
      \  Crawling    0     s01      -2\n                2   Crawling    0     s01
      \     -3\n                3   Crawling    0     s01       1\n                4
      \  Crawling    0     s01       2\n                5   Crawling    0     s01
      \      5\n                6   Crawling    0     s01       2\n                7
      \  Crawling    0     s01      -2\n                8   Crawling    1     s01
      \     -3\n                9   Crawling    1     s01      -1\n                10
      \ Crawling    1     s01       1\n                11  Crawling    1     s01      -3\n
      \               12  Crawling    1     s01      -4\n                13  Crawling
      \   1     s01       1\n                14  Crawling    1     s01       2\n                15
      \ Crawling    1     s01       6\n                16  Crawling    1     s01       2\n
      \               17  Crawling    1     s01      -3\n                18  Crawling
      \   1     s01      -2\n                19  Crawling    1     s01      -1\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Total Area of
      High Frequency\"],\n                        params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                         function_defaults={\"columns\":['accelx'],\n
      \                                                    'sample_rate' : 10.,\n
      \                                                    'smoothing_factor' :2})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                  Class  Rep Subject  gen_0001_accelxTotAreaAc\n
      \           0  Crawling    0     s01                      0.15\n            1
      \ Crawling    1     s01                      0.10\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_total_area_high_frequency.c
    core: true
    dcl_executable: false
    c_function_name: total_area_high_frequency
- model: library.transform
  pk: 41
  fields:
    uuid: 60c78056-eef1-4285-82b5-9e14dfd8005d
    name: Absolute Area of High Frequency
    function_in_file: absolute_area_high_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Absolute area of high frequency components of the signal.
      It calculates absolute\n    area by applying a moving average filter on the
      signal with a smoothing factor\n    and subtracting the filtered signal from
      the original.\n\n\n    Args:\n        input_data: Input dataframe\n\n        sample_rate:
      float; Sampling rate of the signal\n\n        smoothing_factor: int; Determines
      the amount of attenuation for frequencies\n                         over the
      cutoff frequency.\n\n        columns: List of str; Set of columns on which to
      apply the feature generator\n\n        group_columns: List of str; Set of columns
      by which to aggregate\n\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                              'Class':
      ['Crawling'] * 20,\n                              'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n                      Class
      \ Rep Subject  accelx\n                0   Crawling    0     s01       1\n                1
      \  Crawling    0     s01      -2\n                2   Crawling    0     s01
      \     -3\n                3   Crawling    0     s01       1\n                4
      \  Crawling    0     s01       2\n                5   Crawling    0     s01
      \      5\n                6   Crawling    0     s01       2\n                7
      \  Crawling    0     s01      -2\n                8   Crawling    1     s01
      \     -3\n                9   Crawling    1     s01      -1\n                10
      \ Crawling    1     s01       1\n                11  Crawling    1     s01      -3\n
      \               12  Crawling    1     s01      -4\n                13  Crawling
      \   1     s01       1\n                14  Crawling    1     s01       2\n                15
      \ Crawling    1     s01       6\n                16  Crawling    1     s01       2\n
      \               17  Crawling    1     s01      -3\n                18  Crawling
      \   1     s01      -2\n                19  Crawling    1     s01      -1\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Absolute Area
      of High Frequency\"],\n                    params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                              function_defaults={\"columns\":['accelx'],\n
      \                                               'sample_rate' : 10.,\n                                                'smoothing_factor'
      :2})\n        >>> result, stats = dsk.pipeline.execute()\n        >>> print
      result\n            out:\n                  Class  Rep Subject  gen_0001_accelxTotAreaAc\n
      \           0  Crawling    0     s01                      0.15\n            1
      \ Crawling    1     s01                      0.10\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_absolute_area_high_frequency.c
    core: true
    dcl_executable: false
    c_function_name: absolute_area_high_frequency
- model: library.transform
  pk: 42
  fields:
    uuid: ac64b8cd-9df2-454f-8423-b3feacaa93da
    name: Absolute Area of Spectrum
    function_in_file: absolute_area_spectrum
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Absolute area of spectrum.\n\n    Args:\n        input_data:
      DataFrame\n\n        sample_rate: Sampling rate of the signal\n\n        columns:
      List of str; Set of columns on which to apply the feature generator\n\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20,\n                              'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n
      \                     Class  Rep Subject  accelx\n                0   Crawling
      \   0     s01       1\n                1   Crawling    0     s01      -2\n                2
      \  Crawling    0     s01      -3\n                3   Crawling    0     s01
      \      1\n                4   Crawling    0     s01       2\n                5
      \  Crawling    0     s01       5\n                6   Crawling    0     s01
      \      2\n                7   Crawling    0     s01      -2\n                8
      \  Crawling    1     s01      -3\n                9   Crawling    1     s01
      \     -1\n                10  Crawling    1     s01       1\n                11
      \ Crawling    1     s01      -3\n                12  Crawling    1     s01      -4\n
      \               13  Crawling    1     s01       1\n                14  Crawling
      \   1     s01       2\n                15  Crawling    1     s01       6\n                16
      \ Crawling    1     s01       2\n                17  Crawling    1     s01      -3\n
      \               18  Crawling    1     s01      -2\n                19  Crawling
      \   1     s01      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Absolute
      Area of Spectrum\"],\n                params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                 function_defaults={\"columns\":['accelx'],\n
      \                                   'sample_rate' : 10.,\n                                    'smoothing_factor'
      :2})\n        >>> result, stats = dsk.pipeline.execute()\n        >>> print
      result\n            out:\n                  Class  Rep Subject  gen_0001_accelxAbsAreaSpec\n
      \           0  Crawling    0     s01                    8.546426\n            1
      \ Crawling    1     s01                    7.643549\n    "
    path: "../library/core_functions/fg_area.py"
    type: Feature Generator
    subtype: Area
    has_c_version: true
    c_file_name: fg_area_power_spectrum_density.c
    core: true
    dcl_executable: false
    c_function_name: area_power_spectrum_density
- model: library.transform
  pk: 43
  fields:
    uuid: 519e7bbd-04a0-49da-95b2-176b15061734
    name: Ratio of Peak to Peak of High Frequency between two halves
    function_in_file: ratio_high_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Ratio of peak to peak of high frequency between two halves.
      The high frequency\n    signal is calculated by subtracting the moving average
      filter output from the original signal.\n\n    Args:\n        input_data:\n
      \       smoothing_factor: int; Determines the amount of attenuation for\n          frequencies
      over the cutoff frequency. The number\n          of elements in individual columns
      should be al least\n          three times the smoothing factor.\n        columns:
      List of str; Set of columns on which to apply the feature generator\n        group_columns:
      List of str; Set of columns by which to aggregate\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,'Class': ['Crawling'] * 20 ,'Rep': [0] * 20})\n        >>> df['accelx']
      = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                  Class  Rep Subject  accelx\n            0
      \  Crawling    0     s01       1\n            1   Crawling    0     s01      -2\n
      \           2   Crawling    0     s01      -3\n            3   Crawling    0
      \    s01       1\n            4   Crawling    0     s01       2\n            5
      \  Crawling    0     s01       5\n            6   Crawling    0     s01       2\n
      \           7   Crawling    0     s01      -2\n            8   Crawling    0
      \    s01      -3\n            9   Crawling    0     s01      -1\n            10
      \ Crawling    0     s01       1\n            11  Crawling    0     s01      -3\n
      \           12  Crawling    0     s01      -4\n            13  Crawling    0
      \    s01       1\n            14  Crawling    0     s01       2\n            15
      \ Crawling    0     s01       6\n            16  Crawling    0     s01       2\n
      \           17  Crawling    0     s01      -3\n            18  Crawling    0
      \    s01      -2\n            19  Crawling    0     s01      -1\n\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Ratio of Peak
      to Peak of High Frequency between two halves\"],\n                params = {\"group_columns\":
      ['Subject', 'Class', 'Rep']},\n                function_defaults={\"columns\":['accelx'],\n
      \                                 'smoothing_factor' : 3})\n        >>> result,
      stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \                 Class  Rep Subject  gen_0001_accelxACRatio\n            0
      \ Crawling    0     s01                1.263158\n    "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Shape
    has_c_version: true
    c_file_name: fg_shape_ratio_high_freq.c
    core: true
    dcl_executable: false
    c_function_name: ratio_high_frequency
- model: library.transform
  pk: 44
  fields:
    uuid: b2e12337-5c29-430e-9912-96ed5325f2f6
    name: Difference of Peak to Peak of High Frequency between two halves
    function_in_file: difference_high_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Difference of peak to peak of high frequency between two halves.\n
      \   The high frequency signal is calculated by subtracting the moving\n    average
      filter output from the original signal.\n\n    Args:\n        input_data:\n
      \       smoothing_factor: int; Determines the amount of attenuation for\n            frequencies
      over the cutoff frequency. The number\n            of elements in individual
      columns should be at lest\n            three times the smoothing factor.\n        columns:
      List of str; Set of columns on which to apply the feature generator\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                              'Class':
      ['Crawling'] * 20 ,\n                              'Rep': [0] * 20})\n        >>>
      df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3,
      -2, -1]\n        >>> df\n            out:\n                  Class  Rep Subject
      \ accelx\n            0   Crawling    0     s01       1\n            1   Crawling
      \   0     s01      -2\n            2   Crawling    0     s01      -3\n            3
      \  Crawling    0     s01       1\n            4   Crawling    0     s01       2\n
      \           5   Crawling    0     s01       5\n            6   Crawling    0
      \    s01       2\n            7   Crawling    0     s01      -2\n            8
      \  Crawling    0     s01      -3\n            9   Crawling    0     s01      -1\n
      \           10  Crawling    0     s01       1\n            11  Crawling    0
      \    s01      -3\n            12  Crawling    0     s01      -4\n            13
      \ Crawling    0     s01       1\n            14  Crawling    0     s01       2\n
      \           15  Crawling    0     s01       6\n            16  Crawling    0
      \    s01       2\n            17  Crawling    0     s01      -3\n            18
      \ Crawling    0     s01      -2\n            19  Crawling    0     s01      -1\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Difference
      of Peak to Peak of High Frequency between two halves\"],\n                params
      = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n                function_defaults={\"columns\":['accelx'],
      'smoothing_factor' : 3})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0001_accelxACDiff\n            0  Crawling    0     s01              -1.666667\n
      \   "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Shape
    has_c_version: true
    c_file_name: fg_shape_difference_high_freq.c
    core: true
    dcl_executable: false
    c_function_name: difference_high_frequency
- model: library.transform
  pk: 45
  fields:
    uuid: 9f1d20bb-3e07-48d7-9eb6-9482f241baa8
    name: Global Peak to Peak of Low Frequency
    function_in_file: global_p2p_low_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Global peak to peak of low frequency. The low frequency signal
      is calculated by\n    applying a moving average filter with a smoothing factor.\n\n
      \   Args:\n        input_data:\n        smoothing_factor: int; Determines the
      amount of attenuation for\n          frequencies over the cutoff frequency\n
      \       columns: List of str; Set of columns on which to apply the feature generator\n
      \       group_columns: List of str; Set of columns by which to aggregate\n        **kwargs:\n\n
      \   Returns:\n        DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                                'Class': ['Crawling'] * 20 ,\n
      \                               'Rep': [0] * 20})\n        >>> df['accelx']
      = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                  Class  Rep Subject  accelx\n            0
      \  Crawling    0     s01       1\n            1   Crawling    0     s01      -2\n
      \           2   Crawling    0     s01      -3\n            3   Crawling    0
      \    s01       1\n            4   Crawling    0     s01       2\n            5
      \  Crawling    0     s01       5\n            6   Crawling    0     s01       2\n
      \           7   Crawling    0     s01      -2\n            8   Crawling    0
      \    s01      -3\n            9   Crawling    0     s01      -1\n            10
      \ Crawling    0     s01       1\n            11  Crawling    0     s01      -3\n
      \           12  Crawling    0     s01      -4\n            13  Crawling    0
      \    s01       1\n            14  Crawling    0     s01       2\n            15
      \ Crawling    0     s01       6\n            16  Crawling    0     s01       2\n
      \           17  Crawling    0     s01      -3\n            18  Crawling    0
      \    s01      -2\n            19  Crawling    0     s01      -1\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Global Peak to
      Peak of Low Frequency\"],\n                        params = {\"group_columns\":
      ['Subject', 'Class', 'Rep']},\n                        function_defaults={\"columns\":['accelx'],\n
      \                                           'smoothing_factor' : 3})\n        >>>
      result, stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \                 Class  Rep Subject  gen_0001_accelxMaxP2PGlobalDC\n            0
      \ Crawling    0     s01                       5.333333\n    "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Amplitude
    has_c_version: true
    c_file_name: fg_amplitude_global_p2p_low_frequency.c
    core: true
    dcl_executable: false
    c_function_name: global_p2p_low_frequency
- model: library.transform
  pk: 46
  fields:
    uuid: 10de21b7-067b-45b4-ba12-9e99cf83fc89
    name: Global Peak to Peak of High Frequency
    function_in_file: global_p2p_high_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Global peak to peak of high frequency. The high frequency
      signal is calculated by\n    subtracting the moving average filter output from
      the original signal.\n\n    Args:\n        input_data:\n        smoothing_factor:
      int; Determines the amount of attenuation for frequencies\n          over the
      cutoff frequency. The number of elements in individual\n          columns should
      be al least three times the smoothing factor.\n        columns: List of str;
      Set of columns on which to apply the feature generator\n        group_columns:
      List of str; Set of columns by which to aggregate\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20 ,\n
      \                             'Rep': [0] * 20})\n        >>> df['accelx'] =
      [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                  Class  Rep Subject  accelx\n            0
      \  Crawling    0     s01       1\n            1   Crawling    0     s01      -2\n
      \           2   Crawling    0     s01      -3\n            3   Crawling    0
      \    s01       1\n            4   Crawling    0     s01       2\n            5
      \  Crawling    0     s01       5\n            6   Crawling    0     s01       2\n
      \           7   Crawling    0     s01      -2\n            8   Crawling    0
      \    s01      -3\n            9   Crawling    0     s01      -1\n            10
      \ Crawling    0     s01       1\n            11  Crawling    0     s01      -3\n
      \           12  Crawling    0     s01      -4\n            13  Crawling    0
      \    s01       1\n            14  Crawling    0     s01       2\n            15
      \ Crawling    0     s01       6\n            16  Crawling    0     s01       2\n
      \           17  Crawling    0     s01      -3\n            18  Crawling    0
      \    s01      -2\n            19  Crawling    0     s01      -1\n        >>>
      dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data', df,
      force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Global Peak to
      Peak of High Frequency\"],\n                params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                function_defaults={\"columns\":['accelx'],\n
      \                                  'smoothing_factor' : 3})\n        >>> result,
      stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \                 Class  Rep Subject  gen_0001_accelxMaxP2PGlobalAC\n            0
      \ Crawling    0     s01                            8.0\n    "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Amplitude
    has_c_version: true
    c_file_name: fg_amplitude_global_p2p_high_frequency.c
    core: true
    dcl_executable: false
    c_function_name: global_p2p_high_frequency
- model: library.transform
  pk: 47
  fields:
    uuid: c53ea0ab-ba35-4d4e-a71f-637123b57f71
    name: Max Peak to Peak of first half of High Frequency
    function_in_file: max_p2p_half_high_frequency
    input_contract:
      - type: DataFrame
        name: input_data
      - type: int
        name: smoothing_factor
        handle_by_set: true
        description:
          Determines the amount of attenuation for frequencies over the cutoff
          frequency
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Max Peak to Peak of first half of High Frequency. The high
      frequency signal\n    is calculated by subtracting the moving average filter
      output from the original signal.\n\n    Args:\n        input_data:\n        smoothing_factor:
      int; Determines the amount of attenuation for frequencies\n          over the
      cutoff frequency\n        columns: List of str; Set of columns on which to apply
      the feature generator\n        group_columns: List of str; Set of columns by
      which to aggregate\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20 ,\n                               'Rep': [0] * 20})\n        >>>
      df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3,
      -2, -1]\n        >>> df\n            out:\n                  Class  Rep Subject
      \ accelx\n            0   Crawling    0     s01       1\n            1   Crawling
      \   0     s01      -2\n            2   Crawling    0     s01      -3\n            3
      \  Crawling    0     s01       1\n            4   Crawling    0     s01       2\n
      \           5   Crawling    0     s01       5\n            6   Crawling    0
      \    s01       2\n            7   Crawling    0     s01      -2\n            8
      \  Crawling    0     s01      -3\n            9   Crawling    0     s01      -1\n
      \           10  Crawling    0     s01       1\n            11  Crawling    0
      \    s01      -3\n            12  Crawling    0     s01      -4\n            13
      \ Crawling    0     s01       1\n            14  Crawling    0     s01       2\n
      \           15  Crawling    0     s01       6\n            16  Crawling    0
      \    s01       2\n            17  Crawling    0     s01      -3\n            18
      \ Crawling    0     s01      -2\n            19  Crawling    0     s01      -1\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Max Peak
      to Peak of first half of High Frequency\"],\n                 params = {\"group_columns\":
      ['Subject', 'Class', 'Rep']},\n                 function_defaults={\"columns\":['accelx'],\n
      \                                   'smoothing_factor' : 3})\n        >>> result,
      stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \                 Class  Rep Subject  gen_0001_accelxMaxP2P1stHalfAC\n            0
      \ Crawling    0     s01                        6.333333\n\n\n    "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Amplitude
    has_c_version: true
    c_file_name: fg_amplitude_max_p2p_half_high_frequency.c
    core: true
    dcl_executable: false
    c_function_name: max_p2p_half_high_frequency
- model: library.transform
  pk: 48
  fields:
    uuid: 7fc88f96-82ce-4f52-ba9b-574b054d7426
    name: Global Peak to Peak
    function_in_file: peak_to_peak
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Global Peak to Peak of signal.\n\n    Args:\n        input_data:
      DataFrame\n        columns: (list of str): Set of columns on which to apply
      the feature generator\n        group_columns:(list of str): Set of columns by
      which to aggregate\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20 ,\n                               'Rep': [0] * 10 + [1] *10
      })\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4,
      1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n                   Class
      \ Rep Subject  accelx\n            0   Crawling    0     s01       1\n            1
      \  Crawling    0     s01      -2\n            2   Crawling    0     s01      -3\n
      \           3   Crawling    0     s01       1\n            4   Crawling    0
      \    s01       2\n            5   Crawling    0     s01       5\n            6
      \  Crawling    0     s01       2\n            7   Crawling    0     s01      -2\n
      \           8   Crawling    0     s01      -3\n            9   Crawling    0
      \    s01      -1\n            10  Crawling    1     s01       1\n            11
      \ Crawling    1     s01      -3\n            12  Crawling    1     s01      -4\n
      \           13  Crawling    1     s01       1\n            14  Crawling    1
      \    s01       2\n            15  Crawling    1     s01       6\n            16
      \ Crawling    1     s01       2\n            17  Crawling    1     s01      -3\n
      \           18  Crawling    1     s01      -2\n            19  Crawling    1
      \    s01      -1\n\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Global Peak
      to Peak\"],\n                    params = {\"group_columns\": ['Subject', 'Class',
      'Rep']},\n                    function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                  Class  Rep Subject  gen_0001_accelxP2P\n
      \           0  Crawling    0     s01                   8\n            1  Crawling
      \   1     s01                  10\n    "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Amplitude
    has_c_version: true
    c_file_name: fg_amplitude_peak_to_peak.c
    core: true
    dcl_executable: false
    c_function_name: peak_to_peak
- model: library.transform
  pk: 49
  fields:
    uuid: fd87d747-2870-4b3e-b5a2-7b3632e59cb3
    name: Duration of the Signal
    function_in_file: signal_duration
    input_contract:
      - type: DataFrame
        name: input_data
      - type: numeric
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Duration of the signal. It is calculated by dividing the length
      of vector\n    by the sampling rate.\n\n    Args:\n        input_data: DataFrame;\n\n
      \       sample_rate: float; Sampling rate\n\n        columns: List of str; Set
      of columns on which to apply the feature generator\n\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                             'Class': ['Crawling'] * 20 ,\n                             'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n
      \                  Class  Rep Subject  accelx\n            0   Crawling    0
      \    s01       1\n            1   Crawling    0     s01      -2\n            2
      \  Crawling    0     s01      -3\n            3   Crawling    0     s01       1\n
      \           4   Crawling    0     s01       2\n            5   Crawling    0
      \    s01       5\n            6   Crawling    0     s01       2\n            7
      \  Crawling    0     s01      -2\n            8   Crawling    1     s01      -3\n
      \           9   Crawling    1     s01      -1\n            10  Crawling    1
      \    s01       1\n            11  Crawling    1     s01      -3\n            12
      \ Crawling    1     s01      -4\n            13  Crawling    1     s01       1\n
      \           14  Crawling    1     s01       2\n            15  Crawling    1
      \    s01       6\n            16  Crawling    1     s01       2\n            17
      \ Crawling    1     s01      -3\n            18  Crawling    1     s01      -2\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Duration
      of the Signal\"],\n                    params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                    function_defaults={\"columns\":['accelx'],\n
      \                                      'sample_rate' : 5})\n        >>> result,
      stats = dsk.pipeline.execute()\n        >>> print result\n            out:\n
      \                 Class  Rep Subject  gen_0001_accelxDurSignal\n            0
      \ Crawling    0     s01                       1.6\n            1  Crawling    1
      \    s01                       2.4\n    "
    path: "../library/core_functions/fg_time.py"
    type: Feature Generator
    subtype: Time
    has_c_version: true
    c_file_name: fg_time_signal_duration.c
    core: true
    dcl_executable: false
    c_function_name: signal_duration
- model: library.transform
  pk: 50
  fields:
    uuid: 1ed959b2-000e-478e-8b36-b811ed2933dc
    name: Percent Time Over Zero
    function_in_file: pct_time_over_zero
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Percentage of samples in the series that are positive.\n\n
      \   Args:\n        input_data: DataFrame\n\n        columns: List of str; Set
      of columns on which to apply the feature generator\n\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20 ,\n
      \                             'Rep': [0] * 8 + [1] * 12})\n        >>> df['accelx']
      = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                   Class  Rep Subject  accelx\n            0
      \  Crawling    0     s01       1\n            1   Crawling    0     s01      -2\n
      \           2   Crawling    0     s01      -3\n            3   Crawling    0
      \    s01       1\n            4   Crawling    0     s01       2\n            5
      \  Crawling    0     s01       5\n            6   Crawling    0     s01       2\n
      \           7   Crawling    0     s01      -2\n            8   Crawling    1
      \    s01      -3\n            9   Crawling    1     s01      -1\n            10
      \ Crawling    1     s01       1\n            11  Crawling    1     s01      -3\n
      \           12  Crawling    1     s01      -4\n            13  Crawling    1
      \    s01       1\n            14  Crawling    1     s01       2\n            15
      \ Crawling    1     s01       6\n            16  Crawling    1     s01       2\n
      \           17  Crawling    1     s01      -3\n            18  Crawling    1
      \    s01      -2\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Percent Time
      Over Zero\"],\n                    params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                    function_defaults={\"columns\":['accelx'],
      'sample_rate' : 5})\n        >>> result, stats = dsk.pipeline.execute()\n        >>>
      print result\n            out:\n                  Class  Rep Subject  gen_0001_accelxPctTimeOverZero\n
      \           0  Crawling    0     s01                        0.625000\n            1
      \ Crawling    1     s01                        0.416667\n    "
    path: "../library/core_functions/fg_time.py"
    type: Feature Generator
    subtype: Time
    has_c_version: true
    c_file_name: fg_time_pct_time_over_zero.c
    core: true
    dcl_executable: false
    c_function_name: pct_time_over_zero
- model: library.transform
  pk: 51
  fields:
    uuid: fab72283-4bbe-4d8e-8a70-9d6c5e0bc6e0
    name: Percent Time Over Sigma
    function_in_file: pct_time_over_sigma
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Percentage of samples in the series that are above the sample
      mean + one sigma\n\n    Args:\n        input_data: DataFrame\n\n        columns:
      List of str; Set of columns on which to apply the feature generator\n\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                    'Class': ['Crawling'] * 20 ,\n                    'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n
      \                  Class  Rep Subject  accelx\n            0   Crawling    0
      \    s01       1\n            1   Crawling    0     s01      -2\n            2
      \  Crawling    0     s01      -3\n            3   Crawling    0     s01       1\n
      \           4   Crawling    0     s01       2\n            5   Crawling    0
      \    s01       5\n            6   Crawling    0     s01       2\n            7
      \  Crawling    0     s01      -2\n            8   Crawling    1     s01      -3\n
      \           9   Crawling    1     s01      -1\n            10  Crawling    1
      \    s01       1\n            11  Crawling    1     s01      -3\n            12
      \ Crawling    1     s01      -4\n            13  Crawling    1     s01       1\n
      \           14  Crawling    1     s01       2\n            15  Crawling    1
      \    s01       6\n            16  Crawling    1     s01       2\n            17
      \ Crawling    1     s01      -3\n            18  Crawling    1     s01      -2\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Percent Time Over Sigma\"],\n                    params
      = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n                    function_defaults={\"columns\":['accelx'],
      'sample_rate' : 5})\n        >>> result, stats = dsk.pipeline.execute()\n        >>>
      print result\n            out:\n                  Class  Rep Subject  gen_0001_accelxPctTimeOverSigma\n
      \           0  Crawling    0     s01                         0.125000\n            1
      \ Crawling    1     s01                         0.083333\n    "
    path: "../library/core_functions/fg_time.py"
    type: Feature Generator
    subtype: Time
    has_c_version: true
    c_file_name: fg_time_pct_time_over_sigma.c
    core: true
    dcl_executable: false
    c_function_name: pct_time_over_sigma
- model: library.transform
  pk: 52
  fields:
    uuid: 6f441332-6dbc-4cbb-ad9e-c6a059b5efe8
    name: Percent Time Over Second Sigma
    function_in_file: pct_time_over_second_sigma
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Percentage of samples in the series that are above  the sample
      mean + two sigma\n\n    Args:\n        input_data: DataFrame\n\n        columns:
      List of str; Set of columns on which to apply the feature generator\n\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20 ,\n
      \                             'Rep': [0] * 8 + [1] * 12})\n        >>> df['accelx']
      = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                   Class  Rep Subject  accelx\n            0
      \  Crawling    0     s01       1\n            1   Crawling    0     s01      -2\n
      \           2   Crawling    0     s01      -3\n            3   Crawling    0
      \    s01       1\n            4   Crawling    0     s01       2\n            5
      \  Crawling    0     s01       5\n            6   Crawling    0     s01       2\n
      \           7   Crawling    0     s01      -2\n            8   Crawling    1
      \    s01      -3\n            9   Crawling    1     s01      -1\n            10
      \ Crawling    1     s01       1\n            11  Crawling    1     s01      -3\n
      \           12  Crawling    1     s01      -4\n            13  Crawling    1
      \    s01       1\n            14  Crawling    1     s01       2\n            15
      \ Crawling    1     s01       6\n            16  Crawling    1     s01       2\n
      \           17  Crawling    1     s01      -3\n            18  Crawling    1
      \    s01      -2\n        >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n
      \       >>> dsk.pipeline.add_feature_generator([\"Percent Time Over Second Sigma\"],\n
      \                   params = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n
      \                   function_defaults={\"columns\":['accelx'], 'sample_rate'
      : 5})\n        >>> result, stats = dsk.pipeline.execute()\n        >>> print
      result\n            out:\n                  Class  Rep Subject  accelxPctTimeOver2ndSigma\n
      \           0  Crawling    0     s01                   0.000000\n            1
      \ Crawling    1     s01                   0.083333\n    "
    path: "../library/core_functions/fg_time.py"
    type: Feature Generator
    subtype: Time
    has_c_version: true
    c_file_name: fg_time_pct_time_over_second_sigma.c
    core: true
    dcl_executable: false
    c_function_name: pct_time_over_second_sigma
- model: library.transform
  pk: 53
  fields:
    uuid: 8ff62e0a-a883-49ca-8371-c5cfc9a05375
    name: Average of Movement Intensity
    function_in_file: average_movement_intensity
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Average of movement intensity\n\n    Args:\n        input_data:
      input DataFrame.\n        columns:  List of str; The `columns` represents a
      list of all column\n            names on which `are` is to be found.\n        group_columns:
      List of str; Set of columns by which to aggregate\n\n    Returns:\n        DataFrame\n\n
      \   "
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: true
    c_file_name: fg_physical_average_movement_intensity.c
    core: true
    dcl_executable: false
    c_function_name: average_movement_intensity
- model: library.transform
  pk: 54
  fields:
    uuid: e0f67730-a794-4d5f-ad0b-637603e631a6
    name: Variance of Movement Intensity
    function_in_file: variance_movement_intensity
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Variance of movement intensity\n\n    Args:\n        input_data:
      input DataFrame.\n        columns:  List of str; The `columns` represents a
      list of all column\n          names on which `variance_movement_intensity` is
      to be found.\n        group_columns: List of str; Set of columns by which to
      aggregate\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n    Examples:\n
      \       >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20,\n                               'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df['accely'] = [0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df['accelz'] = [1, -2,
      3, -1, 2, 5, 2, -2, -3, 1, 1, 3, 4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n
      \           out:\n                       Class  Rep Subject  accelx  accely
      \ accelz\n                0   Crawling    0     s01       1       0       1\n
      \               1   Crawling    0     s01      -2       9      -2\n                2
      \  Crawling    0     s01      -3       5       3\n                3   Crawling
      \   0     s01       1      -5      -1\n                4   Crawling    0     s01
      \      2      -9       2\n                5   Crawling    0     s01       5
      \      0       5\n                6   Crawling    0     s01       2       9
      \      2\n                7   Crawling    0     s01      -2       5      -2\n
      \               8   Crawling    1     s01      -3      -5      -3\n                9
      \  Crawling    1     s01      -1      -9       1\n                10  Crawling
      \   1     s01       1       0       1\n                11  Crawling    1     s01
      \     -3       9       3\n                12  Crawling    1     s01      -4
      \      5       4\n                13  Crawling    1     s01       1      -5
      \      1\n                14  Crawling    1     s01       2      -9       2\n
      \               15  Crawling    1     s01       6       0       6\n                16
      \ Crawling    1     s01       2       9       2\n                17  Crawling
      \   1     s01      -3       5      -3\n                18  Crawling    1     s01
      \     -2      -5      -2\n                19  Crawling    1     s01      -1
      \     -9      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Variance
      of Movement Intensity\"],\n             params = {\"group_columns\": ['Subject',
      'Class', 'Rep']},\n                        function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0000_VarInt\n            0  Crawling    0     s01         6.704651\n            1
      \ Crawling    1     s01         5.555739\n    "
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name: fg_physical_variance_movement_intensity.c
    core: true
    dcl_executable: false
    c_function_name: variance_movement_intensity
- model: library.transform
  pk: 55
  fields:
    uuid: 30a525cf-f79f-4ba0-8800-5a44700c353d
    name: Average Signal Magnitude Area (Average Energy of Movement Intensity)
    function_in_file: average_signal_magnitude_area
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Average signal magnitude area (average energy of movement
      intensity)\n\n    Args:\n        input_data: input DataFrame.\n        columns:
      \ List of str; The `columns` represents a list of all column\n          names
      on which `average_signal_magnitude_area` is to be found.\n        group_columns:
      List of str; Set of columns by which to aggregate\n        **kwargs:\n\n    Returns:\n
      \       DataFrame\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                              'Class': ['Crawling'] * 20,\n                              'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df['accely'] = [0, 9,
      5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df['accelz']
      = [1, -2, 3, -1, 2, 5, 2, -2, -3, 1, 1, 3, 4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                       Class  Rep Subject  accelx  accely
      \ accelz\n                0   Crawling    0     s01       1       0       1\n
      \               1   Crawling    0     s01      -2       9      -2\n                2
      \  Crawling    0     s01      -3       5       3\n                3   Crawling
      \   0     s01       1      -5      -1\n                4   Crawling    0     s01
      \      2      -9       2\n                5   Crawling    0     s01       5
      \      0       5\n                6   Crawling    0     s01       2       9
      \      2\n                7   Crawling    0     s01      -2       5      -2\n
      \               8   Crawling    1     s01      -3      -5      -3\n                9
      \  Crawling    1     s01      -1      -9       1\n                10  Crawling
      \   1     s01       1       0       1\n                11  Crawling    1     s01
      \     -3       9       3\n                12  Crawling    1     s01      -4
      \      5       4\n                13  Crawling    1     s01       1      -5
      \      1\n                14  Crawling    1     s01       2      -9       2\n
      \               15  Crawling    1     s01       6       0       6\n                16
      \ Crawling    1     s01       2       9       2\n                17  Crawling
      \   1     s01      -3       5      -3\n                18  Crawling    1     s01
      \     -2      -5      -2\n                19  Crawling    1     s01      -1
      \     -9      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Average Signal
      Magnitude Area (Average Energy of Movement Intensity)\"],\n                 params
      = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n                 function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0000_AvgSigMag\n            0  Crawling    0     s01            9.750000\n
      \           1  Crawling    1     s01           10.666667\n    "
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: true
    c_file_name: fg_physical_average_signal_magnitude_area.c
    core: true
    dcl_executable: false
    c_function_name: average_signal_magnitude_area
- model: library.transform
  pk: 56
  fields:
    uuid: 015ea79e-8dc4-4bbb-ad17-de29dba81924
    name: Dominant Direction of Linear Velocity
    function_in_file: dominant_direction_linear
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description: Dominant direction of linear velocity.
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 57
  fields:
    uuid: 9f382a6d-6acc-4993-af89-13b7c6632b9f
    name: Second Direction of Linear Velocity
    function_in_file: second_direction_linear
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description: Second direction of linear velocity.
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 58
  fields:
    uuid: 7e1dc3d7-d11c-4d9b-b9dd-1033521454e6
    name: Dominant Direction of Angular Velocity
    function_in_file: dominant_direction_angular
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description: Dominant direction of angular velocity.
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 59
  fields:
    uuid: 64190a6e-934c-495d-930c-4a1f04f32138
    name: Second Direction of Angular Velocity
    function_in_file: second_direction_angular
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description: Second direction of angular velocity.
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 60
  fields:
    uuid: 2e37cfcf-49e2-4467-9adb-4458f33365d3
    name: CAGH - Correlation of Linear Acceleration between Gravity and Heading Direction
    function_in_file: cagh
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    CAGH - Correlation of linear acceleration between gravity
      and heading direction.\n\n    Args:\n        input_data: input DataFrame.\n
      \       columns:  List of str; The `columns` represents a list of all\n            column
      names on which `cagh` is to be found.\n        group_columns: List of str; Set
      of columns by which to aggregate\n\n    Returns:\n        DataFrame\n\n    Examples:\n
      \       >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                             'Class':
      ['Crawling'] * 20,\n                             'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df['accely'] = [0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df['accelz'] = [1, -2,
      3, -1, 2, 5, 2, -2, -3, 1, 1, 3, 4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n
      \           out:\n                       Class  Rep Subject  accelx  accely
      \ accelz\n                0   Crawling    0     s01       1       0       1\n
      \               1   Crawling    0     s01      -2       9      -2\n                2
      \  Crawling    0     s01      -3       5       3\n                3   Crawling
      \   0     s01       1      -5      -1\n                4   Crawling    0     s01
      \      2      -9       2\n                5   Crawling    0     s01       5
      \      0       5\n                6   Crawling    0     s01       2       9
      \      2\n                7   Crawling    0     s01      -2       5      -2\n
      \               8   Crawling    1     s01      -3      -5      -3\n                9
      \  Crawling    1     s01      -1      -9       1\n                10  Crawling
      \   1     s01       1       0       1\n                11  Crawling    1     s01
      \     -3       9       3\n                12  Crawling    1     s01      -4
      \      5       4\n                13  Crawling    1     s01       1      -5
      \      1\n                14  Crawling    1     s01       2      -9       2\n
      \               15  Crawling    1     s01       6       0       6\n                16
      \ Crawling    1     s01       2       9       2\n                17  Crawling
      \   1     s01      -3       5      -3\n                18  Crawling    1     s01
      \     -2      -5      -2\n                19  Crawling    1     s01      -1
      \     -9      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"CAGH - Correlation
      of Linear Acceleration between Gravity and Heading Direction\"],\n                    params
      = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n                    function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0000_CAGH\n            0  Crawling    0     s01       0.252282\n            1
      \ Crawling    1     s01      -0.144809\n    Notes:\n        Zhang, Mi, and Alexander
      A. Sawchuk. \"Motion primitive-based human activity recognition\n        using
      a bag-of-features approach.\"  Proceedings of the 2nd ACM SIGHIT International\n
      \       Health Informatics Symposium. ACM, 2012.\n    "
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name: fg_physical_cagh.c
    core: true
    dcl_executable: false
    c_function_name: cagh
- model: library.transform
  pk: 61
  fields:
    uuid: 4d7c8707-dba1-4562-859c-2774ba27a709
    name: ARATG - Average Angle of Rotation in the Direction of Gravity
    function_in_file: aratg
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    ARATG - Average angle of rotation in the direction of gravity\n
      \   Must have at least four columns. Always looking for 4th column in the columns
      list\n\n    Args:\n        input_data: input DataFrame.\n        columns:  List
      of str; The `columns` represents a list of all column\n          names on which
      `aratg` is to be found.\n        group_columns: List of str; Set of columns
      by which to aggregate\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20,\n                               'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df['accely'] = [0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df['accelz'] = [1, -2,
      3, -1, 2, 5, 2, -2,-3, 1, 1, 3, 4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n
      \           out:\n                       Class  Rep Subject  accelx  accely
      \ accelz\n                0   Crawling    0     s01       1       0       1\n
      \               1   Crawling    0     s01      -2       9      -2\n                2
      \  Crawling    0     s01      -3       5       3\n                3   Crawling
      \   0     s01       1      -5      -1\n                4   Crawling    0     s01
      \      2      -9       2\n                5   Crawling    0     s01       5
      \      0       5\n                6   Crawling    0     s01       2       9
      \      2\n                7   Crawling    0     s01      -2       5      -2\n
      \               8   Crawling    1     s01      -3      -5      -3\n                9
      \  Crawling    1     s01      -1      -9       1\n                10  Crawling
      \   1     s01       1       0       1\n                11  Crawling    1     s01
      \     -3       9       3\n                12  Crawling    1     s01      -4
      \      5       4\n                13  Crawling    1     s01       1      -5
      \      1\n                14  Crawling    1     s01       2      -9       2\n
      \               15  Crawling    1     s01       6       0       6\n                16
      \ Crawling    1     s01       2       9       2\n                17  Crawling
      \   1     s01      -3       5      -3\n                18  Crawling    1     s01
      \     -2      -5      -2\n                19  Crawling    1     s01      -1
      \     -9      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"ARATG - Average
      Angle of Rotation in the Direction of Gravity\"],\n                params =
      {\"group_columns\": ['Subject', 'Class', 'Rep']},\n                function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  ARATG     Class
      \ Rep Subject\n            0  1.000000  Crawling    0     s01\n            1
      \ 0.916667  Crawling    1     s01\n    Notes:\n        Zhang, Mi, and Alexander
      A. Sawchuk. \"Motion primitive-based human\n        activity recognition using
      a bag-of-features approach.\" Proceedings\n        of the 2nd ACM SIGHIT International
      Health Informatics Symposium. ACM, 2012.\n    "
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name: fg_physical_aratg.c
    core: true
    dcl_executable: false
    c_function_name: aratg
- model: library.transform
  pk: 62
  fields:
    uuid: 875f18e7-d4c6-4793-ab6b-a674cfa50e73
    name: Angular Displacement in both Angular and Cartesian coordinate system
    function_in_file: angular_displacement
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - type: int
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      Angular displacement in both Angular and Cartesian coordinate system
      .
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 63
  fields:
    uuid: 51311f94-52d4-4b7e-8742-0b4bfb069f4b
    name: Compressed Subband Cepstral Coefficients
    function_in_file: cscc
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description: Compressed Subband Cepstral Coefficients.
    path: "../library/core_functions/fg_physical.py"
    type: Feature Generator
    subtype: Physical
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 66
  fields:
    uuid: c3489ae0-2bb1-4363-9737-f8e1354ec570
    name: Total Energy
    function_in_file: total_energy
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Total Energy.\n\n    1) Calculate the element-wise square
      of the input columns.\n    2) Sum the energy values over all streams to get
      the total energy.\n\n    Args:\n        input_data: input DataFrame.\n\n        columns:
      \ List of str; The `columns` represents a list of all column names\n                 on
      which total energy is to be found.\n\n        group_columns: List of str; Set
      of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n        Dataframe\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                              'Class':
      ['Crawling'] * 20,\n                               'Rep': [0] * 8 + [1] * 12})\n
      \       >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4, 1,
      2, 6, 2, -3, -2, -1]\n        >>> df['accely'] = [0, 9, 5, -5, -9, 0, 9, 5,
      -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df['accelz'] = [1, -2,
      3, -1, 2, 5, 2, -2, -3, 1, 1, 3, 4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df\n
      \           out:\n                       Class  Rep Subject  accelx  accely
      \ accelz\n                0   Crawling    0     s01       1       0       1\n
      \               1   Crawling    0     s01      -2       9      -2\n                2
      \  Crawling    0     s01      -3       5       3\n                3   Crawling
      \   0     s01       1      -5      -1\n                4   Crawling    0     s01
      \      2      -9       2\n                5   Crawling    0     s01       5
      \      0       5\n                6   Crawling    0     s01       2       9
      \      2\n                7   Crawling    0     s01      -2       5      -2\n
      \               8   Crawling    1     s01      -3      -5      -3\n                9
      \  Crawling    1     s01      -1      -9       1\n                10  Crawling
      \   1     s01       1       0       1\n                11  Crawling    1     s01
      \     -3       9       3\n                12  Crawling    1     s01      -4
      \      5       4\n                13  Crawling    1     s01       1      -5
      \      1\n                14  Crawling    1     s01       2      -9       2\n
      \               15  Crawling    1     s01       6       0       6\n                16
      \ Crawling    1     s01       2       9       2\n                17  Crawling
      \   1     s01      -3       5      -3\n                18  Crawling    1     s01
      \     -2      -5      -2\n                19  Crawling    1     s01      -1
      \     -9      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Total Energy\"],\n
      \            params = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n             function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0000_TotEng\n            0  Crawling    0     s01              422\n            1
      \ Crawling    1     s01              720\n    "
    path: "../library/core_functions/fg_energy.py"
    type: Feature Generator
    subtype: Energy
    has_c_version: true
    c_file_name: fg_energy_total_energy.c
    core: true
    dcl_executable: false
    c_function_name: total_energy
- model: library.transform
  pk: 67
  fields:
    uuid: 481bbc23-a0e7-4982-aff1-ed60421a9db7
    name: Downsample
    function_in_file: downsampler_for_features
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the transform
      - type: int
        name: new_length
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: data_out
        family: true
    description:
      "\n    This function takes `input_data` dataframe as input and group
      by `group_columns`.\n    Then for each group, it drops the `passthrough_columns`
      and perform downsampling\n    on the remaining columns.\n\n    On each column,
      perform the following steps:\n\n    - Divide the entire column into windows
      of size total length/`new_length`.\n    - Calculate mean for each window\n    -
      Concatenate all the mean values.\n    - The length of the downsampled signal
      is equal to 'new length'.\n\n    Then all such means are concatenated to get
      `new_length` * # of columns. These constitute\n    features in downstream analyses.
      For instance, if there are three columns and the\n    `new_length` value is
      12, then total number of means we get is 12 * 3 = 36. Each will represent a
      feature.\n\n    Args:\n        input_data: dataframe\n        columns: List
      of columns to be downsampled\n        group_columns (a list): List of columns
      on which grouping is to be done.\n                                 Each group
      will go through downsampling one at a time\n        new_length: integer; Downsampled
      length\n        **kwargs:\n\n    Returns:\n        DataFrame; downsampled dataframe\n\n
      \   Examples:\n        >>> from pandas import DataFrame\n        >>> df = DataFrame([[3,
      3], [4, 5], [5, 7], [4, 6],\n                            [3, 1], [3, 1], [4,
      3], [5, 5],\n                            [4, 7], [3, 6]], columns=['accelx',
      'accely'])\n        >>> df\n        Out:\n           accelx  accely\n        0
      \      3       3\n        1       4       5\n        2       5       7\n        3
      \      4       6\n        4       3       1\n        5       3       1\n        6
      \      4       3\n        7       5       5\n        8       4       7\n        9
      \      3       6\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Downsample\"],\n
      \                params = {\"group_columns\": []},\n                           function_defaults={\"columns\":['accelx',
      'accely'],\n                                             'new_length' : 5})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           Out:\n                   accelx_1  accelx_2  accelx_3  accelx_4
      \ accelx_5  accely_1  accely_2\n                0       3.5       4.5         3
      \      4.5       3.5         4       6.5\n                   accely_3  accely_4
      \ accely_5\n                0         1         4       6.5\n    "
    path: "../library/core_functions/fg_downsampling.py"
    type: Feature Generator
    subtype: Sampling
    has_c_version: true
    c_file_name: fg_sampling_downsample.c
    core: true
    dcl_executable: false
    c_function_name: downsampler_for_features
- model: library.transform
  pk: 68
  fields:
    uuid: 20d402ac-6abc-414b-98bc-06e1b755807e
    name: Variance Threshold
    function_in_file: vt_select
    input_contract:
      - type: DataFrame
        name: input_data
      - default: 0.05
        type: float
        name: threshold
        description:
          Minimum variance threshold under which features should be eliminated
          (0 to 1)
      - element_type: str
        type: list
        name: passthrough_columns
        handle_by_set: true
        description: The set of columns the selector should ignore
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Feature selector that removes all low-variance features. This
      step is an unsupervised feature selection algorithm     and looks only at the
      input features (X) and not the Labels or outputs (y).  Select features whose
      variance exceeds     the given threshold (default is set to 0.05). It should
      be applied prior to standardization.\n\n    Args:\n        input_data: DataFrame\n\n
      \       threshold: float; default = 0.05. Minimum variance threshold under which
      features should be eliminated (0 to 1)\n\n        passthrough_columns: list
      of column names; The set of columns the selector should ignore\n\n    Returns:\n
      \       DataFrame: DataFrame which includes selected features and the passthrough
      columns.\n\n    "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Unsupervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 69
  fields:
    uuid: 105fbef6-0cbc-48d1-9294-519b53a57b76
    name: Univariate Selection
    function_in_file: univariate_select
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: label_column
        handle_by_set: true
      - type: int
        name: number_of_features
        handle_by_set: true
        description:
          The number of     features you would like the selector to reduce
          to
      - element_type: str
        type: list
        name: passthrough_columns
        handle_by_set: true
        description: The set of columns the selector should ignore
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Select features with the highest univariate (ANOVA) F-values;
      It is supervised feature selection method and requires     both a input features
      and labels.\n\n    Args:\n        input_data: Pandas DataFrame; Input DataFrame\n\n
      \       label_column: str; columns name of the label or output\n\n        number_of_features:
      int; The number of features you would like the selector to reduce to.\n\n        passthrough_columns:
      The set of columns the selector should ignore\n\n    Returns:\n        DataFrame:
      DataFrame which includes selected features and the passthrough columns.\n\n
      \   Notes:\n        Please see the following for more information:\n        http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n
      \       http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif\n\n
      \   "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Supervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 70
  fields:
    uuid: 9824c04a-0da0-4e12-ae72-9d8337923927
    name: Correlation Threshold
    function_in_file: correlation_select_remove_most_corr_first
    input_contract:
      - type: DataFrame
        name: input_data
      - type: float
        name: threshold
        description:
          Maximum correlation     threshold over which features should be
          eliminated (0 to 1)
      - element_type: str
        type: list
        name: passthrough_columns
        handle_by_set: true
        description: The set of columns the selector should ignore
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is an unsupervised feature selection algorithm that selects
      features based\n    on absolute correlation (similar to backward feature selection).
      It first calculates\n    a pair-wise correlation matrix consisting of all features.
      Then, a candidate feature\n    is identified for removal. This candidate feature
      is the one that correlates to the\n    maximum number of other features having
      correlation coefficient higher than the\n    `threshold`. This step is repeated
      until there is no feature with correlation\n    coefficient higher that the
      `threshold` or when there is no feature left.\n\n    Args:\n        input_data:
      DataFrame\n        threshold: float; default = 0.85. Minimum correlation threshold
      over which\n            features should be eliminated (0 to 1)\n        passthrough_columns:
      list of column names; The set of columns the selector should\n            ignore\n\n
      \   Returns:\n        DataFrame: DataFrame which includes selected features
      and the passthrough columns.\n\n    "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Unsupervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 71
  fields:
    uuid: 58a5287b-77e0-4912-b247-cab6332701d0
    name: Recursive Feature Elimination
    function_in_file: rfe_select
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: label_column
        handle_by_set: true
      - description: Selection method
        type: str
        name: method
        options:
          - name: Log R
          - name: Linear SVC
      - type: int
        name: number_of_features
        handle_by_set: true
        description:
          The number of     features you would like the selector to reduce
          to
      - element_type: str
        type: list
        name: passthrough_columns
        handle_by_set: true
        description: The set of columns the selector should ignore
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is a supervised method of feature selection.  The goal
      of recursive feature elimination (RFE) is to select     features by recursively
      considering smaller and smaller sets of features. First, the estimator (`method`:
      'Log R' or 'Linear SVC')     is trained on the initial set of features and weights
      are assigned to each one of them. Then, features whose absolute weights are
      \    the smallest are pruned from the current set features. That procedure is
      recursively repeated on the pruned set     until the desired number of features
      `number_of_features` to select is eventually reached.\n\n\n    Args:\n        input_data:
      Pandas DataFrame; Input DataFrame\n\n        label_column: str; columns name
      of the label or output\n\n        method: str; The type of selection method.
      Two options available: 1) `Log R` and 2) `Linear SVC`.  For `Log R`,             the
      value of Inverse of regularization strength `C` is default to 1.0 and `penalty
      is defaulted to `l1`.             For `Linear SVC`, the default for `C` is 0.01,
      penalty is `l1` and dual is set to `False`.\n\n        number_of_features: int;
      The number of features you would like the selector to reduce to.\n\n        passthrough_columns:
      The set of columns the selector should ignore\n\n    Returns:\n        DataFrame:
      DataFrame which includes selected features and the passthrough columns.\n\n
      \   Notes:\n        For more information on defaults of `Log R`, please see:
      http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n
      \       For `Linear SVC`, please see: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n\n
      \   "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Supervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 72
  fields:
    uuid: e57d104e-b3f2-4c12-aa8d-c35e6d92b979
    name: Tree-based Selection
    function_in_file: tree_select
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: label_column
        handle_by_set: true
      - element_type: str
        type: list
        name: passthrough_columns
        handle_by_set: true
        description: The set of columns the selector should ignore
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Select features using a supervised tree-based algorithm. This
      class implements a meta estimator that fits a number     of randomized decision
      trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging
      to improve     the predictive accuracy and control overfitting. The default
      number of trees in the forest is set at 250, and the     `random_state` to be
      0. Please see notes for more information.\n\n    Args:\n\n        input_data:
      DataFrame\n\n        label_column: str; columns name of the label or output\n\n
      \       passthrough_columns: list of column names; The set of columns the selector
      should ignore\n\n    Returns:\n        DataFrame: DataFrame which includes selected
      features and the passthrough columns.\n\n    Notes:\n        For more information,
      please see: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n\n
      \   "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Supervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 73
  fields:
    uuid: 1ba8dcfa-2ffc-4f0f-b03d-0c6bd4782786
    name: PME
    function_in_file: pme
    input_contract:
      - type: str
        name: distance_mode
        options:
          - name: L1
          - name: Lsup
      - type: str
        name: classification_mode
        options:
          - name: RBF
          - name: KNN
      - type: int
        name: max_aif
      - type: int
        name: min_aif
    output_contract: []
    description: Corresponds to the PME classifier on the AtlasPeak device
    path: "../library/core_functions/mg_contracts.py"
    type: Classifier
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 74
  fields:
    uuid: 76a47a79-17cc-4cd3-a389-3057654325ee
    name: Leave-One-Subject-Out
    function_in_file: leave_one_subject_out
    input_contract:
      - element_type: str
        type: list
        name: group_columns
    output_contract: []
    description:
      "\n    A cross-validation scheme which holds out the samples for
      all but one subject for testing\n    in each fold. In other words, for a data
      set consisting of 10 subjects, each fold will\n    consist of a training set
      from 9 subjects and test set from 1 subject; thus, in all, there\n    will be
      10 folds, one for each left out test subject.\n\n    Args:\n        group_columns
      (list[str]): list of column names that define the groups (subjects)\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Validation Method
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 75
  fields:
    uuid: 3f9eaedf-ae91-49b2-b42c-7c98e828d8f6
    name: Stratified K-Fold Cross-Validation
    function_in_file: k_fold_strat
    input_contract:
      - type: int
        name: number_of_folds
    output_contract: []
    description:
      "\n    A variation of k-fold which returns stratified folds: each
      set contains approximately the\n    same percentage of samples of each target
      class as the complete set. In other words, for a\n    data set consisting of
      total 100 samples with 40 samples from class 1 and 60 samples from\n    class
      2, for a stratified 2-fold scheme, each fold will consist of total 50 samples
      with 20\n    samples from class 1 and 30 samples from class 2.\n\n    Args:\n
      \       number_of_folds (int): the number of stratified folds to produce\n\n
      \   "
    path: "../library/core_functions/mg_contracts.py"
    type: Validation Method
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 76
  fields:
    uuid: d5d8f701-50c1-4de6-8a07-33ba8e77afe7
    name: Set Sample Validation
    function_in_file: set_sampler
    input_contract:
      - type: list
        name: set_mean
      - type: list
        name: set_stdev
      - type: list
        name: mean_limit
      - type: list
        name: stdev_limit
      - type: int
        name: retries
      - type: dict
        name: samples_per_class
      - type: str
        name: norm
        options:
          - name: L1
          - name: Lsup
      - type: str
        name: optimize_mean_std
        options:
          - name: both
          - name: mean
    output_contract: []
    description:
      "\n    A validation scheme wherein the data set is divided into training
      and test sets based\n    on some statistical parameter like mean and standard
      deviation.\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Validation Method
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 77
  fields:
    uuid: ae609a34-db32-40d3-a68a-a6b6770418fc
    name: Stratified Shuffle Split
    function_in_file: stratified_shuffle_split
    input_contract:
      - type: float
        name: test_size
      - type: float
        name: validation_size
      - default: 1
        type: int
        name: number_of_folds
    output_contract: []
    description:
      "\n    A validation scheme which splits the data set into training,
      validation, and (optionally)\n    test sets based on the parameters provided,
      with similar distribution of labels (hence\n    stratified). In other words,
      for a data set consisting of 100 samples in total with 40\n    samples from
      class 1 and 60 samples from class 2, for stratified shuffle split with\n    validation_size
      = 0.4 and reserve_test = False, the validation set will consist of 40\n    samples
      with 16 samples from class 1 and 24 samples from class 2, and the training set
      will\n    consist of 60 samples with 24 samples from class 1 and 36 samples
      from class 2. For each fold,\n    training and validation data re-shuffle and
      split.\n\n    Args:\n        test_size (float): target percent of total size
      to use for testing\n        validation_size (float): target percent of total
      size to use for validation\n        reserve_test (boolean): whether or not to
      reserve a set of data, unseen by any training,\n          for final testing
      (default is False)\n        number_of_folds (int): the number of stratified
      folds (iteration) to produce\n\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Validation Method
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 78
  fields:
    uuid: aba9cdb8-52c1-4933-afdc-df312e71a3e0
    name: Split by Metadata Value
    function_in_file: split_by_metadata_value
    input_contract:
      - type: str
        name: metadata_name
      - element_type: str
        type: list
        name: training_values
      - element_type: str
        type: list
        name: validation_values
    output_contract: []
    description:
      "\n    A validation scheme wherein the data set is divided into training
      and test sets based\n    on the metadata value. In other words, for a data set
      consisting of 100 samples with the\n    metadata column set to 'train' for 60
      samples, and 'test' for 40 samples, the training set\n    will consist of 60
      samples for which the metadata value is 'train' and the test set will\n    consist
      of 40 samples for which the metadata value is 'test'.\n\n    Args:\n        metadata_name
      (str): name of the metadata column to use for splitting\n        training_values
      (list[str]): list of values of the named column to select samples for\n          training\n
      \       validation_values (list[str)): list of values of the named column to
      select samples for\n          validation\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Validation Method
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 79
  fields:
    uuid: 5abd3dd1-9a9f-4de2-ab3f-0998a98ed5cc
    name: Recall
    function_in_file: recall
    input_contract: []
    output_contract: []
    description:
      "\n    The simplest validation method, wherein the training set itself
      is used as the\n    test set. In other words, for a data set consisting of 100
      samples in total, both\n    the training set and the test set consist of the
      same set of 100 samples.\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Validation Method
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 80
  fields:
    uuid: f313ecae-5090-485f-8183-354836970102
    name: RBF with Neuron Allocation Optimization
    function_in_file: vanilla_burlington
    input_contract:
      - type: DataFrame
        name: input_data
        handle_by_set: true
      - type: str
        name: label_column
        handle_by_set: true
      - type: int
        "True": false
        name: number_of_iterations
      - element_type: str
        type: list
        name: ignore_columns
        handle_by_set: true
      - element_type: str
        type: list
        name: classifiers
        handle_by_set: true
        options:
          - name: PME
      - element_type: str
        type: list
        name: validation_methods
        handle_by_set: true
        options:
          - name: Stratified K-Fold Cross-Validation
          - name: Leave-One-Subject-Out
          - name: Stratified Shuffle Split
      - type: bool
        name: turbo
        handle_by_set: false
      - default: 128
        type: int
        name: number_of_neurons
        handle_by_set: false
      - type: bool
        name: aggressive_neuron_creation
    output_contract: []
    description:
      "\n    RBF with Neuron Allocation Optimization takes as input feature
      vectors, corresponding\n    class labels, and desired number of iterations (or
      trials), and outputs a set of\n    models. For each iteration the input vectors
      are randomly shuffled and presented to\n    the PME (\"Burlington\") simulator
      which either places the pattern as a neuron or does\n    not. When a neuron
      is placed, an area of influence (AIF) is determined based on the\n    neuron's
      proximity to other neurons in the model and their respective classes.\n\n    Args:\n
      \       input_data (DataFrame): input feature vectors with a label column\n
      \       label_column (str): the name of the column in input_data containing
      labels\n        number_of_iterations (int): the number of times to shuffle the
      training set;\n            also the number of models that will be returned\n
      \       turbo (boolean): a flag that when True runs through the set of unplaced
      feature\n            vectors repeatedly until no new neurons are placed (default
      is True)\n        number_of_neurons (int): the maximum allowed number of neurons;
      when the\n            model reaches this number, the algorithm will stop training\n
      \       aggressive_neuron_creation (bool): flag for placing neurons even if
      they are within\n            the influence field of another neuron of the same
      category (default is False)\n\n    Returns:\n        a set of models\n\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Training Algorithm
    subtype: Training Algorithm
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 81
  fields:
    uuid: c1558ccc-cb9d-40e4-bb99-eabbb9f2e8d8
    name: Hierarchical Clustering with Neuron Optimization
    function_in_file: cluster_learn
    input_contract:
      - type: DataFrame
        name: input_data
        handle_by_set: true
      - type: str
        name: label_column
        handle_by_set: true
      - type: int
        name: number_of_neurons
        handle_by_set: false
      - type: str
        name: linkage_method
        handle_by_set: false
        options:
          - name: average
          - name: complete
          - name: ward
          - name: single
      - type: str
        name: centroid_calculation
        handle_by_set: false
        options:
          - name: robust
          - name: mean
          - name: median
      - type: int
        name: flip
        handle_by_set: false
      - type: str
        name: cluster_method
        handle_by_set: false
        options:
          - name: DHC
          - name: DLHC
          - name: kmeans
      - type: str
        name: aif_method
        handle_by_set: false
        options:
          - name: min
          - name: max
          - name: robust
          - name: mean
          - name: median
      - type: int
        name: singleton_aif
        handle_by_set: false
      - type: int
        name: min_number_of_dominant_vector
        handle_by_set: false
      - type: int
        name: max_number_of_weak_vector
        handle_by_set: false
      - element_type: str
        type: list
        name: ignore_columns
        handle_by_set: true
      - element_type: str
        type: list
        name: classifiers
        handle_by_set: true
        options:
          - name: PME
      - element_type: str
        type: list
        name: validation_methods
        handle_by_set: true
        options:
          - name: Stratified K-Fold Cross-Validation
          - name: Leave-One-Subject-Out
          - name: Stratified Shuffle Split
    output_contract: []
    description:
      "\n    Hierarchical Clustering with Neuron Optimization takes as
      input feature vectors,\n    corresponding class labels, and desired number of
      patterns, and outputs a model. Each\n    pattern in a model consists of a centroid,
      its class label, and its area of influence\n    (AIF). Each centroid is calculated
      as an average of objects in the cluster, each class\n    label is the label
      of the majority class, and each AIF is the distance between the\n    centroid
      and the farthest object in that cluster.\n\n    Args:\n        input_data (DataFrame):
      input feature vectors with a label column\n        label_column (str): the name
      of the column in input_data containing labels\n        number_of_neurons (int):
      the maximum number of output clusters (neurons) desired\n        linkage_method
      (str): options are average, complete, ward, and single (default\n            is
      average)\n        centroid_calculation (str): options are robust, mean, and
      median (default is\n            robust)\n        flip (int): default is 1\n
      \       cluster_method (str): options are DLCH, DHC, and kmeans (default is
      DLHC)\n        aif_method (str): options are min, max, robust, mean, median
      (default is max)\n        singleton_aif (int): default is 0\n        min_number_of_dominant_vector
      (int) : It is used for pruning. It defines min. number of\n            vector
      for dominant class in the cluster.\n        max_number_of_weak_vector(int) :
      It is used for pruning. It defines max. number of\n            vector for weak
      class in the cluster.\n\n    Returns:\n        one or more models\n\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Training Algorithm
    subtype: Training Algorithm
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 82
  fields:
    uuid: 21a88278-5b23-43ad-9bad-5f6b095d512d
    name: RBF with Neuron Allocation Limit
    function_in_file: train_n_prune
    input_contract:
      - type: DataFrame
        name: input_data
        handle_by_set: true
      - type: str
        name: label_column
        handle_by_set: true
      - type: int
        name: chunk_size
        handle_by_set: false
      - type: int
        name: inverse_relearn_frequency
        handle_by_set: false
      - type: int
        name: max_neurons
        handle_by_set: false
      - element_type: str
        type: list
        name: ignore_columns
        handle_by_set: true
      - element_type: str
        type: list
        name: classifiers
        handle_by_set: true
        options:
          - name: PME
      - element_type: str
        type: list
        name: validation_methods
        handle_by_set: true
        options:
          - name: Set Sample Validation
          - name: Stratified Shuffle Split
      - type: bool
        name: aggressive_neuron_creation
    output_contract: []
    description:
      "\n    The Train and Prune algorithm takes as input feature vectors,
      corresponding class\n    labels, and maximum desired number of neurons, and
      outputs a model. The training\n    vectors are partitioned into subsets (chunks)
      and presented to the PME (\"Burlington\")\n    simulator which places neurons
      and determines areas of influence (AIFs). After each\n    subset is learned,
      the neurons that fired the most on the validation set are retained\n    and
      the others are removed (pruned) from the model. After a defined number of train
      and\n    prune cycles, the complete retained set of neurons is then re-learned,
      which results in\n    larger neuron AIFs. Train/prune/re-learn cycles continue
      to run on all of the remaining\n    chunks, keeping the total number of neurons
      within the limit while giving preference to\n    neurons that fire frequently.\n\n
      \   Args:\n        input_data (DataFrame): input feature vectors with a label
      column\n        label_column (str): the name of the column in input_data containing
      labels\n        chunk_size (int): the number of training vectors in each chunk\n
      \       inverse_relearn_frequency (int): the number of chunks to train and prune
      between\n            each re-learn phase\n        max_neurons (int): the maximum
      allowed number of neurons\n        aggressive_neuron_creation (bool): flag for
      placing neurons even if they are within\n            the influence field of
      another neuron of the same category (default is False)\n\n    Returns:\n        a
      model\n\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Training Algorithm
    subtype: Training Algorithm
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 153
  fields:
    uuid: 5e11e050-1788-4bc0-bbe1-7770997feb0b
    name: Windowing
    function_in_file: sg_windowing
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: group_columns
      - default: 250
        type: int
        name: window_size
        c_param: 1
      - default: 250
        type: int
        name: delta
        c_param: 2
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
        description: Append columns start and stop of the segment index.
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This function takes the `input_data` and group by `group_column`.
      It divides\n    each group into windows of size `window_size`. The argument
      `delta`\n    represents the extent of overlap.\n\n    Args:\n        input_data:
      Pandas dataframe\n        group_columns: list of column names on which grouping
      is to be done\n        window_size: Size of each window\n        delta: The
      number of samples to increment. It is similar to overlap.\n          If delta
      is equal to window size, this means no overlap.\n        return_segment_index
      (False): Set to true to see the segment indexes\n          for start and end.
      Note: This should only be used for visualization not\n          pipeline building.\n
      \   Returns:\n        DataFrame: Returns dataframe with `SegmentID` column added
      to the original dataframe.\n\n    Example:\n        >>> dsk.pipeline.reset()\n
      \       >>> df = dsk.datasets.load_activity_raw_toy()\n        >>> df\n            out:\n
      \                  Subject     Class  Rep  accelx  accely  accelz\n                0
      \     s01  Crawling    1     377     569    4019\n                1      s01
      \ Crawling    1     357     594    4051\n                2      s01  Crawling
      \   1     333     638    4049\n                3      s01  Crawling    1     340
      \    678    4053\n                4      s01  Crawling    1     372     708
      \   4051\n                5      s01  Crawling    1     410     733    4028\n
      \               6      s01  Crawling    1     450     733    3988\n                7
      \     s01  Crawling    1     492     696    3947\n                8      s01
      \ Crawling    1     518     677    3943\n                9      s01  Crawling
      \   1     528     695    3988\n                10     s01  Crawling    1      -1
      \   2558    4609\n                11     s01   Running    1     -44   -3971
      \    843\n                12     s01   Running    1     -47   -3982     836\n
      \               13     s01   Running    1     -43   -3973     832\n                14
      \    s01   Running    1     -40   -3973     834\n                15     s01
      \  Running    1     -48   -3978     844\n                16     s01   Running
      \   1     -52   -3993     842\n                17     s01   Running    1     -64
      \  -3984     821\n                18     s01   Running    1     -64   -3966
      \    813\n                19     s01   Running    1     -66   -3971     826\n
      \               20     s01   Running    1     -62   -3988     827\n                21
      \    s01   Running    1     -57   -3984     843\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_transform('Windowing', params={\n
      \                                      'group_columns':['Subject', 'Class',
      'Rep'],\n                                       'window_size' : 5,\n                                       'delta':
      5})\n        >>> results, stats = dsk.pipeline.execute()\n        >>> print
      results\n            out:\n                      Class  SegmentID Subject  accelx
      \ accely  accelz  window_id\n                0   Crawling    1     s01     377
      \    569    4019          0\n                1   Crawling    1     s01     357
      \    594    4051          0\n                2   Crawling    1     s01     333
      \    638    4049          0\n                3   Crawling    1     s01     340
      \    678    4053          0\n                4   Crawling    1     s01     372
      \    708    4051          0\n                5   Crawling    1     s01     410
      \    733    4028          1\n                6   Crawling    1     s01     450
      \    733    3988          1\n                7   Crawling    1     s01     492
      \    696    3947          1\n                8   Crawling    1     s01     518
      \    677    3943          1\n                9   Crawling    1     s01     528
      \    695    3988          1\n                10  Crawling    1     s01      -1
      \   2558    4609          2\n                11   Running    1     s01     -44
      \  -3971     843          0\n                12   Running    1     s01     -47
      \  -3982     836          0\n                13   Running    1     s01     -43
      \  -3973     832          0\n                14   Running    1     s01     -40
      \  -3973     834          0\n                15   Running    1     s01     -48
      \  -3978     844          0\n                16   Running    1     s01     -52
      \  -3993     842          1\n                17   Running    1     s01     -64
      \  -3984     821          1\n                18   Running    1     s01     -64
      \  -3966     813          1\n                19   Running    1     s01     -66
      \  -3971     826          1\n                20   Running    1     s01     -62
      \  -3988     827          1\n                21   Running    1     s01     -57
      \  -3984     843          2\n    "
    path: "../library/core_functions/sg_windowing.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: true
    c_file_name: sg_windowing.c
    core: true
    dcl_executable: false
    c_function_name: sg_windowing
- model: library.transform
  pk: 156
  fields:
    uuid: 38630d53-4f5d-4fd8-88b9-be6dd40bbc37
    name: Double Twist Segmentation
    function_in_file: streaming_gesture_spotting
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - streams: true
        type: str
        name: axis_of_interest
        display_name: Axis Of Interest
      - default: 225
        display_name: Max Segment Length
        type: int
        name: max_segment_length
        description: Maximum length of a segment that can be identified
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - default: true
        display_name: Warm-up
        type: boolean
        name: warmup
        description: Apply a 5-second 'warm-up' period before segments are detected
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
        description: Append columns start and stop of the segment index.
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Performs double-twist segmentation and truncates all event
      data streams to just the\n    section of the time series determined to be a
      valid gesture. There should\n    be multiple events expected per group, and
      each will be given a rep ID\n    in the output. The user must provide the names
      of one stream to use for\n    spotting.\n\n    Args:\n        input_data (DataFrame):
      The input data.\n        axis_of_interest (str): The stream to use for segmentation.\n
      \       group_columns ([str]): A list of column names to use for grouping.\n
      \       warmup (bool): Applies a 5-second warm-up before segments are detected.\n
      \           Defaults to True.\n        max_segment_length (int): This is the
      maximum number of samples a\n            segment can contain. A segment length
      too large will not fit on the\n            device.\n        return_segment_index
      (False): Set to true to see the segment indexes for start and end.\n            Note:
      This should only be used for visualization not pipeline building.\n\n    Returns:\n
      \       DataFrame: The segmented result will have three new column called `SegmentID`,\n
      \       `Seg_Begin`, and `Seg_End`. The `SegmentID` contains the segment IDs
      and\n        columns for Peak Begin and Peak End. `Seg_Begin` and `Seg_End`
      contain\n        the actual begin and end indices of each segment. These three
      values will\n        be duplicated for the entire segment.\n\n    Examples:\n
      \       >>> dsk.pipeline.reset()\n        >>> df = dsk.datasets.load_gesture_raw()\n
      \       >>> df.head()\n            out:\n              Subject  accelx  accely
      \ accelz  gyrox  gyroy  gyroz\n            0     s01   -1546   -3619     345
      \  1701    242    493\n            1     s01   -1563   -3780     212   1543
      \   392    487\n            2     s01   -1561   -3771      75   1403    483
      \   506\n            3     s01   -1578   -3689     -67   1285    487    507\n
      \           4     s01   -1539   -3617    -194   1180    428    494\n        >>>
      dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>> axis_of_interest
      = u'gyroy'\n        >>> group_columns = [u'Subject']\n        >>> warmup = True\n
      \       >>> dsk.pipeline.add_transform('Double Twist Segmentation',\n                                       params={'group_columns':
      group_columns,\n                                               'axis_of_interest'
      : axis_of_interest,\n                                               'warmup'
      : warmup,\n                                                'return_segment_index':True})\n
      \       >>> results, stats = dsk.pipeline.execute()\n        >>> print results\n
      \           out:\n                        Seg_Begin  Seg_End  SegmentID Subject
      \ accelx  accely  accelz  gyrox  gyroy                  0            589       905
      \   0     s01   -4314     190  -12029  -1650  28108\n                1            589
      \      905    0     s01   -7188     789  -11445  -1605  25163\n                2
      \           589       905    0     s01  -13083     613  -10928  -1582  18906\n
      \               3            589       905    0     s01  -14347    1434   -9917
      \ -1384  12045\n                4            589       905    0     s01  -11531
      \   1785   -8071  -1193   7023\n                5            589       905    0
      \    s01   -8853     709   -4726   -980   4479\n                        gyroz\n
      \               0      2445\n                1      2243\n                2
      \     2248\n                3      2585\n                4      3018\n                5
      \     3229\n    "
    path: "../library/core_functions/sg_double_twist.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: true
    c_file_name: sg_double_twist.c
    core: true
    dcl_executable: true
    c_function_name: streaming_gesture_spotting
- model: library.transform
  pk: 158
  fields:
    uuid: 1628367e-8da6-4cd8-8216-70d585356def
    name: Segment Filter MSE
    function_in_file: sg_filter_mse
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: input_column
      - element_type: str
        type: list
        name: group_columns
      - default: -1
        type: float
        name: MSE_target
      - default: 0.01
        type: float
        name: MSE_threshold
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "Filters out groups that do not pass the MSE threshold.\n\n    Args:\n
      \       input_data (DataFrame): the input dataframe\n        input_column (str):
      the name of the column to use for filtering\n        group_columns (list[str]):
      set of columns that define the unique groups\n        MSE_target (float): filter
      target value (default is -1.0)\n        MSE_threshold (float): filter threshold
      value (default is 0.01)\n\n    Returns:\n        DataFrame: The filtered dataframe.\n\t"
    path: "../library/core_functions/sg_filter_mse.py"
    type: Transform
    subtype: Segment
    has_c_version: true
    c_file_name: sg_filter_mse.c
    core: true
    dcl_executable: false
    c_function_name: sg_filter_mse
- model: library.transform
  pk: 685
  fields:
    uuid: 69ffc36a-67b0-4411-b8f8-c5cbe0ba4da9
    name: Dur
    function_in_file: signal_duration
    input_contract:
      - type: DataFrame
        name: input_data
      - type: float
        name: sample_rate
        handle_by_set: true
        description: Sample rate of the sensor data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description: Duration of the signal
    path: C:\Users\mbuehle\Documents\nbi_jv-nbijv\server\library\files/69ffc36a-67b0-4411-b8f8-c5cbe0ba4da9.py
    type: Feature Generator
    subtype: ""
    has_c_version: false
    c_file_name: ""
    core: false
    dcl_executable: false
    c_function_name: ""
- model: library.transform
  pk: 689
  fields:
    uuid: 7a711510-1c58-4843-8c48-5780efd48e10
    name: Neuron Optimization
    function_in_file: neuron_optimize
    input_contract:
      - type: DataFrame
        name: input_data
        handle_by_set: true
      - type: str
        name: label_column
        handle_by_set: true
      - type: int
        name: number_of_neurons
        handle_by_set: false
      - type: str
        name: linkage_method
        handle_by_set: false
        options:
          - name: average
          - name: complete
          - name: ward
          - name: single
      - type: str
        name: centroid_calculation
        handle_by_set: false
        options:
          - name: robust
          - name: mean
          - name: median
      - type: int
        name: norm_order
        handle_by_set: false
      - type: int
        name: flip
        handle_by_set: false
      - type: str
        name: cluster_method
        handle_by_set: false
        options:
          - name: DHC
          - name: DLHC
          - name: kmeans
      - type: str
        name: aif_method
        handle_by_set: false
        options:
          - name: min
          - name: max
          - name: robust
          - name: mean
          - name: median
      - type: int
        name: singleton_aif
        handle_by_set: false
      - element_type: str
        type: list
        name: ignore_columns
        handle_by_set: true
      - element_type: str
        type: list
        name: classifiers
        handle_by_set: true
        options:
          - name: PME
      - element_type: int
        type: list
        name: neuron_range
      - element_type: str
        type: list
        name: validation_methods
        handle_by_set: true
        options:
          - name: Stratified K-Fold Cross-Validation
          - name: Leave-One-Subject-Out
          - name: Stratified Shuffle Split
    output_contract: []
    description:
      This performs an optimized search over neuron and classification
      space for the Heirarchcial clustering training algorithm on PME.
    path: "../library/core_functions/mg_contracts.py"
    type: Training Algorithm
    subtype: Training Algorithm
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1032
  fields:
    uuid: 3421d973-2296-47d3-9416-8e987b462749
    name: CDF
    function_in_file: fg_fixed_width_cumulative_distribution_function
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - type: int
        name: range_left
      - type: int
        name: range_right
      - default:
        type: int
        name: normalize_factor
      - default: 128
        type: int
        name: number_of_bins
      - default: 255
        type: int
        name: scaling_factor
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "Translates to the data stream(s) from a segment into a feature vector
      containing the cumulative\n         distribution function probabilities.\n\n
      \   Args:\n        input_data (DataFrame): input data\n        column (list
      of strings): name of the sensor streams to use\n        range_left (int): the
      left limit (or the min) of the range for a fixed bin histogram\n        range_right
      (int): the right limit (or the max) of the range for a fixed bin histogram\n
      \       normalize_factor (None, optional): the number of samples used to calculate
      the histogram\n        number_of_bins (int, optional): the number of bins used
      for the histogram\n        scaling_factor (int, optional): scaling factor used
      to fit for the device\n\n\n    Returns:\n        DataFrame: feature vector in
      cdf space.\n    "
    path: "../library/core_functions/fg_histogram.py"
    type: Feature Generator
    subtype: Histogram
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1033
  fields:
    uuid: 9c267fc4-262d-4d5e-af5a-2163e1b66925
    name: Histogram
    function_in_file: fg_fixed_width_histogram
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Column on which to apply the feature generator
      - type: int
        name: range_left
      - type: int
        name: range_right
      - default: 128
        type: int
        name: number_of_bins
      - default: 255
        type: int
        name: scaling_factor
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
        family: true
    description:
      "Translates to the data stream(s) from a segment into a feature vector
      in histogram space.\n\n    Args:\n        input_data (DataFrame): input data\n
      \       column (list of strings): name of the sensor streams to use\n        range_left
      (int): the left limit (or the min) of the range for a fixed bin histogram\n
      \       range_right (int): the right limit (or the max) of the range for a fixed
      bin histogram\n        number_of_bins (int, optional): the number of bins used
      for the histogram\n        scaling_factor (int, optional): scaling factor used
      to fit for the device\n\n\n\n    Returns:\n        DataFrame: feature vector
      in histogram space.\n    "
    path: "../library/core_functions/fg_histogram.py"
    type: Feature Generator
    subtype: Histogram
    has_c_version: true
    c_file_name: fg_histogram.c
    core: true
    dcl_executable: false
    c_function_name: fg_fixed_width_histogram
- model: library.transform
  pk: 1408
  fields:
    uuid: ff96dd2c-8d1e-4639-85e0-54d6adc94596
    name: Load Neuron Array
    function_in_file: load_neuron_array
    input_contract:
      - type: DataFrame
        name: input_data
        handle_by_set: true
      - type: str
        name: label_column
        handle_by_set: true
      - element_type: str
        type: list
        name: ignore_columns
        handle_by_set: true
      - element_type: str
        type: list
        name: classifiers
        handle_by_set: true
        options:
          - name: PME
      - element_type: str
        type: list
        name: validation_methods
        handle_by_set: true
        options:
          - name: Stratified K-Fold Cross-Validation
          - name: Leave-One-Subject-Out
          - name: Stratified Shuffle Split
      - element_type: dict
        type: list
        name: neuron_array
        handle_by_set: false
      - type: dict
        name: class_map
        handle_by_set: false
    output_contract: []
    description:
      "\n    Load Neuron Array takes an input of feature vectors, corresponding\n
      \   class labels, and a neuron array to use for classification.\n    The neuron
      array is loaded into the hardware simulator and classification\n    is performed.
      Note: This Training Algorithm does not perform optimizations\n    on the provided
      neurons.\n\n    Args:\n        input_data (DataFrame): input feature vectors
      with a label column\n        label_column (str): the name of the column in input_data
      containing labels\n        neuron_array (list): A list of neurons to load into
      the hardware simulator.\n        class_map (dict): class map for converting
      labels to neuron categories.\n\n    Returns:\n        a set of models\n\n    "
    path: "../library/core_functions/mg_contracts.py"
    type: Training Algorithm
    subtype: Training Algorithm
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1409
  fields:
    uuid: 7c1d4fd9-a7ae-4880-9569-2e65375a3e21
    name: Variance-Based Segmentation
    function_in_file: variance_spotting
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - streams: true
        number_of_elements: 1
        type: str
        name: column_of_interest
        display_name: Column Of Interest
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - display_name: Window Size
        name: window_size
        default: 50
        c_param_index: 0
        type: int
        description: Samples in window
      - display_name: Peak Window Length
        name: peak_window_length
        default: 10
        c_param_index: 1
        type: int
        description: Samples used for Peak Detection
      - display_name: Vertical Threshold
        name: vertical_threshold
        default: 0.25
        c_param_index: 2
        type: float
        description: ""
      - display_name: Horizontal Threshold
        name: horizontal_threshold
        default: 25
        c_param_index: 3
        type: float
        description: ""
      - display_name: Segmentation Sum
        name: segmentation_sum
        default: 500
        c_param_index: 4
        type: int
        description: ""
      - default: true
        display_name: Remove Mean
        type: boolean
        name: remove_mean
        description: Subtract mean before computing variance
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
        description: Append columns start and stop of the segment index.
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the segmentation of time series based on sliding
      window and variance\n    value of windows. Transforms original timeseries domain
      to std domain.\n\n    Args:\n        input_data (DataFrame): input data\n        column_of_interest
      (str): name of the stream to use for segmentation\n        group_columns (list[str]):
      list of column names to use for grouping\n        window_size (int): number
      of samples in the window (default is 50)\n        peak_window_length (int):
      delta number of samples used in peak detection subroutine\n            (default
      is 10)\n        vertical_threshold (float): any value less than this will be
      set to 0 (default is 0.25)\n        horizontal_threshold (float): any segment
      with width less than this will not be\n            accepted as a segment\n        segmentation_sum
      (int): sum of time series in the segment, it\n           is also used to detect
      segments\n        remove_mean (bool): when true, the column_of_interest will
      have its mean\n           subtracted before variance computation\n        return_segment_index
      (False): set to true to see the segment indexes for start and end.\n\n    Returns:\n
      \       DataFrame: The segmented result will have a new column called `SegmentID`
      that\n        contains the segment IDs.\n    "
    path: "../library/core_functions/sg_variance_based.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1410
  fields:
    uuid: 5965c245-c36d-4085-860c-79617906db02
    name: Sum
    function_in_file: stats_sum
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the cumulative sum of each column in 'columns' in
      the dataframe.\n\n    Args:\n        input_data (DataFrame) : input data as
      pandas dataframe\n        columns:  list of columns on which to apply the feature
      generator\n        group_columns: List of column names for grouping\n\n    Returns:\n
      \       DataFrame: Returns data frame containing sum values of each specified
      column.\n\n    Examples:\n        >>> import pandas as pd\n        >>> df =
      pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0, 6, 3],\n                               [-2,
      8, 7], [2, 9, 6]],\n                               columns= ['accelx', 'accely',
      'accelz'])\n        >>> df\n            out:\n               accelx  accely
      \ accelz\n            0      -3       6       5\n            1       3       7
      \      8\n            2       0       6       3\n            3      -2       8
      \      7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Sum\"],\n                params = {\"group_columns\":
      []},\n                function_defaults={\"columns\":['accelx', 'accely', 'accelz']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                   accelxSum  accelySum  accelzSum\n            0
      \        0.0         36.0         29.0\n    "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_sum.c
    core: true
    dcl_executable: false
    c_function_name: stats_sum
- model: library.transform
  pk: 1411
  fields:
    uuid: ec5395d8-733f-4758-a8e8-dccc6285a88f
    name: Offset Factor
    function_in_file: tr_segment_offset_factor
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: input_columns
      - default: 0
        type: int
        name: offset_factor
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n\n    Adds an offset to each column in input data. This can be
      used in conjunction with scale\n    factor to split multiple channels into distinct
      bands of data.\n\n    column[0]+= offset_factor*0\n    column[1]+= offset_factor*1\n
      \   column[2]+= offset_factor*2\n\n    Note: Be careful about using too large
      of an offset otherwise your trained model will not\n    match up with what is
      on the device. Currently, the supported devices buffer is int16.\n\n    Args:\n
      \       input_data: DataFrame\n        input_columns: list of column names\n
      \       offset_factor: int; number by which input_columns are offset by\n\n
      \   Returns:\n        DataFrame\n\n    "
    path: "../library/core_functions/tr_segment_offset_factor.py"
    type: Transform
    subtype: Segment
    has_c_version: true
    c_file_name: tr_segment_offset.c
    core: true
    dcl_executable: false
    c_function_name: tr_segment_offset
- model: library.transform
  pk: 1412
  fields:
    uuid: 47e36385-499b-4574-9704-ff161f7a5d77
    name: Pre-specified Bin Histogram
    function_in_file: fg_pre_specified_bins_histogram
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Column on which to apply the feature generator
      - element_type: list
        type: list
        name: pre_specified_bins
      - default:
        type: int
        name: normalize_factor
      - default: 255
        type: int
        name: scaling_factor
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "Translates to the data stream(s) from a segment into a feature vector
      in histogram\n    space based on pre_specified bins.\n\n    Args:\n        input_data
      (DataFrame): input data\n        column (list of strings): name of the sensor
      streams to use\n        pre_specified_bins (list of list of two floats: begin
      and end, in a increasing order):\n            pre-specified bins: begin and
      end pair\n        normalize_factor (None, optional): the number of samples used
      to calculate the histogram\n        scaling_factor (int, optional): scaling
      factor used to fit for the device\n\n\n    Returns:\n        DataFrame: feature
      vector in histogram space.\n    "
    path: "../library/core_functions/fg_histogram.py"
    type: Feature Generator
    subtype: Histogram
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1416
  fields:
    uuid: 863293c1-66cb-4b49-a9b6-eddcfd6a491f
    name: Magnitude
    function_in_file: tr_magnitude
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: input_columns
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Computes the magnitude (square sum) of a signal across the
      input_columns\n    streams.\n\n    Args:\n        input_data: DataFrame containing
      the time series data\n        input_columns: sensor streams to use in computing
      the magnitude\n\n    Returns:\n        The input DataFrame with an additional
      column containing the per-sample\n        magnitude of the desired input_columns\n
      \   "
    path: "../library/core_functions/tr_magnitude.py"
    type: Transform
    subtype: Sensor
    has_c_version: true
    c_file_name: tr_sensor_magnitude.c
    core: true
    dcl_executable: false
    c_function_name: tr_sensor_magnitude
- model: library.transform
  pk: 1418
  fields:
    uuid: 56d888d4-5940-4f5f-9074-4ca1ace7901f
    name: Pause-Based Segmentation
    function_in_file: pause_based_3d
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - display_name: Columns Of Interest
        name: columns_of_interest
        element_type: str
        streams: true
        number_of_elements: 3
        type: list
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - display_name: Sample Rate
        type: int
        name: sample_rate
        c_param: 8
        description: ""
      - display_name: Minimum Buffer Size (sec)
        name: min_buffer_size_s
        c_param: 1
        default: 0.8
        type: float
        description: Minimum duration of buffer in seconds
      - display_name: Maximum Buffer Size (sec)
        name: max_buffer_size_s
        c_param: 2
        default: 5
        type: float
        description: Maximum duration of buffer in seconds
      - display_name: Standard Deviation Threshold
        name: std_threshold
        c_param: 3
        default: 300
        type: int
        description: Standard Deviation Threshold
      - display_name: Minimum Gesture Length (sec)
        name: min_gesture_length_s
        c_param: 4
        default: 0.75
        type: float
        description: Minimum duration of gesture in seconds
      - display_name: Maximum Gesture Length (sec)
        name: max_gesture_length_s
        c_param: 5
        default: 3
        type: float
        description: Maximum duration of gesture in seconds
      - display_name: Minimum Gesture Peak to Peak Amplitude
        name: min_gesture_p2p_amplitude
        c_param: 6
        default: 3000
        type: int
        description: Minimum gesture amplitude
      - display_name: Maximum Gesture Peak to Peak Amplitude
        name: max_gesture_p2p_amplitude
        c_param: 7
        default: 20000
        type: int
        description: Maximum gesture amplitude
      - default: 0
        display_name: Padding Length
        type: int
        name: pad_length
        description:
          Number of samples to pad the beginning and end of the capture to
          help identify additional segments
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
        description: Append columns start and stop of the segment index.
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Performs pause-based segmentation and truncates all event
      data streams to just\n    the section of the time series determined to be a
      valid segment. There should\n    be multiple events expected per group, and
      each will be given a SegmentID\n    in the output. The user must provide the
      names of three streams to use for\n    spotting.\n\n    Args:\n        input_data
      (DataFrame): the input data\n        columns_of_interest (list[str]): three
      streams to use for segmentation\n        group_columns (list[str]): list of
      column names to use for grouping\n        sample_rate (int): sensor sampling
      rate\n        min_buffer_size_s (float): minimum duration of buffer in seconds
      (default is 0.8)\n        max_buffer_size_s (float): maximum duration of buffer
      in seconds (default is 5.0)\n        std_threshold (int): standard deviation
      threshold (default is 300)\n        min_gesture_length_s (float): minimum duration
      of gesture in seconds (default is 0.75)\n        max_gesture_length_s (float):
      maximum duration of gesture in seconds (default is 3.0)\n        min_gesture_p2p_amplitude
      (int): minimum gesture amplitude (default is 3000)\n        max_gesture_p2p_amplitude
      (int): maximum gesture amplitude (default is 20000)\n        pad_length (int):
      number of samples to pad the beginning and end of the capture to\n            help
      identify the first and last segments (default is 0)\n        return_segment_index
      (boolean): set to true to see the segment indexes for start and end\n\n    Returns:\n
      \       DataFrame: The segmented result will have a new column called `SegmentID`
      that contains the\n        segment IDs.\n    "
    path: "../library/core_functions/sg_pause_based.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: true
    c_file_name: sg_std_based.c
    core: true
    dcl_executable: true
    c_function_name: keyless_gesture_segmenter
- model: library.transform
  pk: 1423
  fields:
    uuid: dca38b75-f176-46c7-b240-5e1fccf4febc
    name: Absolute Average
    function_in_file: tr_absolute_average
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: input_columns
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Computes the absolute average of a signal across the input_columns\n
      \   streams.\n\n    Args:\n        input_data: DataFrame containing the time
      series data\n        input_columns: sensor streams to use in computing the magnitude\n\n
      \   Returns:\n        The input DataFrame with an additional column containing
      the per-sample\n        absolute average of the desired input_columns\n    "
    path: "../library/core_functions/tr_absolute_average.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1424
  fields:
    uuid: 0d666c7e-670b-43a4-99c4-ee5adbd0ce4b
    name: High Pass Filter
    function_in_file: tr_high_pass_filter
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: input_columns
      - type: float
        name: alpha
      - default: 0
        type: int
        name: x0
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Performs a high pass filter on the input columns, modifies
      the sensor streams in place.\n\n    Args:\n        input_data: DataFrame containing
      the time series data\n        input_columns: sensor streams to apply high pass
      filter against\n        alpha: attenuation coefficient\n        x0: Initial
      value to start data stream with. If None, uses first value in data_stream\n\n
      \   Returns:\n        input data after having been passed through high pass
      filter\n    "
    path: "../library/core_functions/tr_high_pass_filter.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1425
  fields:
    uuid: a70f45da-1bcf-4f0c-92bb-0214961c1169
    name: Average
    function_in_file: tr_average
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: input_columns
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Computes the average of a signal across the input_columns
      streams.\n\n    Args:\n        input_data: DataFrame containing the time series
      data\n        input_columns: sensor streams to use in computing the average\n\n
      \   Returns:\n        The input DataFrame with an additional column containing
      the per-sample\n        average of the desired input_columns\n    "
    path: "../library/core_functions/tr_average.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1426
  fields:
    uuid: e220141c-74e0-4aab-841c-a5688ed04625
    name: Windowing Threshold Segmentation
    function_in_file: windowing_threshold
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - streams: true
        number_of_elements: 1
        type: str
        name: column_of_interest
        display_name: Column Of Interest
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - display_name: Window Size
        name: window_size
        c_param: 1
        default: 200
        type: int
        description: number of samples in the segment
      - display_name: Offset
        name: offset
        c_param: 2
        default: 0
        type: int
        description: Offset from anchor point to start of segment.
      - display_name: Vertical Threshold
        name: vt_threshold
        c_param: 3
        default: 1000
        type: float
        description: threshold above which a segment will be identified
      - display_name: Threshold Space Width
        name: threshold_space_width
        c_param: 4
        default: 25
        type: int
        description: the size of the window to transform into threshold space
      - display_name: Threshold Space
        description: space to transform signal into
        default: std
        type: str
        options:
          - name: std
          - name: absolute sum
          - name: sum
          - name: variance
          - name: absolute avg
        name: threshold_space
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is a single pass threshold segmentation algorithm which
      transforms a window\n    of the data stream of size threshold_space_width into
      threshold space. The threshold\n    space can be computed as standard deviation,
      sum, absolute sum, absolute\n    average and variance. The vt threshold is then
      compared against the\n    calculated value with a comparison type of >=. Once
      the threshold space is\n    detected above the vt_threshold that becomes the
      anchor point. The segment\n    starts at the index of the detected point minus
      a user specified offset. The end of\n    the segment is immediately set to the
      window size.\n\n    Args:\n        input_data (DataFrame): input data\n        column_of_interest
      (str): name of the stream to use for segmentation\n        group_columns (list[str]):
      list of column names to use for grouping.\n        window_size (int): number
      of samples in the window (default is 100)\n        offset (int): The offset
      from the anchor point and the start of the\n        segment. for a offset of
      0, the start of the window will start at the\n        anchor point. ( default
      is 0)\n        vt_threshold (int): vt_threshold value which determines the segment.\n
      \       threshold_space_width (int): Size of the threshold buffer.\n        threshold_space
      (str): Threshold transformation space. (std, sum, absolute sum, variance, absolute
      avg)\n        return_segment_index (False): Set to true to see the segment indexes
      for start and end.\n\n    Returns:\n        DataFrame: The segmented result
      will have a new column called `SegmentID` that\n        contains the segment
      IDs.\n    "
    path: "../library/core_functions/sg_windowing_threshold.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: true
    c_file_name: sg_windowing_threshold.c
    core: true
    dcl_executable: true
    c_function_name: windowing_threshold_segmenter
- model: library.transform
  pk: 1427
  fields:
    uuid: 3efc9a87-e18b-4b68-b1b6-14905313fc53
    name: Max Min Threshold Segmentation
    function_in_file: max_min_threshold
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - streams: true
        number_of_elements: 1
        type: str
        name: column_of_interest
        display_name: Column Of Interest
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - display_name: Maximum Segment Length
        name: max_segment_length
        c_param: 1
        default: 200
        type: int
        description: maximum number of samples a segment can have
      - display_name: Minimum Segment Length
        name: min_segment_length
        c_param: 2
        default: 50
        type: int
        description: minimum number of samples a segment can have
      - display_name: Threshold Space Width
        name: threshold_space_width
        c_param: 3
        default: 25
        type: int
        description: the size of the window to transform into threshold space
      - display_name: Threshold Space
        description:
          space to transform signal into to compare against the vertical
          thresholds
        default: std
        type: str
        options:
          - name: std
          - name: absolute sum
          - name: sum
          - name: variance
          - name: absolute avg
        name: threshold_space
      - display_name: Initial Vertical Threshold
        name: first_vt_threshold
        c_param: 4
        default: 1000
        type: float
        description: the segment starts when the threshold space is above this value
      - display_name: Second Vertical Threshold
        name: second_vt_threshold
        c_param: 5
        default: 1000
        type: float
        description: the segment ends when the threshold space is below this value
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is a max min threshold segmentation algorithm which transforms
      a window\n    of the data stream of size threshold_space_width into threshold
      space. The threshold\n    space can be computed as standard deviation, sum,
      absolute sum, absolute\n    average and variance. The vt threshold is then compared
      against the\n    calculated value with a comparison type of >= for the start
      of the segment\n    and <= for the end of the segment. This algorithm is a two
      pass\n    detection, the first pass detects the start of the segment, the second
      pass\n    detects the end of the segment.\n\n    Args:\n        input_data (DataFrame):
      input data\n        column_of_interest (str): name of the stream to use for
      segmentation\n        group_columns (list[str]): list of column names to use
      for grouping\n        max_segment_length (int): number of samples in the window
      (default is 100)\n        min_segment_length: The smallest segment allowed.\n
      \       first_vt_threshold (int):vt_threshold value to begin detecting a segment\n
      \       second_vt_threshold (int):vt_threshold value to detect a segments end.\n
      \       threshold_space_width (float): number of samples to check for being
      above the\n          vt_threshold before forgetting segment.\n        return_segment_index
      (False): set to true to see the segment indexes for start and end.\n\n    Returns:\n
      \       DataFrame: The segmented result will have a new column called `SegmentID`
      that\n        contains the segment IDs.\n    "
    path: "../library/core_functions/sg_max_min_threshold.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: true
    c_file_name: sg_max_min_threshold.c
    core: true
    dcl_executable: true
    c_function_name: max_min_threshold_segmenter
- model: library.transform
  pk: 1428
  fields:
    uuid: 82fe1e29-5dcc-4b55-86dd-b4f2a10f8362
    name: Global Min Max Sum
    function_in_file: min_max_sum
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This function is the sum of the maximum and minimum values.
      It is also\n    used as the 'min max amplitude difference'.\n\n    Args:\n        input_data:
      DataFrame\n        columns: (list of str): Set of columns on which to apply
      the feature generator\n        group_columns:(list of str): Set of columns by
      which to aggregate\n        **kwargs:\n\n    Returns:\n        DataFrame\n\n
      \   Examples:\n        >>> df = pd.DataFrame({'Subject': ['s01'] * 20,\n                               'Class':
      ['Crawling'] * 20 ,\n                               'Rep': [0] * 10 + [1] *10
      })\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2, -3, -1, 1, -3, -4,
      1, 2, 6, 2, -3, -2, -1]\n        >>> df\n            out:\n                   Class
      \ Rep Subject  accelx\n            0   Crawling    0     s01       1\n            1
      \  Crawling    0     s01      -2\n            2   Crawling    0     s01      -3\n
      \           3   Crawling    0     s01       1\n            4   Crawling    0
      \    s01       2\n            5   Crawling    0     s01       5\n            6
      \  Crawling    0     s01       2\n            7   Crawling    0     s01      -2\n
      \           8   Crawling    0     s01      -3\n            9   Crawling    0
      \    s01      -1\n            10  Crawling    1     s01       1\n            11
      \ Crawling    1     s01      -3\n            12  Crawling    1     s01      -4\n
      \           13  Crawling    1     s01       1\n            14  Crawling    1
      \    s01       2\n            15  Crawling    1     s01       6\n            16
      \ Crawling    1     s01       2\n            17  Crawling    1     s01      -3\n
      \           18  Crawling    1     s01      -2\n            19  Crawling    1
      \    s01      -1\n\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Global Min
      Max Sum\"],\n                    params = {\"group_columns\": ['Subject', 'Class',
      'Rep']},\n                    function_defaults={\"columns\":['accelx']})\n
      \       >>> result, stats = dsk.pipeline.execute()\n        >>> print result\n
      \           out:\n                  Class  Rep Subject   accelxMinMaxSum\n            0
      \ Crawling    0     s01                 2\n            1  Crawling    1     s01
      \                2\n    "
    path: "../library/core_functions/fg_shape_amplitude.py"
    type: Feature Generator
    subtype: Amplitude
    has_c_version: true
    c_file_name: fg_amplitude_min_max_sum.c
    core: true
    dcl_executable: false
    c_function_name: min_max_sum
- model: library.transform
  pk: 1429
  fields:
    uuid: 908bfec1-ba0d-4a10-89fe-5efd4160117d
    name: General Threshold Segmentation
    function_in_file: general_threshold
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - display_name: First Column Of Interest
        name: first_column_of_interest
        streams: true
        number_of_elements: 1
        type: str
        description: column used to identify the start of the segment
      - display_name: Second Column Of Interest
        name: second_column_of_interest
        streams: true
        number_of_elements: 1
        type: str
        description: column used to identify the end of the segment
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - display_name: Maximum Segment Length
        name: max_segment_length
        c_param: 1
        default: 200
        type: int
        description: maximum number of samples a segment can have
      - display_name: Minimum Segment Length
        name: min_segment_length
        c_param: 2
        default: 50
        type: int
        description: minimum number of samples a segment can have
      - display_name: Threshold Space Width
        name: threshold_space_width
        c_param: 3
        default: 25
        type: int
        description: the size of the window to transform into threshold space
      - display_name: Initial Vertical Threshold
        name: first_vt_threshold
        c_param: 4
        default: 1000
        type: float
        description: the threshold value to identify the start of a segment
      - display_name: First Threshold Space
        description:
          space to transform signal into to compare against the first vertical
          threshold
        default: std
        type: str
        options:
          - name: std
          - name: absolute sum
          - name: sum
          - name: variance
          - name: absolute avg
        name: first_threshold_space
      - display_name: First Comparison
        description:
          the comparison between threshold space and vertical threshold (>=,
          <=)
        default: max
        type: str
        options:
          - name: max
          - name: min
        name: first_comparison
      - display_name: Second Vertical Threshold
        name: second_vt_threshold
        c_param: 5
        default: 1000
        type: float
        description: the threshold value to identify the end of a segment
      - display_name: Second Threshold Space
        description:
          space to transform signal into to compare against the second vertical
          threshold
        default: std
        type: str
        options:
          - name: std
          - name: absolute sum
          - name: sum
          - name: variance
          - name: absolute avg
        name: second_threshold_space
      - display_name: Second Comparison
        description:
          the comparison between threshold space and vertical threshold (>=,
          <=)
        default: min
        type: str
        options:
          - name: max
          - name: min
        name: second_comparison
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is a general threshold segmentation algorithm which transforms
      a window\n    of the data stream of size threshold_space_width into threshold
      space. The threshold\n    space can be computed as standard deviation, sum,
      absolute sum, absolute\n    average and variance. The vt threshold is then compared
      against the\n    calculated value with a comparison type of <= or >= based on
      the use of\n    \"min\" or \"max\" in the comparison type. This algorithm is
      a two pass\n    detection, the first pass detects the start of the segment,
      the second pass\n    detects the end of the segment. In this generalized algorithm,
      the two can\n    be set independently.\n\n    Args:\n        input_data (DataFrame):
      input data\n        first_column_of_interest (str): name of the stream to use
      for first threshold segmentation\n        second_column_of_interest (str): name
      of the stream to use for second threshold segmentation\n        group_columns
      (list[str]): list of column names to use for grouping\n        max_segment_length
      (int): number of samples in the window (default is 200)\n        min_segment_length
      (int): The smallest segment allowed. (default 100)\n        first_vt_threshold
      (int):vt_threshold value to begin detecting a segment\n        first_threshold_space
      (str): threshold space to detect segment against (std, variance, absolute avg,
      absolute sum, sum)\n        first_comparison (str): detect threshold above(max)
      or below(min) the vt_threshold (max, min)\n        second_vt_threshold (int):vt_threshold
      value to detect a segments\n        end.\n        second_threshold_space (str):
      threshold space to detect segment end (std, variance, absolute avg, absolute
      sum, sum)\n        second_comparison (str): detect threshold above(max) or below(min)
      the vt_threshold (max, min)\n        threshold_space_width (int): the size of
      the buffer that the threshold value is\n        calculated from.\n        return_segment_index
      (False): set to true to see the segment indexes for start and end.\n\n    Returns:\n
      \       DataFrame: The segmented result will have a new column called `SegmentID`
      that\n        contains the segment IDs.\n    "
    path: "../library/core_functions/sg_general_threshold.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: true
    c_file_name: sg_general_threshold.c
    core: true
    dcl_executable: true
    c_function_name: general_threshold_segmenter
- model: library.transform
  pk: 1430
  fields:
    uuid: 03b43910-4ccc-4b90-9294-5f1b7fd6e7e4
    name: Average Energy
    function_in_file: average_energy
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Average Energy.\n\n    1) Calculate the element-wise square
      of the input columns.\n    2) Sum the squared components across each column
      for the total energy per sample.\n    3) Take the average of the sum of squares
      to get the average energy.\n\n    Args:\n        input_data: input DataFrame.\n\n
      \       columns:  List of str; The `columns` represents a list of all\n                  column
      names on which `average_energy` is to be found.\n\n        group_columns: List
      of str; Set of columns by which to aggregate\n\n        **kwargs:\n\n    Returns:\n
      \       Dataframe\n\n    Examples:\n        >>> df = pd.DataFrame({'Subject':
      ['s01'] * 20,\n                             'Class': ['Crawling'] * 20,\n                             'Rep':
      [0] * 8 + [1] * 12})\n        >>> df['accelx'] = [1, -2, -3, 1, 2, 5, 2, -2,
      -3, -1, 1, -3, -4, 1, 2, 6, 2, -3, -2, -1]\n        >>> df['accely'] = [0, 9,
      5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9, 0, 9, 5, -5, -9]\n        >>> df['accelz']
      = [1, -2, 3, -1, 2, 5, 2, -2, -3, 1, 1, 3, 4, 1, 2, 6, 2, -3, -2, -1]\n        >>>
      df\n            out:\n                       Class  Rep Subject  accelx  accely
      \ accelz\n                0   Crawling    0     s01       1       0       1\n
      \               1   Crawling    0     s01      -2       9      -2\n                2
      \  Crawling    0     s01      -3       5       3\n                3   Crawling
      \   0     s01       1      -5      -1\n                4   Crawling    0     s01
      \      2      -9       2\n                5   Crawling    0     s01       5
      \      0       5\n                6   Crawling    0     s01       2       9
      \      2\n                7   Crawling    0     s01      -2       5      -2\n
      \               8   Crawling    1     s01      -3      -5      -3\n                9
      \  Crawling    1     s01      -1      -9       1\n                10  Crawling
      \   1     s01       1       0       1\n                11  Crawling    1     s01
      \     -3       9       3\n                12  Crawling    1     s01      -4
      \      5       4\n                13  Crawling    1     s01       1      -5
      \      1\n                14  Crawling    1     s01       2      -9       2\n
      \               15  Crawling    1     s01       6       0       6\n                16
      \ Crawling    1     s01       2       9       2\n                17  Crawling
      \   1     s01      -3       5      -3\n                18  Crawling    1     s01
      \     -2      -5      -2\n                19  Crawling    1     s01      -1
      \     -9      -1\n        >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Average Energy\"],\n
      \            params = {\"group_columns\": ['Subject', 'Class', 'Rep']},\n             function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                  Class  Rep Subject
      \ gen_0000_AvgEng\n            0  Crawling    0     s01            52.75\n            1
      \ Crawling    1     s01            60.00\n    "
    path: "../library/core_functions/fg_energy.py"
    type: Feature Generator
    subtype: Energy
    has_c_version: true
    c_file_name: fg_energy_average_energy.c
    core: true
    dcl_executable: false
    c_function_name: average_energy
- model: library.transform
  pk: 1433
  fields:
    uuid: 1223926b-fd8f-4a91-989e-41970d51419a
    name: Threshold Crossing Filter
    function_in_file: tr_threshold_crossing_filter
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: group_columns
      - type: str
        name: input_column
      - default: 0
        type: float
        name: threshold
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "Filters out groups that do not cross the threshold at least once
      in the\n    desired input column.\n\n    Args:\n        input_data (DataFrame):
      the input DataFrame\n        group_columns (list[str]): set of columns that
      define the unique groups\n        input_column (str): the name of the column
      to use for filtering\n        threshold (float): the threshold (default is 0.0)\n\n
      \   Returns:\n        DataFrame: The filtered DataFrame.\n    "
    path: "../library/core_functions/tr_threshold_crossing_filter.py"
    type: Transform
    subtype: Segment
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1434
  fields:
    uuid: 2475e49d-c6ea-4e0d-9d35-93277855bcb6
    name: First Derivative
    function_in_file: tr_first_derivative
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: input_column
    output_contract:
      - type: DataFrame
        name: input_data
    description:
      "\n    Computes the first derivative of a single sensor input column.\n\n
      \   Args:\n        input_data: DataFrame\n        input_column: name of the
      column of which to calculate the derivative\n\n    Returns:\n        The input
      DataFrame with an additional column containing the integer\n        derivative
      of the desired input_column\n    "
    path: "../library/core_functions/tr_derivative.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1435
  fields:
    uuid: 8264da8b-81ba-4ff8-abcb-2c983d201903
    name: Second Derivative
    function_in_file: tr_second_derivative
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: input_column
    output_contract:
      - type: DataFrame
        name: input_data
    description:
      "\n    Computes the second derivative of a single sensor input column.\n\n
      \   Args:\n        input_data: DataFrame\n        input_column: name of the
      column of which to calculate the second\n         derivative\n\n    Returns:\n
      \       The input DataFrame with an additional column containing the integer\n
      \       second derivative of the desired input_column.\n    "
    path: "../library/core_functions/tr_derivative.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1436
  fields:
    uuid: f30f5ec2-3a46-4d21-9b41-e6497bdc0c76
    name: Sample By Metadata
    function_in_file: sample_by_metadata
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: metadata_name
      - element_type: str
        type: list
        name: metadata_values
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Select rows from the input DataFrame based on a metadata column.
      Rows\n    that have a metadata value that is in the values list will be returned.\n\n
      \   Args:\n        input_data (DataFrame): input DataFrame\n        metadata_name
      (str): name of the metadata column to use for sampling\n        metadata_values
      (list[str]): list of values of the named column for which to\n          select
      rows of the input data.\n\n    Returns:\n        DataFrame containing only the
      rows for which the metadata value is in the\n        accepted list\n\n    "
    path: "../library/core_functions/samplers.py"
    type: Sampler
    subtype:
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1437
  fields:
    uuid: 465eb1fc-e36f-4962-9bb2-febf63fd9e61
    name: SMA Filter
    function_in_file: tr_ma_filter_symmetric
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: group_columns
      - type: str
        name: input_column
      - default: 1
        type: int
        name: filter_order
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "\n    Performs a symmetric moving average filter on the input column,\n
      \   creates a new column with the filtered data.\n\n    Args:\n        input_data:
      DataFrame containing the time series data.\n        group_columns: columns to
      group data by before processing.\n        input_column: sensor stream to apply
      moving average filter on.\n        filter order: the number of samples to average
      to the left and right.\n\n    Returns:\n        input data after having been
      passed through symmetric moving average filter\n    "
    path: "../library/core_functions/tr_ma_filter_symmetric.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1438
  fields:
    uuid: f425d96a-2eea-4673-ab12-abaa2f1a4df3
    name: Information Gain
    function_in_file: information_gain
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: label_column
        handle_by_set: true
        description: Name of the label column
      - default: 2
        type: int
        name: feature_number
        description: Number of features will be selected for each class
      - default: 0.9
        type: float
        name: target_sensor_weight
        description:
          It is used to compute information gain score. It is a coefficient
          score
      - type: list
        name: passthrough_columns
        handle_by_set: true
        description: List of non sensor columns
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is a supervised feature selection algorithm that selects
      features based\n    on Information Gain (one class vs other class approaches).\n\n
      \   It first calculates Information Gain (IG) for each class separately to all
      features then sort\n    features based on IG scores. Features with highest IG
      is better feature to differentiate the\n    class from others. At the end each
      feature has their own feature list.\n\n    Args:\n        input_data: DataFrame,
      it holds input data\n        label_column: Name of the label column\n        feature_number:
      Number of features will be selected for each class.\n        target_sensor_weight:
      It is used to compute the information gain score. It is a coefficient\n        score.\n
      \       passthrough_columns: List of column names; The set of columns the selector
      should ignore\n\n    Returns:\n        DataFrame which includes selected features
      and the passthrough columns.\n    "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Supervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1439
  fields:
    uuid: 9ecfa420-145e-4a93-862c-3f087b8fdaf5
    name: Absolute Sum
    function_in_file: stats_abs_sum
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the cumulative sum of absolute values in each column
      in 'columns' in the dataframe.\n\n    Args:\n        input_data (DataFrame)
      : input data as pandas dataframe\n        columns:  list of columns on which
      to apply the feature generator\n        group_columns: List of column names
      for grouping\n\n    Returns:\n        DataFrame: Returns data frame containing
      absolute sum values of each specified column.\n\n    Examples:\n        >>>
      import pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5], [3, 7, 8], [0,
      6, 3],\n                               [-2, 8, 7], [2, 9, 6]],\n                               columns=
      ['accelx', 'accely', 'accelz'])\n        >>> df\n            out:\n               accelx
      \ accely  accelz\n            0      -3       6       5\n            1       3
      \      7       8\n            2       0       6       3\n            3      -2
      \      8       7\n            4       2       9       6\n        >>> dsk.pipeline.reset()\n
      \       >>> dsk.pipeline.set_input_data('test_data', df, force=True)\n        >>>
      dsk.pipeline.add_feature_generator([\"Absolute Sum\"],\n                params
      = {\"group_columns\": []},\n                function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                   accelxAbsSum
      \ accelyAbsSum  accelzAbsSum\n            0         10        36         29\n
      \   "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_abs_sum.c
    core: true
    dcl_executable: false
    c_function_name: stats_abs_sum
- model: library.transform
  pk: 1440
  fields:
    uuid: 41fd603d-f294-4d4e-909d-5e886bab0aaa
    name: Absolute Mean
    function_in_file: stats_abs_mean
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Set of columns on which to apply the feature generator
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    Computes the arithmetic mean of absolute value in each column
      of `columns` in the dataframe.\n\n    Args:\n        input_data (DataFrame)
      : input data as pandas dataframe\n        columns:  list of columns on which
      to apply the feature generator\n        group_columns: List of column names
      for grouping\n        **kwargs:\n\n    Returns:\n        DataFrame: Returns
      data frame containing absolute mean values of each specified column.\n\n    Examples:\n
      \       >>> import pandas as pd\n        >>> df = pd.DataFrame([[-3, 6, 5],
      [3, 7, 8],\n                               [0, 6, 3], [-2, 8, 7],\n                               [2,
      9, 6]], columns= ['accelx', 'accely', 'accelz'])\n        >>> df\n            out:\n
      \              accelx  accely  accelz\n            0      -3       6       5\n
      \           1       3       7       8\n            2       0       6       3\n
      \           3      -2       8       7\n            4       2       9       6\n
      \       >>> dsk.pipeline.reset()\n        >>> dsk.pipeline.set_input_data('test_data',
      df, force=True)\n        >>> dsk.pipeline.add_feature_generator([\"Absolute
      Mean\"],\n                 params = {\"group_columns\": []},\n                 function_defaults={\"columns\":['accelx',
      'accely', 'accelz']})\n        >>> result, stats = dsk.pipeline.execute()\n
      \       >>> print result\n            out:\n                   accelxAbsMean
      \ accelyAbsMean  accelzAbsMean\n            0         2.0            7.2            5.8\n
      \   "
    path: "../library/core_functions/fg_stats.py"
    type: Feature Generator
    subtype: Statistical
    has_c_version: true
    c_file_name: fg_stats_abs_mean.c
    core: true
    dcl_executable: false
    c_function_name: stats_abs_mean
- model: library.transform
  pk: 1441
  fields:
    uuid: 08c55556-0590-45d8-9571-2668f56019dd
    name: Filter Extreme Values
    function_in_file: filter_extreme_values
    input_contract:
      - type: DataFrame
        name: input_data
      - type: list
        name: input_columns
      - type: int
        name: min_bound
      - type: int
        name: max_bound
      - default: {}
        type: dict
        name: signal_min_max_parameters
    output_contract:
      - type: DataFrame
        name: input_data
      - type: list
        name: signal_min_max_parameters
        persist: true
    description:
      "\n    Filters sensor streams removing samples that contain values
      outside bounds.\n    User can specify a global min_bound and max_bound to act
      on all input_columns,\n    or use signal_min_max_parameters to customize min
      and max bounds per column.\n\n    Args:\n        input_data (DataFrame): Dataframe
      to be filtered\n        input_columns (list): List of column names to be acted
      on.\n        min_bound (int): global min bound\n        max_bound (int): global
      max bound\n        signal_min_max_parameters (dict): Dictionary of per-column
      maximums\n            and per-column minimums. This parameter is optional. If
      it is not passed,\n            the global min_bound and max_bound will be used
      for all input columns.\n\n    Returns:\n        df_out (DataFrame): the new
      data frame with data rows removed if they contained\n            a value outside
      minimums and maximums.\n        signal_min_max_params (dict): If 'signal_min_max_parameters'
      = {}, it will take\n            the same min and max from the global min_bound
      and max_bound for each selected column.\n\n\n    Examples:\n\n        Using
      global min_bound and max_bound\n\n        >>> from pandas import DataFrame\n
      \       df = DataFrame([[-100, -200, -300], [3, 7, 8], [0, 6, 90],\n                        [-2,
      8, 7], [8, 9, 6], [8, 9, 6], [100, 200, 300]],\n                        columns=['AccelerometerX',
      'AccelerometerY', 'AccelerometerZ'])\n        >>> df['Subject'] = 's01'\n        >>>
      dsk.pipeline.set_input_data('test_data', df, force = True)\n        >>> dsk.pipeline.add_transform('Filter
      Extreme Values',\n                                        params={'input_columns]:['AccelerometerX',\n
      \                                                               'AccelerometerY',\n
      \                                                               'AccelerometerZ'],\n
      \                                       'min_bound' : -100,\n                                        'max_bound'
      : 100})\n        >>> result, stats = dsk.pipeline.execute()\n        >>> result\n
      \           Out:\n                AccelerometerX  AccelerometerY  AccelerometerZ
      \ Subject\n            1                3               7              8       s01\n
      \           2                0               6             90       s01\n            3
      \              -2               8              7       s01\n            4                8
      \              9              6       s01\n            5                8               9
      \             6       s01\n\n\n        Using signal_min_max_parameters\n\n        >>>
      dsk.pipeline.reset()\n        >>> min_max_param = {'maximums': {'AccelerometerX':
      400,\n                                            'AccelerometerY': 90,\n                                            'AccelerometerZ':
      500},\n                            'minimums': {'AccelerometerX': 0,\n                                            'AccelerometerY':
      0,\n                                            'AccelerometerZ': -100}}\n        >>>
      dsk.pipeline.set_input_data('test_data', df, force = True)\n        >>> dsk.pipeline.add_transform('Filter
      Extreme Values',\n                                        params={'input_columns':['AccelerometerX',\n
      \                                                               'AccelerometerY',\n
      \                                                               'AccelerometerZ'],\n
      \                                       'min_bound' : -100,\n                                        'max_bound'
      : 100,\n                                        'signal_min_max_parameters':
      min_max_param})\n            result, stats = dsk.pipeline.execute()\n            result\n
      \           Out:\n\n                AccelerometerX  AccelerometerY  AccelerometerZ
      \ Subject\n            1                3               7               8      s01\n
      \           2                8               9               6      s01\n            3
      \               8               9               6      s01\n\n    "
    path: "../library/core_functions/tr_filter_extreme_values.py"
    type: Transform
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1442
  fields:
    uuid: bc8f435d-8c54-46b7-a7fa-bc461fffd36d
    name: MFCC
    function_in_file: fg_mfcc
    input_contract:
      - type: DataFrame
        name: input_data
      - element_type: str
        type: list
        name: columns
        description: Column on which to apply the feature generator
      - type: int
        name: sample_rate
      - default: 23
        type: int
        name: cepstra_count
      - element_type: str
        type: list
        name: group_columns
        handle_by_set: true
        description: Set of columns by which to aggregate
    output_contract:
      - type: DataFrame
        name: output_data
        family: true
    description:
      "Translates the data stream(s) from a segment into a feature vector
      of\n    Mel-frequency Cepstral coefficients (MFCC). The features are derived
      in the\n    frequency domain that mimic human auditory response.\n\n    Args:\n
      \       input_data (DataFrame): input data\n        columns (list of strings):
      names of the sensor streams to use\n        sample_rate (int): sampling rate\n
      \       cepstra_count (int): number of coefficients to generate\n        group_columns
      (list of str): set of columns by which to aggregate\n\n    Returns:\n        DataFrame:
      feature vector of MFCC coefficients.\n    "
    path: "../library/core_functions/fg_mfcc.py"
    type: Feature Generator
    subtype: MFCC
    has_c_version: true
    c_file_name: fg_mfcc.c
    core: true
    dcl_executable: false
    c_function_name: fg_mfcc
- model: library.transform
  pk: 1443
  fields:
    uuid: 66aadbce-9473-4c5b-adf8-9276bc96c2a0
    name: Pre-emphasis Filter
    function_in_file: tr_segment_pre_emphasis_filter
    input_contract:
      - type: DataFrame
        name: input_data
      - type: str
        name: input_column
      - element_type: str
        type: list
        name: group_columns
      - default: 0.97
        type: float
        name: alpha
      - default: 0
        type: int
        name: prior
    output_contract:
      - type: DataFrame
        name: df_out
    description:
      "Performs a pre-emphasis filter on the input columns and modifies
      the sensor\n    streams in place. This is a first-order Fir filter that performs
      a weighted\n    average of each sample with the previous sample.\n\n    Args:\n
      \       input_data (DataFrame): DataFrame containing the time series data\n
      \       input_column (str): sensor stream to apply pre_emphasis filter against\n
      \       group_columns (list of str): set of columns by which to aggregate\n
      \       alpha (float): pre-emphasis factor (weight given to the previous sample)\n
      \       prior (int): the value of the previous sample, default is 0\n\n    Returns:\n
      \       input data after having been passed through a pre-emphasis filter\n
      \   "
    path: "../library/core_functions/tr_segment_pre_emphasis_filter.py"
    type: Transform
    subtype: Segment
    has_c_version: true
    c_file_name: tr_segment_pre_emphasis_filter.c
    core: true
    dcl_executable: false
    c_function_name: tr_segment_pre_emphasis_filter
- model: library.transform
  pk: 1444
  fields:
    uuid: f783d606-a1c6-407d-85ee-84924bd58969
    name: Custom Feature Selection
    function_in_file: custom_feature_selection
    input_contract:
      - type: DataFrame
        name: input_data
      - type: dict
        name: custom_feature_selection
        description: Describes which features to keep
      - type: list
        name: passthrough_columns
        handle_by_set: true
        description: List of non sensor columns
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "This is a feature selection method which allows custom feature selection.
      This takes a dictionary where\n    the key is the feature generator number and
      the value is an array of the features for the feature generator to keep.\n    All
      feature generators that are not added as keys in the dictionary will be kept.\n\n
      \   Args:\n        input_data (DataFrame): Input data\n        custom_feature_selection
      (dict): feature generator number and array of features to keep.\n\n    Returns:\n
      \        DataFrame: which includes selected features and the passthrough columns.\n
      \        list: unselected features\n    "
    path: "../library/core_functions/selectors.py"
    type: Feature Selector
    subtype: Supervised
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: false
    c_function_name:
- model: library.transform
  pk: 1445
  fields:
    uuid: 50169016-be4d-4cab-a77d-3d5f964853e2
    name: Windowing Peak Detection
    function_in_file: windowing_peak_detection_segmenter
    input_contract:
      - no_display: true
        type: DataFrame
        name: input_data
      - display_name: First Column Of Interest
        name: first_column_of_interest
        streams: true
        number_of_elements: 1
        type: str
        description: column used to identify the start of the segment
      - display_name: Second Column Of Interest
        name: second_column_of_interest
        streams: true
        number_of_elements: 1
        type: str
        description: column used to identify the end of the segment
      - no_display: true
        element_type: str
        type: list
        name: group_columns
      - display_name: Maximum Segment Length
        name: max_segment_length
        c_param: 1
        default: 200
        type: int
        description: maximum number of samples a segment can have
      - display_name: Minimum Segment Length
        name: min_segment_length
        c_param: 2
        default: 50
        type: int
        description: minimum number of samples a segment can have
      - display_name: Moving Average Width
        name: moving_average_width
        c_param: 3
        default: 25
        type: int
        description: the order of the moving average
      - display_name: First Threshold Space
        description:
          space to transform signal into to compare against the first vertical
          threshold
        default: normal
        type: str
        options:
          - name: normal
        name: first_threshold_space
      - display_name: First Comparison
        description:
          the comparison between threshold space and vertical threshold (>=,
          <=)
        default: maximum
        type: str
        options:
          - name: maximum
          - name: minimum
        name: first_comparison
      - display_name: Second Threshold Space
        description:
          space to transform signal into to compare against the second vertical
          threshold
        default: normal
        type: str
        options:
          - name: normal
        name: second_threshold_space
      - display_name: Second Comparison
        description: Find the maximum of minimum as of the end segment.
        default: minimum
        type: str
        options:
          - name: maximum
          - name: minimum
        name: second_comparison
      - default: false
        no_display: true
        type: boolean
        name: return_segment_index
    output_contract:
      - type: DataFrame
        name: output_data
    description:
      "\n    This is a sliding window peak detection algorithm which detects
      the local maximum or minimum\n    within a window and sets the start of a segment
      to that point. It then detects the local\n    maximum or minimum within the
      max_segment_length distance from that point and sets that\n    as the end of
      the segment.\n\n    Args:\n        input_data (DataFrame): input data\n        first_column_of_interest
      (str): name of the stream to use for first threshold segmentation\n        second_column_of_interest
      (str): name of the stream to use for second threshold segmentation\n        group_columns
      (list[str]): list of column names to use for grouping\n        max_segment_length
      (int): number of samples in the window (default is 200)\n        min_segment_length
      (int): The smallest segment allowed. (default 100)\n        first_threshold_space
      (str): threshold space to detect segment against (normal)\n        first_comparison
      (str): detect the maximum or minimum value within the\n        window (minimum,
      minimum)\n        second_threshold_space (str): threshold space to detect segment
      end (normal)\n        second_comparison (str): detect the maximum or minimum
      value within the\n        window (minimum, minimum)\n        moving_average_width
      (int): the size of the moving average window.\n        return_segment_index
      (False): set to true to see the segment indexes for start and end.\n\n    Returns:\n
      \       DataFrame: The segmented result will have a new column called `SegmentID`
      that\n        contains the segment IDs.\n    "
    path: "../library/core_functions/sg_windowing_peak_detection.py"
    type: Segmenter
    subtype: Sensor
    has_c_version: false
    c_file_name:
    core: true
    dcl_executable: true
    c_function_name:
- model: library.functioncost
  pk: 1
  fields:
    function:
    c_function_name: calc_area
    function_type: support
    flash: "280"
    sram: "0"
    stack: "100.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 2
  fields:
    function: 36
    c_function_name: total_area
    function_type: core
    flash: "180"
    sram: "0"
    stack: "72.0"
    latency: "17.5"
    flash_dependencies:
      - calc_area
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 3
  fields:
    function: 37
    c_function_name: absolute_area
    function_type: core
    flash: "188"
    sram: "0"
    stack: "72.0"
    latency: "21"
    flash_dependencies:
      - calc_area
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 4
  fields:
    function: 38
    c_function_name: total_area_low_frequency
    function_type: core
    flash: "296"
    sram: "0"
    stack: "144.0"
    latency: "40.8"
    flash_dependencies:
      - calc_area
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 5
  fields:
    function: 39
    c_function_name: absolute_area_low_frequency
    function_type: core
    flash: "148"
    sram: "0"
    stack: "44.4"
    latency: "50"
    flash_dependencies:
      - calc_area
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 6
  fields:
    function: 40
    c_function_name: total_area_high_frequency
    function_type: core
    flash: "144"
    sram: "0"
    stack: "144.0"
    latency: "47.3"
    flash_dependencies:
      - calc_area
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 7
  fields:
    function: 41
    c_function_name: absolute_area_high_frequency
    function_type: core
    flash: "148"
    sram: "0"
    stack: "50.9"
    latency: "50"
    flash_dependencies:
      - calc_area
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 8
  fields:
    function: 1430
    c_function_name: average_energy
    function_type: core
    flash: "160"
    sram: "0"
    stack: "112.0"
    latency: "2"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 9
  fields:
    function: 66
    c_function_name: total_energy
    function_type: core
    flash: "136"
    sram: "0"
    stack: "76.0"
    latency: "6.7"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 10
  fields:
    function:
    c_function_name: calc_dominant_frequency
    function_type: support
    flash: "156"
    sram: "0"
    stack: "76.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 11
  fields:
    function:
    c_function_name: calc_spectral_entropy
    function_type: support
    flash: "292"
    sram: "0"
    stack: "80.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 12
  fields:
    function: 34
    c_function_name: dominant_frequency
    function_type: core
    flash: "2396"
    sram: "1024"
    stack: "96.0"
    latency: "100"
    flash_dependencies:
      - calc_dominant_frequency
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 13
  fields:
    function: 35
    c_function_name: spectral_entropy
    function_type: core
    flash: "2480"
    sram: "1024"
    stack: "120.0"
    latency: "130.3"
    flash_dependencies:
      - calc_spectral_entropy
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 14
  fields:
    function:
    c_function_name: crossing_rate
    function_type: support
    flash: "336"
    sram: "0"
    stack: "60.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 15
  fields:
    function: 30
    c_function_name: mean_crossing_rate
    function_type: core
    flash: "104"
    sram: "0"
    stack: "96.0"
    latency: "21.5"
    flash_dependencies:
      - crossing_rate
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 16
  fields:
    function: 29
    c_function_name: mean_difference
    function_type: core
    flash: "196"
    sram: "0"
    stack: "80.0"
    latency: "1"
    flash_dependencies:
      - crossing_rate
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 17
  fields:
    function: 31
    c_function_name: zero_crossing_rate
    function_type: core
    flash: "168"
    sram: "0"
    stack: "120.0"
    latency: "14.5"
    flash_dependencies:
      - crossing_rate
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 18
  fields:
    function: 32
    c_function_name: sigma_crossing_rate
    function_type: core
    flash: "112"
    sram: "0"
    stack: "124.0"
    latency: "31"
    flash_dependencies:
      - crossing_rate
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 19
  fields:
    function: 33
    c_function_name: second_sigma_crossing_rate
    function_type: core
    flash: "116"
    sram: "0"
    stack: "124.0"
    latency: "29"
    flash_dependencies:
      - crossing_rate
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 20
  fields:
    function:
    c_function_name: max_min_high_low_freq
    function_type: support
    flash: "496"
    sram: "0"
    stack: "108.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 21
  fields:
    function: 43
    c_function_name: ratio_high_frequency
    function_type: core
    flash: "136"
    sram: "0"
    stack: "48.0"
    latency: "42"
    flash_dependencies:
      - max_min_high_low_freq
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 22
  fields:
    function: 44
    c_function_name: difference_high_frequency
    function_type: core
    flash: "128"
    sram: "0"
    stack: "48.0"
    latency: "42"
    flash_dependencies:
      - max_min_high_low_freq
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 23
  fields:
    function: 46
    c_function_name: global_p2p_high_frequency
    function_type: core
    flash: "104"
    sram: "0"
    stack: "148.0"
    latency: "47.3"
    flash_dependencies:
      - max_min_high_low_freq
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 24
  fields:
    function: 45
    c_function_name: global_p2p_low_frequency
    function_type: core
    flash: "108"
    sram: "0"
    stack: "148.0"
    latency: "36"
    flash_dependencies:
      - max_min_high_low_freq
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 25
  fields:
    function: 47
    c_function_name: max_p2p_half_high_frequency
    function_type: core
    flash: "112"
    sram: "0"
    stack: "472.0"
    latency: "23.8"
    flash_dependencies:
      - max_min_high_low_freq
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 26
  fields:
    function: 48
    c_function_name: peak_to_peak
    function_type: core
    flash: "92"
    sram: "0"
    stack: "156.0"
    latency: "40"
    flash_dependencies:
      - max_min_high_low_freq
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 27
  fields:
    function: 1428
    c_function_name: min_max_sum
    function_type: core
    flash: "92"
    sram: "0"
    stack: "156.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 28
  fields:
    function:
    c_function_name: sorted_copy
    function_type: support
    flash: "180"
    sram: "4"
    stack: "48.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 29
  fields:
    function:
    c_function_name: sortarray
    function_type: support
    flash: "52"
    sram: "0"
    stack: "28.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 30
  fields:
    function:
    c_function_name: stat_mean
    function_type: support
    flash: "104"
    sram: "0"
    stack: "52.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 31
  fields:
    function:
    c_function_name: stat_moment
    function_type: support
    flash: "156"
    sram: "0"
    stack: "60.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 32
  fields:
    function:
    c_function_name: stats_percentile_presorted
    function_type: support
    flash: "200"
    sram: "0"
    stack: "60.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 33
  fields:
    function:
    c_function_name: get_median
    function_type: support
    flash: "64"
    sram: "0"
    stack: "24.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 34
  fields:
    function: 20
    c_function_name: stats_mean
    function_type: core
    flash: "88"
    sram: "0"
    stack: "88.0"
    latency: "7"
    flash_dependencies:
      - mean
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 35
  fields:
    function: 21
    c_function_name: stats_median
    function_type: core
    flash: "244"
    sram: "0"
    stack: "96.0"
    latency: "0"
    flash_dependencies:
      - sorted_copy
      - sortarray
      - get_median
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 36
  fields:
    function: 22
    c_function_name: stats_stdev
    function_type: core
    flash: "96"
    sram: "0"
    stack: "116.0"
    latency: "14.1"
    flash_dependencies:
      - mean
      - std
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 37
  fields:
    function: 23
    c_function_name: stats_skewness
    function_type: core
    flash: "164"
    sram: "0"
    stack: "108.0"
    latency: "410.3"
    flash_dependencies:
      - stat_moment
      - stat_mean
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 38
  fields:
    function: 24
    c_function_name: stats_kurtosis
    function_type: core
    flash: "152"
    sram: "0"
    stack: "108.0"
    latency: "409.9"
    flash_dependencies:
      - stat_moment
      - stat_mean
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 39
  fields:
    function: 25
    c_function_name: stats_iqr
    function_type: core
    flash: "136"
    sram: "0"
    stack: "112.0"
    latency: "3"
    flash_dependencies:
      - sorted_copy
      - sortarray
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 40
  fields:
    function: 26
    c_function_name: stats_pct025
    function_type: core
    flash: "116"
    sram: "0"
    stack: "104.0"
    latency: "3"
    flash_dependencies:
      - sorted_copy
      - sortarray
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 41
  fields:
    function: 27
    c_function_name: stats_pct075
    function_type: core
    flash: "116"
    sram: "0"
    stack: "104.0"
    latency: "0"
    flash_dependencies:
      - sorted_copy
      - sortarray
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 42
  fields:
    function: 28
    c_function_name: stats_pct100
    function_type: core
    flash: "116"
    sram: "0"
    stack: "104.0"
    latency: "0.1"
    flash_dependencies:
      - sorted_copy
      - sortarray
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 43
  fields:
    function:
    c_function_name: stats_variance
    function_type: disabled
    flash: "356"
    sram: "0"
    stack: "92.0"
    latency: "22"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 44
  fields:
    function: 1033
    c_function_name: fg_fixed_width_histogram
    function_type: core
    flash: "412"
    sram: "0"
    stack: "104.0"
    latency: "23.95"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 45
  fields:
    function: 1410
    c_function_name: stats_sum
    function_type: core
    flash: "88"
    sram: "0"
    stack: "88.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 46
  fields:
    function:
    c_function_name: sum
    function_type: disabled
    flash: "80"
    sram: "0"
    stack: "52.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 47
  fields:
    function:
    c_function_name: mean
    function_type: support
    flash: "100"
    sram: "0"
    stack: "52.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 48
  fields:
    function:
    c_function_name: std
    function_type: support
    flash: "132"
    sram: "0"
    stack: "80.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 49
  fields:
    function: 49
    c_function_name: signal_duration
    function_type: core
    flash: "84"
    sram: "0"
    stack: "44.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 50
  fields:
    function: 50
    c_function_name: pct_time_over_zero
    function_type: core
    flash: "196"
    sram: "0"
    stack: "76.0"
    latency: "10.3"
    flash_dependencies:
      - mean
      - std
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 51
  fields:
    function: 51
    c_function_name: pct_time_over_sigma
    function_type: core
    flash: "252"
    sram: "0"
    stack: "104.0"
    latency: "26"
    flash_dependencies:
      - mean
      - std
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 52
  fields:
    function: 52
    c_function_name: pct_time_over_second_sigma
    function_type: core
    flash: "332"
    sram: "0"
    stack: "104.0"
    latency: "27"
    flash_dependencies:
      - mean
      - std
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 53
  fields:
    function: 67
    c_function_name: downsampler_for_features
    function_type: core
    flash: "280"
    sram: "0"
    stack: "100.0"
    latency: "13"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 54
  fields:
    function: 53
    c_function_name: average_movement_intensity
    function_type: core
    flash: "200"
    sram: "0"
    stack: "116.0"
    latency: "6"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 55
  fields:
    function: 54
    c_function_name: variance_movement_intensity
    function_type: core
    flash: "316"
    sram: "0"
    stack: "112.0"
    latency: "12"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 56
  fields:
    function: 55
    c_function_name: average_signal_magnitude_area
    function_type: core
    flash: "140"
    sram: "0"
    stack: "76.0"
    latency: "9"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 57
  fields:
    function: 60
    c_function_name: cagh
    function_type: core
    flash: "748"
    sram: "0"
    stack: "168.0"
    latency: "15"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 58
  fields:
    function: 61
    c_function_name: aratg
    function_type: core
    flash: "48"
    sram: "0"
    stack: "84.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 59
  fields:
    function: 13
    c_function_name: tr_segment_strip
    function_type: core
    flash: "356"
    sram: "0"
    stack: "88.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 60
  fields:
    function:
    c_function_name: quantize254
    function_type: core
    flash: "80"
    sram: "0"
    stack: "24"
    latency: "24"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 61
  fields:
    function: 14
    c_function_name: normalize
    function_type: core
    flash: "116"
    sram: "0"
    stack: "24.0"
    latency: "22"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 62
  fields:
    function: 12
    c_function_name: min_max_scale
    function_type: core
    flash: "228"
    sram: "0"
    stack: "36.0"
    latency: "17"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 63
  fields:
    function: 158
    c_function_name: sg_filter_mse
    function_type: core
    flash: "196"
    sram: "0"
    stack: "80.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 64
  fields:
    function: 4
    c_function_name: tr_segment_scale_factor
    function_type: core
    flash: "184"
    sram: "0"
    stack: "76.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 65
  fields:
    function: 1411
    c_function_name: tr_segment_offset
    function_type: core
    flash: "184"
    sram: "0"
    stack: "80.0"
    latency: "3"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 66
  fields:
    function: 1416
    c_function_name: tr_sensor_magnitude
    function_type: core
    flash: "96"
    sram: "0"
    stack: "88.0"
    latency: "6"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 67
  fields:
    function: 156
    c_function_name: streaming_gesture_spotting
    function_type: core
    flash: "400"
    sram: "9"
    stack: "88.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 68
  fields:
    function: 153
    c_function_name: sg_windowing
    function_type: core
    flash: "132"
    sram: "8"
    stack: "76.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 69
  fields:
    function:
    c_function_name: KeylessGestureSegmenter
    function_type: core
    flash: "2788"
    sram: "87"
    stack: "132"
    latency: "45"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 70
  fields:
    function:
    c_function_name: variance_based_segmenter
    function_type: disabled
    flash: "2048"
    sram: "1832"
    stack: "120.0"
    latency: "58"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 71
  fields:
    function:
    c_function_name: general_threshold
    function_type: core
    flash: "504"
    sram: "8"
    stack: "80"
    latency: "1"
    flash_dependencies:
      - utils_buffer_functions
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 72
  fields:
    function:
    c_function_name: max_min_threshold
    function_type: core
    flash: "588"
    sram: "8"
    stack: "80"
    latency: "1"
    flash_dependencies:
      - utils_buffer_functions
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 73
  fields:
    function:
    c_function_name: windowing_threshold
    function_type: core
    flash: "396"
    sram: "4"
    stack: "76"
    latency: "1"
    flash_dependencies:
      - utils_buffer_functions
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 74
  fields:
    function:
    c_function_name: kbtoplevel
    function_type: framework
    flash: "1250"
    sram: "248"
    stack: "76.0"
    latency: 40/6*num_sensor_streams*max_segment_length
    flash_dependencies:
      - utils_tr_sensor
    stack_dependencies: []
- model: library.functioncost
  pk: 75
  fields:
    function:
    c_function_name: kbfeaturegen
    function_type: framework
    flash: "48"
    sram: "8"
    stack: "32.0"
    latency: "1"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 76
  fields:
    function:
    c_function_name: kbtrainedneurons
    function_type: framework
    flash: "4"
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 77
  fields:
    function:
    c_function_name: columns
    function_type: framework
    flash: "0"
    sram: 4*num_sensor_streams
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 78
  fields:
    function:
    c_function_name: defined_sample_rate
    function_type: framework
    flash: "0"
    sram: "4"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 79
  fields:
    function:
    c_function_name: raw_data_buf_len
    function_type: framework
    flash: "0"
    sram: "4"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 80
  fields:
    function:
    c_function_name: RAW_DATA_BUFFER
    function_type: buffer
    flash: "0"
    sram: 2*num_sensor_streams*max_segment_length
    stack: "0.0"
    latency: max_segment_length * (1000000/sample_rate)
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 81
  fields:
    function:
    c_function_name: senslen
    function_type: framework
    flash: "0"
    sram: "4"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 82
  fields:
    function:
    c_function_name: sorted_data_len
    function_type: framework
    flash: "0"
    sram: "4"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 83
  fields:
    function:
    c_function_name: sortedData
    function_type: framework
    flash: "0"
    sram: 2*max_segment_length
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 84
  fields:
    function:
    c_function_name: kb_result_buf
    function_type: framework
    flash: "0"
    sram: "2"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 85
  fields:
    function:
    c_function_name: rbuffers
    function_type: framework
    flash: "0"
    sram: 20*num_sensor_streams
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 86
  fields:
    function:
    c_function_name: neurons
    function_type: pme
    flash: 134*num_neurons
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 87
  fields:
    function:
    c_function_name: qrk_functions
    function_type: pme
    flash: "632"
    sram: "0"
    stack: "28.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 88
  fields:
    function:
    c_function_name: utils_buffer_functions
    function_type: support
    flash: "252"
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 89
  fields:
    function:
    c_function_name: utils_tr_sensor
    function_type: support
    flash: "72"
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 90
  fields:
    function: 16
    c_function_name: quantize_254
    function_type: core
    flash: "80"
    sram: "0"
    stack: "24.0"
    latency: "24"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 91
  fields:
    function: 1418
    c_function_name: keyless_gesture_segmenter
    function_type: core
    flash: "2788"
    sram: "87"
    stack: "132.0"
    latency: "45"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 92
  fields:
    function: 1429
    c_function_name: general_threshold_segmenter
    function_type: core
    flash: "504"
    sram: "8"
    stack: "80.0"
    latency: "1"
    flash_dependencies:
      - utils_buffer_functions
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 93
  fields:
    function: 1427
    c_function_name: max_min_threshold_segmenter
    function_type: core
    flash: "588"
    sram: "8"
    stack: "80.0"
    latency: "1"
    flash_dependencies:
      - utils_buffer_functions
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 94
  fields:
    function: 1426
    c_function_name: windowing_threshold_segmenter
    function_type: core
    flash: "396"
    sram: "4"
    stack: "76.0"
    latency: "1"
    flash_dependencies:
      - utils_buffer_functions
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 95
  fields:
    function: 1443
    c_function_name: tr_segment_pre_emphasis_filter
    function_type: core
    flash: "128"
    sram: "0"
    stack: "0.0"
    latency: "0.5"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
- model: library.functioncost
  pk: 96
  fields:
    function: 1439
    c_function_name: stats_abs_sum
    function_type: core
    flash: "140"
    sram: "0"
    stack: "0.0"
    latency: "10.72"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 97
  fields:
    function: 1440
    c_function_name: stats_abs_mean
    function_type: core
    flash: "172"
    sram: "0"
    stack: "0.0"
    latency: "10.96"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 98
  fields:
    function: 42
    c_function_name: area_power_spectrum_density
    function_type: core
    flash: "156"
    sram: "0"
    stack: "0.0"
    latency: "16"
    flash_dependencies: []
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 99
  fields:
    function: 1442
    c_function_name: fg_mfcc
    function_type: core
    flash: "272"
    sram: "0"
    stack: "0.0"
    latency: "6.84"
    flash_dependencies:
      - imfcc
      - fftr
      - fixlog
    stack_dependencies:
      - kbtoplevel
      - kbfeaturegen
- model: library.functioncost
  pk: 100
  fields:
    function:
    c_function_name: imfcc
    function_type: support
    flash: "3346"
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 101
  fields:
    function:
    c_function_name: fftr
    function_type: support
    flash: "1532"
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
- model: library.functioncost
  pk: 102
  fields:
    function:
    c_function_name: fixlog
    function_type: support
    flash: "1874"
    sram: "0"
    stack: "0.0"
    latency: "0"
    flash_dependencies: []
    stack_dependencies: []
